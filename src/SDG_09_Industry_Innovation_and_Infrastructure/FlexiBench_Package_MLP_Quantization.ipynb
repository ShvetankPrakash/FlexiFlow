{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **FlexiBench Package Tracking Monitoring MLP Quantization**\n",
        "### Author: Shvetank Prakash\n",
        "### Date: Jan 2025\n",
        "#### Helpful links:\n",
        "\n",
        "[Co-Design of Approximate Multilayer Perceptron for Ultra-Resource Constrained Printed Circuits](https://arxiv.org/abs/2302.14576) (Paper Results Reproduced)\n",
        "\n",
        "[Gemmlowp Paper](https://arxiv.org/pdf/1712.05877)\n",
        "\n",
        "[Gemmlowp Implementation](https://github.com/google/gemmlowp/tree/master)\n"
      ],
      "metadata": {
        "id": "B6xKk1RMw3zG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Imports and Global Defs"
      ],
      "metadata": {
        "id": "zmARXYN5xetM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-model-optimization==0.8.0\n",
        "!pip install ucimlrepo==0.0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-7RwFYPOH7xC",
        "outputId": "62a3a0cb-568d-4287-b531-492405554de0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-model-optimization==0.8.0 in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (0.1.9)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (1.17.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization==0.8.0) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization==0.8.0) (1.17.2)\n",
            "Requirement already satisfied: ucimlrepo==0.0.7 in /usr/local/lib/python3.11/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo==0.0.7) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo==0.0.7) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo==0.0.7) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Setting environment variables\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8tYezt7xmzUa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "INPUT_SIZE = 33\n",
        "HIDDEN_SIZE_1 = 20\n",
        "HIDDEN_SIZE_2 = 10\n",
        "OUTPUT_SIZE = 5\n",
        "\n",
        "# Training Parameters\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE_TRAIN = 32\n",
        "BATCH_SIZE_TEST = 64\n",
        "LEARNING_RATE = 0.005\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "fMfZ9zYJIdCK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read and Preprocess Dataset**"
      ],
      "metadata": {
        "id": "2Jml3fzQBpSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    labels = df.pop('label')\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df.values.astype('float32') / 255., labels.values.astype('int32')))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Load in dataset from csv\n",
        "train_dataset = load_csv_data('training_data.csv')\n",
        "print(train_dataset)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(list(train_dataset)), seed=SEED)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE_TRAIN)\n",
        "\n",
        "test_dataset = load_csv_data('samples.csv')\n",
        "print(test_dataset)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE_TEST)"
      ],
      "metadata": {
        "id": "YJ_KJE3dn4kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b56ea4-1f5d-4a56-8c33-d5b1449659ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_TensorSliceDataset element_spec=(TensorSpec(shape=(33,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n",
            "<_TensorSliceDataset element_spec=(TensorSpec(shape=(33,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print a single element from each dataset\n",
        "# print(\"Single element from train_dataset:\")\n",
        "# for element in train_dataset.take(1):\n",
        "#     print(element)\n",
        "\n",
        "print(\"\\nSingle element from test_dataset:\")\n",
        "for element in test_dataset.take(1):\n",
        "    print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cqoYwwI9JLVs",
        "outputId": "ca1485c2-aa0f-425e-ff3f-792478743ade"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Single element from test_dataset:\n",
            "(<tf.Tensor: shape=(64, 33), dtype=float32, numpy=\n",
            "array([[0.        , 0.48235294, 0.01176471, ..., 0.08235294, 0.13725491,\n",
            "        0.09803922],\n",
            "       [0.        , 0.47843137, 0.01176471, ..., 0.08235294, 0.13725491,\n",
            "        0.13725491],\n",
            "       [0.        , 0.47843137, 0.01176471, ..., 0.08235294, 0.13725491,\n",
            "        0.09803922],\n",
            "       ...,\n",
            "       [0.        , 0.47843137, 0.01176471, ..., 0.09411765, 0.05882353,\n",
            "        0.07450981],\n",
            "       [0.        , 0.48235294, 0.01568628, ..., 0.10980392, 0.05098039,\n",
            "        0.03529412],\n",
            "       [0.        , 0.48235294, 0.01568628, ..., 0.09411765, 0.05882353,\n",
            "        0.03529412]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "      dtype=int32)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Model**"
      ],
      "metadata": {
        "id": "dSBlaBAaAYEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "def create_mlp(input_size, hidden_size1, hidden_size2, output_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.InputLayer(shape=(input_size,)),\n",
        "        layers.Dense(hidden_size1, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        layers.Dense(hidden_size2, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        layers.Dense(output_size, kernel_regularizer=l2(0.001))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = create_mlp(INPUT_SIZE, HIDDEN_SIZE_1, HIDDEN_SIZE_2, OUTPUT_SIZE)\n",
        "opt = Adam(learning_rate=LEARNING_RATE, )\n",
        "model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RTuXXZnTKVJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "694f3733-8dba-4b2d-f4d4-f84cb572f161",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m680\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m210\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m55\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">680</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m945\u001b[0m (3.69 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">945</span> (3.69 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m945\u001b[0m (3.69 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">945</span> (3.69 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train from Scratch in FP32**"
      ],
      "metadata": {
        "id": "OSfrYoT5_vc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial non-quant training\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_dataset\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "trLEGuB-ItPp",
        "outputId": "e774cd74-d74e-4c8b-aee5-4ab5f6b23849"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.4753 - loss: 1.3439 - val_accuracy: 0.7764 - val_loss: 0.6260\n",
            "Epoch 2/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8192 - loss: 0.5682 - val_accuracy: 0.8771 - val_loss: 0.4378\n",
            "Epoch 3/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8685 - loss: 0.4539 - val_accuracy: 0.8114 - val_loss: 0.5294\n",
            "Epoch 4/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8590 - loss: 0.4575 - val_accuracy: 0.8743 - val_loss: 0.4277\n",
            "Epoch 5/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8757 - loss: 0.4207 - val_accuracy: 0.8857 - val_loss: 0.3969\n",
            "Epoch 6/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8880 - loss: 0.4023 - val_accuracy: 0.8714 - val_loss: 0.4100\n",
            "Epoch 7/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8910 - loss: 0.3873 - val_accuracy: 0.8886 - val_loss: 0.3805\n",
            "Epoch 8/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8915 - loss: 0.3773 - val_accuracy: 0.9021 - val_loss: 0.3767\n",
            "Epoch 9/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8985 - loss: 0.3686 - val_accuracy: 0.8671 - val_loss: 0.4092\n",
            "Epoch 10/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9035 - loss: 0.3782 - val_accuracy: 0.8886 - val_loss: 0.3672\n",
            "Epoch 11/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9021 - loss: 0.3538 - val_accuracy: 0.9150 - val_loss: 0.3592\n",
            "Epoch 12/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9123 - loss: 0.3458 - val_accuracy: 0.8829 - val_loss: 0.3940\n",
            "Epoch 13/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8994 - loss: 0.3653 - val_accuracy: 0.8864 - val_loss: 0.3848\n",
            "Epoch 14/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9016 - loss: 0.3604 - val_accuracy: 0.8864 - val_loss: 0.3810\n",
            "Epoch 15/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9122 - loss: 0.3405 - val_accuracy: 0.9150 - val_loss: 0.3535\n",
            "Epoch 16/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9177 - loss: 0.3288 - val_accuracy: 0.8993 - val_loss: 0.3495\n",
            "Epoch 17/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9123 - loss: 0.3353 - val_accuracy: 0.9171 - val_loss: 0.3407\n",
            "Epoch 18/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9260 - loss: 0.3184 - val_accuracy: 0.9036 - val_loss: 0.3501\n",
            "Epoch 19/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9179 - loss: 0.3385 - val_accuracy: 0.9014 - val_loss: 0.3641\n",
            "Epoch 20/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9143 - loss: 0.3351 - val_accuracy: 0.8957 - val_loss: 0.3579\n",
            "Epoch 21/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9079 - loss: 0.3431 - val_accuracy: 0.8829 - val_loss: 0.4060\n",
            "Epoch 22/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9163 - loss: 0.3273 - val_accuracy: 0.9186 - val_loss: 0.3519\n",
            "Epoch 23/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9088 - loss: 0.3450 - val_accuracy: 0.9036 - val_loss: 0.3449\n",
            "Epoch 24/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9116 - loss: 0.3373 - val_accuracy: 0.8907 - val_loss: 0.3892\n",
            "Epoch 25/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9211 - loss: 0.3143 - val_accuracy: 0.9171 - val_loss: 0.3258\n",
            "Epoch 26/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9177 - loss: 0.3231 - val_accuracy: 0.8793 - val_loss: 0.4066\n",
            "Epoch 27/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9132 - loss: 0.3361 - val_accuracy: 0.8529 - val_loss: 0.4816\n",
            "Epoch 28/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9127 - loss: 0.3345 - val_accuracy: 0.9086 - val_loss: 0.3453\n",
            "Epoch 29/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9128 - loss: 0.3292 - val_accuracy: 0.8950 - val_loss: 0.3725\n",
            "Epoch 30/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9216 - loss: 0.3246 - val_accuracy: 0.9036 - val_loss: 0.3513\n",
            "Epoch 31/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9213 - loss: 0.3170 - val_accuracy: 0.9014 - val_loss: 0.3819\n",
            "Epoch 32/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9248 - loss: 0.3100 - val_accuracy: 0.9021 - val_loss: 0.3798\n",
            "Epoch 33/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9188 - loss: 0.3260 - val_accuracy: 0.9186 - val_loss: 0.3322\n",
            "Epoch 34/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9231 - loss: 0.3144 - val_accuracy: 0.9136 - val_loss: 0.3510\n",
            "Epoch 35/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9213 - loss: 0.3168 - val_accuracy: 0.8957 - val_loss: 0.3627\n",
            "Epoch 36/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9113 - loss: 0.3339 - val_accuracy: 0.9014 - val_loss: 0.3846\n",
            "Epoch 37/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9137 - loss: 0.3266 - val_accuracy: 0.9107 - val_loss: 0.3320\n",
            "Epoch 38/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9230 - loss: 0.3182 - val_accuracy: 0.9021 - val_loss: 0.3516\n",
            "Epoch 39/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9154 - loss: 0.3256 - val_accuracy: 0.9114 - val_loss: 0.3393\n",
            "Epoch 40/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9249 - loss: 0.3010 - val_accuracy: 0.9186 - val_loss: 0.3363\n",
            "Epoch 41/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9216 - loss: 0.3148 - val_accuracy: 0.9029 - val_loss: 0.3514\n",
            "Epoch 42/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9256 - loss: 0.3046 - val_accuracy: 0.9136 - val_loss: 0.3219\n",
            "Epoch 43/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9147 - loss: 0.3255 - val_accuracy: 0.8793 - val_loss: 0.4343\n",
            "Epoch 44/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9254 - loss: 0.3045 - val_accuracy: 0.8843 - val_loss: 0.3970\n",
            "Epoch 45/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9227 - loss: 0.3174 - val_accuracy: 0.8921 - val_loss: 0.3853\n",
            "Epoch 46/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9186 - loss: 0.3102 - val_accuracy: 0.9007 - val_loss: 0.3542\n",
            "Epoch 47/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9259 - loss: 0.3065 - val_accuracy: 0.8814 - val_loss: 0.4053\n",
            "Epoch 48/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9189 - loss: 0.3136 - val_accuracy: 0.8950 - val_loss: 0.3696\n",
            "Epoch 49/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9187 - loss: 0.3154 - val_accuracy: 0.8993 - val_loss: 0.3782\n",
            "Epoch 50/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9231 - loss: 0.3124 - val_accuracy: 0.9129 - val_loss: 0.3386\n",
            "Epoch 51/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9295 - loss: 0.2998 - val_accuracy: 0.9071 - val_loss: 0.3693\n",
            "Epoch 52/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9208 - loss: 0.3124 - val_accuracy: 0.8986 - val_loss: 0.3566\n",
            "Epoch 53/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9304 - loss: 0.2964 - val_accuracy: 0.9021 - val_loss: 0.3476\n",
            "Epoch 54/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9214 - loss: 0.3199 - val_accuracy: 0.9164 - val_loss: 0.3224\n",
            "Epoch 55/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9296 - loss: 0.3000 - val_accuracy: 0.8879 - val_loss: 0.4106\n",
            "Epoch 56/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9210 - loss: 0.3081 - val_accuracy: 0.9157 - val_loss: 0.3256\n",
            "Epoch 57/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9201 - loss: 0.3156 - val_accuracy: 0.9007 - val_loss: 0.3564\n",
            "Epoch 58/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9268 - loss: 0.3090 - val_accuracy: 0.9193 - val_loss: 0.3295\n",
            "Epoch 59/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9237 - loss: 0.3077 - val_accuracy: 0.9036 - val_loss: 0.3501\n",
            "Epoch 60/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9272 - loss: 0.2935 - val_accuracy: 0.9200 - val_loss: 0.3327\n",
            "Epoch 61/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9255 - loss: 0.3023 - val_accuracy: 0.9207 - val_loss: 0.3327\n",
            "Epoch 62/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9268 - loss: 0.3032 - val_accuracy: 0.9129 - val_loss: 0.3512\n",
            "Epoch 63/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9264 - loss: 0.3077 - val_accuracy: 0.9050 - val_loss: 0.3707\n",
            "Epoch 64/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9209 - loss: 0.3213 - val_accuracy: 0.9186 - val_loss: 0.3444\n",
            "Epoch 65/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9191 - loss: 0.3155 - val_accuracy: 0.9164 - val_loss: 0.3497\n",
            "Epoch 66/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9259 - loss: 0.3034 - val_accuracy: 0.9143 - val_loss: 0.3507\n",
            "Epoch 67/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9354 - loss: 0.2897 - val_accuracy: 0.9107 - val_loss: 0.3364\n",
            "Epoch 68/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9198 - loss: 0.3160 - val_accuracy: 0.9236 - val_loss: 0.3223\n",
            "Epoch 69/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9295 - loss: 0.2946 - val_accuracy: 0.9007 - val_loss: 0.3499\n",
            "Epoch 70/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9268 - loss: 0.2992 - val_accuracy: 0.9129 - val_loss: 0.3663\n",
            "Epoch 71/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9219 - loss: 0.3173 - val_accuracy: 0.9200 - val_loss: 0.3360\n",
            "Epoch 72/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9246 - loss: 0.3059 - val_accuracy: 0.9057 - val_loss: 0.3382\n",
            "Epoch 73/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9262 - loss: 0.3143 - val_accuracy: 0.9136 - val_loss: 0.3233\n",
            "Epoch 74/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9274 - loss: 0.2977 - val_accuracy: 0.9114 - val_loss: 0.3352\n",
            "Epoch 75/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9291 - loss: 0.2899 - val_accuracy: 0.9057 - val_loss: 0.3609\n",
            "Epoch 76/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9182 - loss: 0.3137 - val_accuracy: 0.9121 - val_loss: 0.3324\n",
            "Epoch 77/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9221 - loss: 0.3075 - val_accuracy: 0.9207 - val_loss: 0.3240\n",
            "Epoch 78/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9255 - loss: 0.3082 - val_accuracy: 0.8857 - val_loss: 0.3961\n",
            "Epoch 79/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9136 - loss: 0.3371 - val_accuracy: 0.8893 - val_loss: 0.3895\n",
            "Epoch 80/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9262 - loss: 0.3081 - val_accuracy: 0.9107 - val_loss: 0.3587\n",
            "Epoch 81/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9290 - loss: 0.2942 - val_accuracy: 0.9043 - val_loss: 0.3419\n",
            "Epoch 82/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9232 - loss: 0.3061 - val_accuracy: 0.9064 - val_loss: 0.3539\n",
            "Epoch 83/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9236 - loss: 0.3042 - val_accuracy: 0.9000 - val_loss: 0.3586\n",
            "Epoch 84/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9323 - loss: 0.3008 - val_accuracy: 0.9071 - val_loss: 0.3426\n",
            "Epoch 85/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9202 - loss: 0.3038 - val_accuracy: 0.9136 - val_loss: 0.3404\n",
            "Epoch 86/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9316 - loss: 0.2946 - val_accuracy: 0.9079 - val_loss: 0.3576\n",
            "Epoch 87/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9277 - loss: 0.2995 - val_accuracy: 0.8764 - val_loss: 0.4221\n",
            "Epoch 88/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9207 - loss: 0.3208 - val_accuracy: 0.8664 - val_loss: 0.4453\n",
            "Epoch 89/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9167 - loss: 0.3220 - val_accuracy: 0.9100 - val_loss: 0.3550\n",
            "Epoch 90/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9234 - loss: 0.3063 - val_accuracy: 0.9243 - val_loss: 0.3365\n",
            "Epoch 91/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9216 - loss: 0.3080 - val_accuracy: 0.9057 - val_loss: 0.3516\n",
            "Epoch 92/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9348 - loss: 0.2909 - val_accuracy: 0.9293 - val_loss: 0.3269\n",
            "Epoch 93/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9266 - loss: 0.3072 - val_accuracy: 0.8993 - val_loss: 0.3612\n",
            "Epoch 94/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9278 - loss: 0.3021 - val_accuracy: 0.9050 - val_loss: 0.3399\n",
            "Epoch 95/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9249 - loss: 0.3033 - val_accuracy: 0.9050 - val_loss: 0.3443\n",
            "Epoch 96/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9277 - loss: 0.3002 - val_accuracy: 0.9200 - val_loss: 0.3219\n",
            "Epoch 97/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9291 - loss: 0.2927 - val_accuracy: 0.9164 - val_loss: 0.3214\n",
            "Epoch 98/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9291 - loss: 0.2930 - val_accuracy: 0.9221 - val_loss: 0.3115\n",
            "Epoch 99/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9339 - loss: 0.2842 - val_accuracy: 0.9029 - val_loss: 0.3695\n",
            "Epoch 100/100\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.2864 - val_accuracy: 0.9029 - val_loss: 0.3514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vizualize Training History**"
      ],
      "metadata": {
        "id": "17dCr9zgAnQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "BpMKkmkDmweF",
        "outputId": "8c165087-f5e4-4ea1-bce1-0ae85d34f8e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Set Classification Accuracy of FP32 Model**"
      ],
      "metadata": {
        "id": "egHE_K6vArh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the FP32 model\n",
        "results = model.evaluate(test_dataset)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IElgrf-TJJPC",
        "outputId": "714f33a1-f7a5-4e2a-b67a-6a3f3e05c0ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9401 - loss: 0.2693 \n",
            "Test Loss: 0.3514212369918823, Test Accuracy: 0.9028571248054504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to TFLite Model**"
      ],
      "metadata": {
        "id": "tdyvZiHWA-A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "# Print tensor details\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "for tensor in interpreter.get_tensor_details():\n",
        "    print(tensor['name'], tensor['dtype'])\n",
        "    try:\n",
        "        # Attempt to get tensor data\n",
        "        tensor_data = interpreter.get_tensor(tensor['index'])\n",
        "        print(tensor_data)\n",
        "    except ValueError:\n",
        "        # Skip tensors with null data\n",
        "        print(f\"Skipping tensor '{tensor['name']}' as it has null data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC6reRpb_00G",
        "outputId": "8157a5bd-c82a-423c-cb6f-48d956b5c8f4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpdsn6crev'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 33), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  133676050400656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242089936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242088016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242087824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242089744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242090896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "serving_default_keras_tensor:0 <class 'numpy.float32'>\n",
            "[[0.00e+00 0.00e+00 1.42e-43 0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00\n",
            "  1.54e-44 1.54e-44 0.00e+00 1.54e-44 2.24e-43 1.40e-45 0.00e+00 0.00e+00\n",
            "  0.00e+00 0.00e+00 1.54e-44 1.54e-44 0.00e+00 3.92e-44 2.33e-43 0.00e+00\n",
            "  0.00e+00 0.00e+00 0.00e+00 0.00e+00 1.54e-44 1.54e-44 0.00e+00 4.20e-44\n",
            "  2.40e-43]]\n",
            "arith.constant <class 'numpy.float32'>\n",
            "[[ 6.86709702e-01  2.03132129e+00 -3.98875117e-01 -8.35544288e-01\n",
            "   1.86980218e-02  6.00831926e-01  1.50704399e-01  1.07325325e-33\n",
            "   1.79650947e-01  7.00317244e-34]\n",
            " [ 2.67617106e-01 -2.61733681e-01 -5.90847665e-03 -1.48524654e+00\n",
            "   2.03599468e-01  4.40812171e-01  9.19794261e-01 -3.90978008e-34\n",
            "   8.16369236e-01 -6.20249502e-34]\n",
            " [ 9.02868882e-02 -9.62362587e-01 -1.99880147e+00  1.27794421e+00\n",
            "  -1.22182004e-01 -1.99540198e-01 -1.07978618e+00  9.42717909e-34\n",
            "  -8.96425962e-01  6.57641886e-34]\n",
            " [-8.92911315e-01 -3.01642530e-03  1.18759143e+00 -9.70978439e-02\n",
            "  -4.19597588e-02  9.43635225e-01  1.24744438e-01  1.11601884e-33\n",
            "   4.56657400e-03 -4.20331479e-34]\n",
            " [-1.54571652e-01 -1.02283931e+00  1.09996843e+00  9.37152207e-01\n",
            "  -1.28804892e-02 -1.97126901e+00 -1.09366283e-01 -5.72791994e-34\n",
            "  -7.59391040e-02 -5.65893817e-34]]\n",
            "arith.constant1 <class 'numpy.float32'>\n",
            "[[-2.54457563e-34  1.15910964e-34  1.75909791e-34  4.78566103e-02\n",
            "   1.98881179e-01  4.69199029e-34  4.06902909e-01  2.77111530e-01\n",
            "  -1.73265398e-01  4.39485012e-34 -2.03046170e-34 -9.34177160e-01\n",
            "  -3.46118808e-01 -5.92765710e-34  7.16253102e-01 -1.18212469e-01\n",
            "  -2.30524689e-02 -1.17941573e-35  1.29095177e-34  3.44354481e-34]\n",
            " [-2.24066856e-34 -3.63780088e-35 -3.54438294e-35  6.07634068e-01\n",
            "   7.60299742e-01 -4.50273111e-34  3.51687670e-01  2.78376222e-01\n",
            "   9.11703557e-02  8.59352859e-34  3.52830369e-34 -8.11399341e-01\n",
            "  -1.30306077e+00  4.88024525e-34 -1.05746150e+00 -1.43989909e+00\n",
            "  -1.25810385e-01 -9.84031668e-34 -3.08970632e-34  1.12095178e-33]\n",
            " [-6.68994728e-34 -5.46644271e-34  4.56879389e-34  5.92355311e-01\n",
            "  -3.49398315e-01 -7.07625375e-34 -9.04225945e-01 -9.67441022e-01\n",
            "   6.21403813e-01  5.65205189e-34  3.36410458e-34  4.61051822e-01\n",
            "   1.93248594e+00  3.32188275e-35 -4.73373681e-01 -9.62801039e-01\n",
            "  -1.94876432e-01 -6.55977276e-35  5.15705489e-34 -5.09069640e-35]\n",
            " [-8.34799537e-34 -2.21791310e-34 -6.72240975e-34  5.81821382e-01\n",
            "   8.42139572e-02  3.62076711e-34 -5.26220739e-01 -4.19413596e-01\n",
            "   3.44323933e-01 -7.80305089e-34 -6.88442593e-34  7.97109365e-01\n",
            "   9.53248262e-01  9.37259666e-34  1.52031767e+00  9.14023399e-01\n",
            "  -3.67011912e-02 -6.79940371e-34  2.72080772e-34  1.15352335e-33]\n",
            " [ 1.14447269e-33  8.47953136e-34 -3.88353028e-34 -1.50523961e-01\n",
            "  -6.55269548e-02 -1.70125400e-35  6.81203902e-02  5.90121634e-02\n",
            "  -5.51809557e-02  6.07390604e-34  6.18749875e-34 -9.15693939e-02\n",
            "  -2.68794131e-02 -8.43479002e-34 -1.00655884e-01  1.72885023e-02\n",
            "   2.94202920e-02  1.09996480e-33  7.15001464e-34  1.13858475e-33]\n",
            " [-1.04146715e-33 -2.54497994e-34  5.81117571e-34  1.34176612e-01\n",
            "   7.63013065e-01  1.29354934e-34  6.50319099e-01  8.29114377e-01\n",
            "   1.50208786e-01  1.06368923e-33 -2.41612578e-34  7.22410560e-01\n",
            "  -1.10521758e+00 -8.11181101e-34 -1.54211164e+00 -2.98194647e-01\n",
            "   1.45723060e-01  1.20950728e-33  7.86059501e-34 -7.49044378e-34]\n",
            " [ 6.22207160e-34  8.46136354e-34 -1.03366508e-33 -4.77740675e-01\n",
            "  -6.65746033e-02 -6.38893669e-34  5.90488911e-01  5.70567906e-01\n",
            "  -2.91408867e-01  3.35411058e-34 -7.56283724e-34 -5.43040574e-01\n",
            "  -3.12887520e-01 -9.65153137e-34 -7.76524365e-01  1.85232405e-02\n",
            "   2.02887058e-01  1.69629712e-34 -3.62828178e-34 -9.40315033e-34]\n",
            " [ 4.21142892e-34 -1.62423611e-34  1.05386512e-33 -2.90024785e-34\n",
            "  -6.25904916e-34 -5.77043014e-34 -5.15529165e-34 -3.61927295e-34\n",
            "  -3.39449478e-34  4.87924241e-34 -7.18849418e-34  8.14886847e-34\n",
            "   2.84054422e-34 -6.69711412e-34 -3.75610187e-34  2.83393275e-34\n",
            "   2.26635495e-34 -1.05181260e-33 -9.93250023e-35 -1.03123006e-33]\n",
            " [ 7.68225048e-35  6.09973661e-34 -3.22241297e-34 -5.18805563e-01\n",
            "  -1.55164510e-01  4.39090257e-34  5.26246667e-01  4.79154050e-01\n",
            "  -2.64010966e-01 -4.01647410e-34  1.91507132e-34 -4.73842293e-01\n",
            "  -2.11270854e-01  8.06466726e-34 -5.59717774e-01  1.64009165e-02\n",
            "   1.55561641e-01  2.86419025e-34  9.75225837e-34 -9.30103385e-34]\n",
            " [-3.61506206e-34 -6.58740606e-34 -3.12878393e-34 -5.21016198e-34\n",
            "  -9.32851941e-35 -1.27357168e-34 -4.13558474e-34 -6.45012668e-34\n",
            "   7.29343735e-34  8.76516179e-34 -1.18236052e-33  5.29789564e-34\n",
            "   1.09133373e-33 -1.35552774e-34  2.83513855e-34 -4.92658315e-34\n",
            "   9.01969500e-36  5.48350023e-34  2.26444431e-34 -5.54327596e-34]]\n",
            "arith.constant2 <class 'numpy.float32'>\n",
            "[ 0.29175663 -1.0055509   0.6255043  -0.3187342   0.22700602]\n",
            "sequential_1/dense_1_2/BiasAdd/ReadVariableOp <class 'numpy.float32'>\n",
            "[ 0.6207859   0.12949544 -0.3386793   0.70000386  0.47513565  0.47803324\n",
            "  1.6216453  -0.03577318  1.6583537  -0.05671048]\n",
            "sequential_1/dense_1/BiasAdd/ReadVariableOp <class 'numpy.float32'>\n",
            "[-0.04025335 -0.02167198 -0.04622271 -0.21552166  0.02714156 -0.00949123\n",
            "  2.4666789   2.256282   -0.0686807  -0.03608859 -0.03238448 -0.13793938\n",
            " -0.13021308 -0.04432531  0.6242737   0.9964248   0.7033693  -0.04627901\n",
            " -0.07980074 -0.04127782]\n",
            "sequential_1/dense_1/MatMul <class 'numpy.float32'>\n",
            "[[ 8.09264953e-34 -8.85284357e-34 -8.05407036e-34 -9.58776355e-34\n",
            "   1.20640370e-33  8.70407925e-34 -8.47349960e-34  5.93622673e-34\n",
            "   1.31521287e-35  1.43431249e-34  9.35677891e-34 -9.06384388e-34\n",
            "  -9.80735324e-34 -8.67095602e-34 -6.91769426e-34 -8.82787625e-34\n",
            "   1.78705172e-34  8.12095966e-34  2.83036081e-34  7.48447998e-34\n",
            "   9.92444809e-34 -2.03727497e-34  4.64274075e-34 -4.01944406e-34\n",
            "  -3.80965092e-34  1.19939325e-33 -1.55246753e-11 -8.62777589e-34\n",
            "   1.14625411e-33  9.97319254e-35  9.43582540e-34 -9.56493049e-34\n",
            "   6.17880514e-34]\n",
            " [-1.11534910e-34  7.28328609e-35  3.46788190e-34  3.13627724e-34\n",
            "   5.05800296e-35  1.31949264e-34  5.16802739e-34  5.91100916e-34\n",
            "   6.97689417e-34  6.15343788e-34  6.03360932e-35 -4.59158609e-34\n",
            "   6.60165479e-34  7.37422182e-34  6.60234080e-36 -3.37535139e-35\n",
            "  -1.21458608e-34 -1.96236706e-34  2.01658949e-34  2.14320492e-34\n",
            "   3.50376754e-34 -2.19832022e-34 -5.94528951e-34  3.29559622e-34\n",
            "  -6.85668266e-35 -3.90205717e-34 -4.09125672e-35 -4.44717109e-34\n",
            "  -1.66405804e-35  4.52764287e-34 -4.23640266e-34  2.03905084e-34\n",
            "   1.58768328e-34]\n",
            " [ 1.91962819e-35  8.97955267e-34  6.77467149e-34  5.51789905e-34\n",
            "   6.92858962e-34  3.77311646e-35  1.02343993e-33  5.58351000e-34\n",
            "  -5.78792067e-34  8.92363771e-34 -6.24031013e-34 -7.27513913e-35\n",
            "  -4.33088853e-34  1.01997865e-33  2.15021473e-34 -6.94712937e-34\n",
            "  -7.70459956e-34  9.44707341e-34  5.91859018e-34 -1.94982508e-34\n",
            "  -1.01238386e-33 -2.78449357e-34  8.15455309e-34 -9.75527242e-34\n",
            "   4.58919056e-34  9.41919215e-34 -9.00599487e-34 -8.52810774e-34\n",
            "  -4.56312213e-34  6.61550405e-34 -4.83252707e-34 -4.07087469e-34\n",
            "   1.71249782e-34]\n",
            " [ 2.59548753e-01 -4.85194698e-02  1.37042791e-01  3.07773292e-01\n",
            "   2.48985276e-01  9.75491703e-02  8.14333782e-02  8.34735036e-02\n",
            "   2.55901203e-03 -1.53108388e-02  1.40151039e-01  2.59241402e-01\n",
            "  -1.46813951e-02  9.66408178e-02  2.96096474e-01  2.83071607e-01\n",
            "   1.14767060e-01  6.44896030e-02 -7.45024160e-03 -2.46084179e-03\n",
            "  -4.00517061e-02  1.23603337e-01  1.17898345e-01 -1.17257856e-01\n",
            "  -4.32929993e-01  5.11469543e-01  4.00964737e-01  3.08181912e-01\n",
            "   2.52943277e-01  1.08524971e-01  8.29598606e-02  1.52709812e-01\n",
            "   1.58096924e-01]\n",
            " [-2.97689438e-01  2.05555316e-02  2.81304002e-01 -1.60529912e-01\n",
            "   5.37020683e-01  9.11322981e-02  1.82800472e-01  8.68985578e-02\n",
            "   1.52175739e-01  3.66793275e-02 -8.88773873e-02 -3.23797017e-01\n",
            "  -3.06489199e-01  2.05549821e-01 -5.36072314e-01  3.96526931e-03\n",
            "   2.37279713e-01  3.30550581e-01  3.74878883e-01  2.93062866e-01\n",
            "   3.11646074e-01  3.15460235e-01 -2.81139970e-01 -8.00081715e-02\n",
            "  -4.59233709e-02 -3.17075551e-01  2.85117716e-01  3.77351612e-01\n",
            "   2.26099342e-01  9.55464840e-02  1.02832861e-01  3.08268249e-01\n",
            "  -3.12139234e-03]\n",
            " [-1.22031352e-34 -2.88893854e-34 -9.54467066e-34 -8.25949167e-35\n",
            "  -4.94683379e-34 -7.33146368e-34 -1.16524900e-33 -1.16717148e-33\n",
            "  -8.21495421e-34 -9.12830873e-34 -8.39887775e-34  2.62097358e-34\n",
            "  -4.00920073e-34 -1.16457557e-34  8.56027243e-35 -8.01461634e-35\n",
            "  -1.75519490e-34 -5.28873643e-34 -7.43630859e-34 -1.04676652e-33\n",
            "  -9.23308293e-34 -1.05858152e-33 -5.15902063e-34  3.20010819e-34\n",
            "  -8.62382604e-34  3.61549748e-35 -3.41957620e-34 -9.06826668e-34\n",
            "  -1.07460792e-33 -8.33963283e-34 -7.15220859e-34 -6.77170888e-34\n",
            "  -6.27096711e-34]\n",
            " [-3.02124023e-01  3.88300508e-01  2.39373520e-01 -4.57830787e-01\n",
            "   1.00266263e-01 -3.38989981e-02  3.27390507e-02 -1.77037224e-01\n",
            "  -2.59636462e-01 -2.29724161e-02 -5.91797717e-02 -1.95181873e-02\n",
            "   7.15932772e-02  7.90290534e-02  3.91616113e-02  6.22957237e-02\n",
            "   2.00432492e-03 -1.18696712e-01  4.41092812e-03 -2.17772439e-01\n",
            "  -7.62536153e-02  6.08267961e-03 -4.23497617e-01  1.58624455e-01\n",
            "   2.19450086e-01 -5.10376215e-01 -2.01231048e-01 -2.94809967e-01\n",
            "  -2.50047058e-01 -2.98679054e-01 -4.85322773e-01 -2.41399646e-01\n",
            "  -4.01322603e-01]\n",
            " [-3.75254840e-01  5.33429384e-01  1.02831662e-01 -5.02936900e-01\n",
            "   1.17951483e-01 -2.16981862e-02  6.57221749e-02 -2.21104383e-01\n",
            "  -2.98093945e-01 -1.04718983e-01 -1.23391680e-01 -2.84561545e-01\n",
            "   1.41845807e-01  9.04792994e-02 -1.06211174e-02  8.45230892e-02\n",
            "   9.78512540e-02 -4.33454067e-02  6.32210821e-02 -1.82189137e-01\n",
            "  -4.25677449e-02  1.40827708e-03 -3.77820432e-01  2.42020398e-01\n",
            "   1.42334774e-01 -4.59288716e-01 -1.38426870e-01 -2.48292938e-01\n",
            "  -2.57277757e-01 -2.54994094e-01 -4.82874244e-01 -2.13569313e-01\n",
            "  -3.56890798e-01]\n",
            " [ 1.70283705e-01 -5.52023984e-02  8.92939940e-02  1.69155270e-01\n",
            "  -2.04362929e-01  1.27861172e-01  8.89334083e-02  6.80671483e-02\n",
            "  -2.19321381e-02  7.82253817e-02  6.71640038e-02  3.01229328e-01\n",
            "   1.67559624e-01  7.67321661e-02  6.22451127e-01  1.62589297e-01\n",
            "  -2.03158408e-02 -2.00135618e-01 -2.72333443e-01 -3.26441079e-01\n",
            "  -2.90281028e-01 -2.16034263e-01  2.18004752e-02 -1.48255318e-01\n",
            "  -2.15656251e-01  4.79960978e-01  2.27878794e-01  1.14728391e-01\n",
            "   2.03450307e-01  9.32174362e-03 -1.43261805e-01 -7.35139698e-02\n",
            "   1.47244096e-01]\n",
            " [-4.01500197e-34 -7.67300815e-34 -8.82904623e-35  7.70594220e-34\n",
            "  -8.68696203e-34 -1.15224748e-33  8.06696223e-34  6.02567588e-35\n",
            "   2.51551199e-35  7.26190196e-34  9.23951784e-34  1.66133351e-34\n",
            "   1.25568143e-34  8.19513519e-34  1.15651159e-33 -3.45181712e-34\n",
            "   6.83477874e-34 -4.20395902e-34  8.81633620e-35  2.50349256e-34\n",
            "   1.54582466e-34  1.08909552e-33  1.88624415e-34  8.77107416e-34\n",
            "   6.30529751e-34  9.46051078e-34  8.93798517e-34 -6.42749107e-34\n",
            "   9.30852487e-34  2.51278172e-34 -7.87018355e-34  1.09415639e-33\n",
            "   9.31689843e-34]\n",
            " [ 8.06108843e-34  4.88810086e-34 -9.39401637e-34  1.07177247e-34\n",
            "   6.22158441e-34 -2.83537250e-34  1.85658014e-34  4.70267260e-34\n",
            "   1.10135399e-33  6.16895854e-34 -5.27045611e-34  5.76221912e-34\n",
            "   1.10255851e-33  1.42504423e-34 -3.07950936e-34 -4.55557831e-34\n",
            "   6.50711842e-34 -1.10580857e-33 -2.47870019e-34  7.65783716e-35\n",
            "   3.25266037e-34 -1.32702900e-34 -5.64495622e-34 -1.26706456e-34\n",
            "   9.17375352e-34 -3.51075479e-35  3.92566808e-34 -5.40787645e-34\n",
            "  -5.26237092e-34  3.48785291e-34  4.78455221e-34  8.33363230e-34\n",
            "   1.15706186e-33]\n",
            " [-1.54566914e-01 -1.80957153e-01 -1.25463769e-01  3.89538020e-01\n",
            "  -6.07250094e-01  3.06506842e-01  3.51792663e-01  5.37965834e-01\n",
            "   3.87963235e-01  4.16382849e-01  2.59570390e-01 -2.58813202e-01\n",
            "   6.51954651e-01  2.39396453e-01  9.61078763e-01  1.52445227e-01\n",
            "  -3.77227180e-02 -3.89321357e-01 -6.40858591e-01 -6.22226000e-01\n",
            "  -5.73845148e-01 -7.64772654e-01 -2.87378550e-01 -1.31585345e-01\n",
            "  -2.39074707e-01  7.26001740e-01  1.40814707e-01  4.02399302e-02\n",
            "   3.93023163e-01 -1.29286364e-01 -2.15269893e-01 -2.68720329e-01\n",
            "   1.45299926e-01]\n",
            " [ 1.09970021e+00  1.81166783e-01 -5.95778167e-01  5.52754521e-01\n",
            "  -5.53426266e-01  7.67011270e-02 -8.05906504e-02  5.56099191e-02\n",
            "  -1.50118366e-01 -1.69828609e-01  1.71483066e-02  1.82116246e+00\n",
            "   1.20118506e-01 -4.85474020e-02  1.13222313e+00  2.40625516e-01\n",
            "  -2.03079343e-01 -2.70017058e-01 -3.25111121e-01 -1.31263003e-01\n",
            "  -2.02514008e-01 -3.99282128e-01  8.16811800e-01  2.99619794e-01\n",
            "  -8.34137082e-01  9.61669326e-01 -6.32544756e-02 -3.52620304e-01\n",
            "  -1.43623471e-01 -2.91404724e-01 -2.57397443e-01 -3.18029583e-01\n",
            "   2.35436946e-01]\n",
            " [ 2.62150026e-34 -5.23184870e-35 -8.40550919e-34  3.31261425e-34\n",
            "  -5.63058580e-34 -6.53264547e-34 -8.25556019e-34  2.23396066e-34\n",
            "   1.04286296e-33 -2.17755920e-34  4.01591757e-34 -1.15951571e-33\n",
            "  -5.33584988e-34 -9.15446899e-34  6.55551526e-34 -6.90158126e-34\n",
            "   3.66196245e-34  1.54240092e-34  1.06957276e-33 -2.60708140e-34\n",
            "   1.13055851e-33  7.58258279e-34 -7.28384513e-34 -4.56795911e-34\n",
            "  -8.42161714e-34  5.82099614e-34 -8.84182055e-34  2.84801021e-34\n",
            "  -6.88910678e-34 -1.05530419e-33  1.00380918e-33 -7.05163725e-34\n",
            "   4.88635828e-34]\n",
            " [ 1.01281834e+00 -2.29713634e-01 -1.47273874e+00  9.32137132e-01\n",
            "   3.79386917e-02 -4.32875812e-01 -5.66274047e-01 -2.88415670e-01\n",
            "  -3.94517750e-01 -5.72621107e-01 -7.57704899e-02  4.17225718e-01\n",
            "   4.23890263e-01 -9.79144633e-01  5.03340244e-01  3.32494378e-01\n",
            "   9.08636898e-02  4.82485024e-03 -3.69763404e-01 -2.69751588e-04\n",
            "  -1.31140083e-01 -2.79604077e-01  4.16979417e-02 -1.49590746e-01\n",
            "  -1.14661431e+00  1.01900148e+00  1.32673487e-01 -2.02628329e-01\n",
            "  -1.92599401e-01 -3.68625224e-01  1.95582986e-01 -2.07129136e-01\n",
            "  -2.55252957e-01]\n",
            " [ 3.51319402e-01 -1.12369776e-01 -1.23703444e+00  5.64687312e-01\n",
            "  -4.09871936e-01 -9.14541706e-02 -1.16849244e-01  2.40880474e-01\n",
            "   3.78637791e-01  1.51077271e-01  2.13141233e-01 -1.12310624e+00\n",
            "   8.58873010e-01 -1.55337363e-01 -1.34659365e-01 -2.29229838e-01\n",
            "  -3.50771025e-02  4.65058256e-03 -2.11493149e-01  2.30091661e-01\n",
            "  -4.41312417e-02 -4.18518573e-01 -4.15486813e-01  6.48058951e-01\n",
            "   8.56494457e-02  1.61738455e-01 -5.73887706e-01 -2.79156238e-01\n",
            "  -1.30357355e-01 -7.22171217e-02  1.84529975e-01 -3.92257631e-01\n",
            "  -2.34626919e-01]\n",
            " [-1.45829692e-01  6.24346435e-02 -2.91609000e-02 -1.13740683e-01\n",
            "  -8.68553370e-02 -2.46307179e-02  1.14863450e-02  8.31639394e-04\n",
            "   1.48960352e-02 -3.46499309e-03 -7.07607567e-02 -1.90100297e-01\n",
            "   6.65923432e-02  9.78749804e-03 -1.55981764e-01 -1.10206500e-01\n",
            "  -3.23677831e-03 -1.81024894e-03 -6.46723015e-03  1.22553511e-02\n",
            "  -3.24167800e-03 -5.89904450e-02 -1.03830189e-01  1.38316795e-01\n",
            "   1.60657540e-01 -2.10446477e-01 -1.36020958e-01 -8.76414180e-02\n",
            "  -7.57806823e-02 -4.53786217e-02 -3.65509838e-02 -6.81125298e-02\n",
            "  -8.44521821e-02]\n",
            " [ 8.94049779e-34 -9.31454836e-35 -9.16399600e-34 -1.08705805e-33\n",
            "  -3.36741938e-35  3.25960612e-34  3.64632562e-34  3.07231520e-35\n",
            "   8.28060832e-34  4.84719641e-34 -1.15587939e-33 -4.64928771e-34\n",
            "   1.06571383e-33 -1.14126359e-33 -3.96232284e-35 -6.54007404e-35\n",
            "  -8.49980129e-34  4.31985266e-34  3.11245741e-34 -7.61871868e-34\n",
            "   7.84756355e-34  1.10249946e-33  6.83749983e-34  5.20495031e-34\n",
            "   2.15795141e-34 -8.58484279e-34 -3.57282141e-34  2.40682239e-34\n",
            "   7.57686511e-34 -3.55600357e-35 -2.72960258e-34  5.32338229e-36\n",
            "   8.59948229e-34]\n",
            " [-2.03158002e-34  3.63726192e-34  1.05839197e-33 -5.15597904e-34\n",
            "   8.90441746e-34  5.03543576e-34 -5.81238013e-34 -5.40901062e-34\n",
            "  -4.68800785e-34 -5.26742830e-34 -8.22668803e-34 -5.11826174e-34\n",
            "   4.71113570e-34 -1.08508690e-34  7.31097793e-34  8.95935621e-34\n",
            "   6.31458071e-34  4.97455480e-34  5.11569402e-34 -4.96748484e-34\n",
            "  -5.33453433e-34  7.74688705e-34 -3.75251340e-34  1.01383596e-33\n",
            "  -1.94502025e-34  3.98726811e-34  7.68296220e-34 -6.54098184e-34\n",
            "   3.85548693e-34  5.47699047e-34 -4.47181148e-34 -1.52615465e-34\n",
            "  -1.09112379e-33]\n",
            " [-7.89730717e-34  6.68495648e-34 -9.50914135e-34 -8.42185958e-34\n",
            "   1.05541834e-33  2.66416611e-34 -8.42789226e-34  5.86563049e-34\n",
            "  -3.19277605e-34 -3.04203612e-34  7.04033138e-34 -2.89420301e-34\n",
            "   6.22620373e-34 -1.18576431e-34  1.13715762e-33  4.40165788e-34\n",
            "  -4.64164056e-34  3.94546092e-34 -1.10920033e-33  4.20439340e-34\n",
            "  -1.77985250e-34  1.51547395e-34 -3.07065963e-34 -2.47142728e-34\n",
            "  -7.19100266e-34  1.08843357e-33  4.13240447e-35 -9.59210370e-34\n",
            "   7.39597214e-34  2.97432144e-34  6.87567263e-34 -7.51587303e-34\n",
            "   6.82497392e-34]]\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd <class 'numpy.float32'>\n",
            "Skipping tensor 'sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd' as it has null data.\n",
            "sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd <class 'numpy.float32'>\n",
            "Skipping tensor 'sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd' as it has null data.\n",
            "StatefulPartitionedCall_1:0 <class 'numpy.float32'>\n",
            "[[1.5e-44 1.5e-44 0.0e+00 4.2e-44 1.4e-43]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to Quantized Model**"
      ],
      "metadata": {
        "id": "C1gaSy-8xaMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide a representative dataset to guide the quantization process\n",
        "def representative_dataset_gen():\n",
        "    for data, _ in test_dataset.unbatch().batch(1).take(100):\n",
        "        yield [data]\n",
        "\n",
        "# Convert the model to int8 format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "converter.experimental_new_quantizer = False  # Optional: Use the default quantizer\n",
        "converter._experimental_disable_per_channel = True\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('model_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "# Print tensor details\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "for tensor in interpreter.get_tensor_details():\n",
        "    print(tensor['name'], tensor['dtype'])\n",
        "    try:\n",
        "        # Attempt to get tensor data\n",
        "        tensor_data = interpreter.get_tensor(tensor['index'])\n",
        "        print(tensor_data)\n",
        "    except ValueError:\n",
        "        # Skip tensors with null data\n",
        "        print(f\"Skipping tensor '{tensor['name']}' as it has null data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu50RF1HBJZ_",
        "outputId": "a80178e6-0b44-41cf-a4f2-eb5af87c30ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpsvbxh2f3'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 33), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  133676050400656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242089936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242088016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242087824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242089744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133673242090896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "serving_default_keras_tensor:0_int8 <class 'numpy.int8'>\n",
            "[[ 0  0  0  0  0  0  0  0 65  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   4  0  0  0  0  0  0  0  0]]\n",
            "arith.constant <class 'numpy.int8'>\n",
            "[[  43  127  -25  -52    1   38    9    0   11    0]\n",
            " [  17  -16    0  -93   13   28   58    0   51    0]\n",
            " [   6  -60 -125   80   -8  -12  -68    0  -56    0]\n",
            " [ -56    0   74   -6   -3   59    8    0    0    0]\n",
            " [ -10  -64   69   59   -1 -123   -7    0   -5    0]]\n",
            "arith.constant1 <class 'numpy.int8'>\n",
            "[[   0    0    0    3   13    0   27   18  -11    0    0  -61  -23    0\n",
            "    47   -8   -2    0    0    0]\n",
            " [   0    0    0   40   50    0   23   18    6    0    0  -53  -86    0\n",
            "   -69  -95   -8    0    0    0]\n",
            " [   0    0    0   39  -23    0  -59  -64   41    0    0   30  127    0\n",
            "   -31  -63  -13    0    0    0]\n",
            " [   0    0    0   38    6    0  -35  -28   23    0    0   52   63    0\n",
            "   100   60   -2    0    0    0]\n",
            " [   0    0    0  -10   -4    0    4    4   -4    0    0   -6   -2    0\n",
            "    -7    1    2    0    0    0]\n",
            " [   0    0    0    9   50    0   43   54   10    0    0   47  -73    0\n",
            "  -101  -20   10    0    0    0]\n",
            " [   0    0    0  -31   -4    0   39   37  -19    0    0  -36  -21    0\n",
            "   -51    1   13    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0]\n",
            " [   0    0    0  -34  -10    0   35   31  -17    0    0  -31  -14    0\n",
            "   -37    1   10    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0]]\n",
            "arith.constant2 <class 'numpy.int32'>\n",
            "[ 1055 -3636  2262 -1153   821]\n",
            "sequential_1/dense_1_2/BiasAdd/ReadVariableOp <class 'numpy.int32'>\n",
            "[ 4109   857 -2242  4634  3145  3164 10734  -237 10977  -375]\n",
            "sequential_1/dense_1/BiasAdd/ReadVariableOp <class 'numpy.int32'>\n",
            "[ -794  -427  -911 -4249   535  -187 48632 44484 -1354  -712  -638 -2720\n",
            " -2567  -874 12308 19645 13867  -912 -1573  -814]\n",
            "sequential_1/dense_1/MatMul <class 'numpy.int8'>\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [  18   -3   10   21   17    7    6    6    0   -1   10   18   -1    7\n",
            "    21   20    8    4   -1    0   -3    9    8   -8  -30   36   28   21\n",
            "    18    8    6   11   11]\n",
            " [ -21    1   20  -11   37    6   13    6   11    3   -6  -23  -21   14\n",
            "   -37    0   17   23   26   20   22   22  -20   -6   -3  -22   20   26\n",
            "    16    7    7   21    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [ -21   27   17  -32    7   -2    2  -12  -18   -2   -4   -1    5    6\n",
            "     3    4    0   -8    0  -15   -5    0  -30   11   15  -36  -14  -21\n",
            "   -17  -21  -34  -17  -28]\n",
            " [ -26   37    7  -35    8   -2    5  -15  -21   -7   -9  -20   10    6\n",
            "    -1    6    7   -3    4  -13   -3    0  -26   17   10  -32  -10  -17\n",
            "   -18  -18  -34  -15  -25]\n",
            " [  12   -4    6   12  -14    9    6    5   -2    5    5   21   12    5\n",
            "    43   11   -1  -14  -19  -23  -20  -15    2  -10  -15   33   16    8\n",
            "    14    1  -10   -5   10]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [ -11  -13   -9   27  -42   21   25   38   27   29   18  -18   45   17\n",
            "    67   11   -3  -27  -45  -43  -40  -53  -20   -9  -17   51   10    3\n",
            "    27   -9  -15  -19   10]\n",
            " [  77   13  -42   39  -39    5   -6    4  -10  -12    1  127    8   -3\n",
            "    79   17  -14  -19  -23   -9  -14  -28   57   21  -58   67   -4  -25\n",
            "   -10  -20  -18  -22   16]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [  71  -16 -103   65    3  -30  -39  -20  -28  -40   -5   29   30  -68\n",
            "    35   23    6    0  -26    0   -9  -19    3  -10  -80   71    9  -14\n",
            "   -13  -26   14  -14  -18]\n",
            " [  24   -8  -86   39  -29   -6   -8   17   26   11   15  -78   60  -11\n",
            "    -9  -16   -2    0  -15   16   -3  -29  -29   45    6   11  -40  -19\n",
            "    -9   -5   13  -27  -16]\n",
            " [ -10    4   -2   -8   -6   -2    1    0    1    0   -5  -13    5    1\n",
            "   -11   -8    0    0    0    1    0   -4   -7   10   11  -15   -9   -6\n",
            "    -5   -3   -3   -5   -6]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd <class 'numpy.int8'>\n",
            "Skipping tensor 'sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd' as it has null data.\n",
            "sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd <class 'numpy.int8'>\n",
            "Skipping tensor 'sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd' as it has null data.\n",
            "StatefulPartitionedCall_1:0_int8 <class 'numpy.int8'>\n",
            "[[0 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Set Classification Accuracy of 8-bit Model**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SZZ-Oth4nJOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test accuracy of quantized model\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "correct = 0.0\n",
        "total = 0.0\n",
        "\n",
        "# Define the transformation function\n",
        "def convert_to_int8(X, y):\n",
        "    X = tf.cast((X*255)-128, tf.int8)  # Multiply by 255 and subtract 128 cast to int8\n",
        "    return X, y\n",
        "\n",
        "# Apply the transformation to the dataset\n",
        "test_dataset_quantized_inputs = test_dataset.map(convert_to_int8).unbatch()\n",
        "\n",
        "# Example: Inspect the first batch\n",
        "for X_batch, y_batch in test_dataset_quantized_inputs.take(1):\n",
        "    print(X_batch.numpy(), y_batch.numpy())\n",
        "for input, label in test_dataset_quantized_inputs.batch(1):\n",
        "    interpreter.set_tensor(input_details[0]['index'], input.numpy().astype('int8'))\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    total += 1\n",
        "    if output.argmax() == label.numpy()[0]:\n",
        "        correct += 1\n",
        "\n",
        "print(f\"Quantized Model Test Set Accuracy: {correct / total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSPokUuAfoOQ",
        "outputId": "48a90d37-ade7-4362-c779-d796e586d30c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-128   -5 -125 -112 -105 -107 -110 -106 -104  -91  -91 -128    9 -111\n",
            "  -95 -100 -101  -92 -103 -111 -108  -85 -128   -4 -122 -122 -119 -114\n",
            " -102 -105 -107  -93 -103] 1\n",
            "Quantized Model Test Set Accuracy: 0.8885714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the quantized TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Extract tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "all_tensor_details = interpreter.get_tensor_details()\n",
        "\n",
        "# Extract weights, biases, scales, and zero points from allocated tensors\n",
        "quantized_params = {}\n",
        "for tensor in all_tensor_details:\n",
        "    # Check if the tensor has quantization parameters and valid data\n",
        "    try:\n",
        "        # Attempt to get tensor data\n",
        "        tensor_data = interpreter.get_tensor(tensor['index'])\n",
        "\n",
        "        if 'quantization_parameters' in tensor and tensor['quantization_parameters']['scales'].size > 0:\n",
        "            quantized_params[tensor['name']] = {\n",
        "                'values': tensor_data,\n",
        "                'scale': tensor['quantization_parameters']['scales'],\n",
        "                'zero_point': tensor['quantization_parameters']['zero_points']\n",
        "            }\n",
        "    except ValueError:\n",
        "        # Skip tensors with null data\n",
        "        if 'quantization_parameters' in tensor and tensor['quantization_parameters']['scales'].size > 0:\n",
        "            quantized_params[tensor['name']] = {\n",
        "                'scale': tensor['quantization_parameters']['scales'],\n",
        "                'zero_point': tensor['quantization_parameters']['zero_points']\n",
        "            }\n",
        "        print(f\"Skipping tensor '{tensor['name']}' as it has null data.\")\n",
        "\n",
        "print(\"Quantized parameters extracted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv4RuhPA3MDI",
        "outputId": "89ec506f-bfb2-4dcc-ca47-0ee8055c506b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping tensor 'sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd' as it has null data.\n",
            "Skipping tensor 'sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd' as it has null data.\n",
            "Quantized parameters extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Print Names, Weights, Scales, and Zero Points of Quantized Model Tensors'**"
      ],
      "metadata": {
        "id": "V3QtTCSOe_8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, params in quantized_params.items():\n",
        "    print(f\"{name} - Scale: {params['scale']}, Zero Point: {params['zero_point']}\")\n",
        "    if 'values' in params:\n",
        "      print(params['values'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vO6T2gMerb9",
        "outputId": "378c1f44-9d95-49ad-9cd7-ab9aff96a0a3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "serving_default_keras_tensor:0_int8 - Scale: [0.0035371], Zero Point: [-128]\n",
            "[[ 53   4  -5  55   0   0   0   0 105  97 110 103 117 108  97 114  83 111\n",
            "  108 118 101   0   0   0  65   0   0   0   0   0   0   0  64]]\n",
            "arith.constant - Scale: [0.01599465], Zero Point: [0]\n",
            "[[  43  127  -25  -52    1   38    9    0   11    0]\n",
            " [  17  -16    0  -93   13   28   58    0   51    0]\n",
            " [   6  -60 -125   80   -8  -12  -68    0  -56    0]\n",
            " [ -56    0   74   -6   -3   59    8    0    0    0]\n",
            " [ -10  -64   69   59   -1 -123   -7    0   -5    0]]\n",
            "arith.constant1 - Scale: [0.01521643], Zero Point: [0]\n",
            "[[   0    0    0    3   13    0   27   18  -11    0    0  -61  -23    0\n",
            "    47   -8   -2    0    0    0]\n",
            " [   0    0    0   40   50    0   23   18    6    0    0  -53  -86    0\n",
            "   -69  -95   -8    0    0    0]\n",
            " [   0    0    0   39  -23    0  -59  -64   41    0    0   30  127    0\n",
            "   -31  -63  -13    0    0    0]\n",
            " [   0    0    0   38    6    0  -35  -28   23    0    0   52   63    0\n",
            "   100   60   -2    0    0    0]\n",
            " [   0    0    0  -10   -4    0    4    4   -4    0    0   -6   -2    0\n",
            "    -7    1    2    0    0    0]\n",
            " [   0    0    0    9   50    0   43   54   10    0    0   47  -73    0\n",
            "  -101  -20   10    0    0    0]\n",
            " [   0    0    0  -31   -4    0   39   37  -19    0    0  -36  -21    0\n",
            "   -51    1   13    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0]\n",
            " [   0    0    0  -34  -10    0   35   31  -17    0    0  -31  -14    0\n",
            "   -37    1   10    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0]]\n",
            "arith.constant2 - Scale: [0.00027655], Zero Point: [0]\n",
            "[ 1055 -3636  2262 -1153   821]\n",
            "sequential_1/dense_1_2/BiasAdd/ReadVariableOp - Scale: [0.00015107], Zero Point: [0]\n",
            "[ 4109   857 -2242  4634  3145  3164 10734  -237 10977  -375]\n",
            "sequential_1/dense_1/BiasAdd/ReadVariableOp - Scale: [5.072154e-05], Zero Point: [0]\n",
            "[ -794  -427  -911 -4249   535  -187 48632 44484 -1354  -712  -638 -2720\n",
            " -2567  -874 12308 19645 13867  -912 -1573  -814]\n",
            "sequential_1/dense_1/MatMul - Scale: [0.01433986], Zero Point: [0]\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [  18   -3   10   21   17    7    6    6    0   -1   10   18   -1    7\n",
            "    21   20    8    4   -1    0   -3    9    8   -8  -30   36   28   21\n",
            "    18    8    6   11   11]\n",
            " [ -21    1   20  -11   37    6   13    6   11    3   -6  -23  -21   14\n",
            "   -37    0   17   23   26   20   22   22  -20   -6   -3  -22   20   26\n",
            "    16    7    7   21    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [ -21   27   17  -32    7   -2    2  -12  -18   -2   -4   -1    5    6\n",
            "     3    4    0   -8    0  -15   -5    0  -30   11   15  -36  -14  -21\n",
            "   -17  -21  -34  -17  -28]\n",
            " [ -26   37    7  -35    8   -2    5  -15  -21   -7   -9  -20   10    6\n",
            "    -1    6    7   -3    4  -13   -3    0  -26   17   10  -32  -10  -17\n",
            "   -18  -18  -34  -15  -25]\n",
            " [  12   -4    6   12  -14    9    6    5   -2    5    5   21   12    5\n",
            "    43   11   -1  -14  -19  -23  -20  -15    2  -10  -15   33   16    8\n",
            "    14    1  -10   -5   10]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [ -11  -13   -9   27  -42   21   25   38   27   29   18  -18   45   17\n",
            "    67   11   -3  -27  -45  -43  -40  -53  -20   -9  -17   51   10    3\n",
            "    27   -9  -15  -19   10]\n",
            " [  77   13  -42   39  -39    5   -6    4  -10  -12    1  127    8   -3\n",
            "    79   17  -14  -19  -23   -9  -14  -28   57   21  -58   67   -4  -25\n",
            "   -10  -20  -18  -22   16]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [  71  -16 -103   65    3  -30  -39  -20  -28  -40   -5   29   30  -68\n",
            "    35   23    6    0  -26    0   -9  -19    3  -10  -80   71    9  -14\n",
            "   -13  -26   14  -14  -18]\n",
            " [  24   -8  -86   39  -29   -6   -8   17   26   11   15  -78   60  -11\n",
            "    -9  -16   -2    0  -15   16   -3  -29  -29   45    6   11  -40  -19\n",
            "    -9   -5   13  -27  -16]\n",
            " [ -10    4   -2   -8   -6   -2    1    0    1    0   -5  -13    5    1\n",
            "   -11   -8    0    0    0    1    0   -4   -7   10   11  -15   -9   -6\n",
            "    -5   -3   -3   -5   -6]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd - Scale: [0.00992819], Zero Point: [-128]\n",
            "sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd - Scale: [0.01729033], Zero Point: [-128]\n",
            "StatefulPartitionedCall_1:0_int8 - Scale: [0.06460411], Zero Point: [-1]\n",
            "[[-32 -26 -13  55   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Map TFLite Provided Names to Intuitive Ones**\n",
        "The TFLite Layer names after quantization are not very intuitive.\n",
        "Use the names above + the [Netron](https://netron.app/) application to update dictionary below so that it is very clear which layer is which. *You may need to update the names if any changes are made to the notebook.*"
      ],
      "metadata": {
        "id": "dDUXkdPJAlNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, params in quantized_params.items():\n",
        "    print(f\"{name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5CLjrd6r2M_",
        "outputId": "f0f2e73e-ea53-48a1-b1bd-171985504763"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "serving_default_keras_tensor:0_int8\n",
            "arith.constant\n",
            "arith.constant1\n",
            "arith.constant2\n",
            "sequential_1/dense_1_2/BiasAdd/ReadVariableOp\n",
            "sequential_1/dense_1/BiasAdd/ReadVariableOp\n",
            "sequential_1/dense_1/MatMul\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd\n",
            "sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd\n",
            "StatefulPartitionedCall_1:0_int8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UQwlPsSStV4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, params in quantized_params.items():\n",
        "    print(f\"Tensor Name: {name}, shape: {params['values'].shape if 'values' in params else 'N/A'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxp3BjWMuiz6",
        "outputId": "86bc8779-9fee-450e-84e6-a3b18d63ceb8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor Name: serving_default_keras_tensor:0_int8, shape: (1, 33)\n",
            "Tensor Name: arith.constant, shape: (5, 10)\n",
            "Tensor Name: arith.constant1, shape: (10, 20)\n",
            "Tensor Name: arith.constant2, shape: (5,)\n",
            "Tensor Name: sequential_1/dense_1_2/BiasAdd/ReadVariableOp, shape: (10,)\n",
            "Tensor Name: sequential_1/dense_1/BiasAdd/ReadVariableOp, shape: (20,)\n",
            "Tensor Name: sequential_1/dense_1/MatMul, shape: (20, 33)\n",
            "Tensor Name: sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd, shape: N/A\n",
            "Tensor Name: sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd, shape: N/A\n",
            "Tensor Name: StatefulPartitionedCall_1:0_int8, shape: (1, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l4RXXe-rtuMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer name map\n",
        "layer_name_map = {\n",
        "    \"input_layer\": \"serving_default_keras_tensor:0_int8\",\n",
        "    \"layer_one_weights\": \"sequential_1/dense_1/MatMul\",\n",
        "    \"layer_one_bias\": \"sequential_1/dense_1/BiasAdd/ReadVariableOp\",\n",
        "    \"layer_one_output_activations\": \"sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd\",\n",
        "    \"layer_two_weights\": \"arith.constant1\",\n",
        "    \"layer_two_bias\": \"sequential_1/dense_1_2/BiasAdd/ReadVariableOp\",\n",
        "    \"layer_two_output_activations\": \"sequential_1/dense_1_2/MatMul;sequential_1/dense_1_2/Relu;sequential_1/dense_1_2/BiasAdd\",\n",
        "    \"layer_three_weights\": \"arith.constant\",\n",
        "    \"layer_three_bias\": \"arith.constant2\",\n",
        "    \"output_layer\": \"StatefulPartitionedCall_1:0_int8\"\n",
        "}"
      ],
      "metadata": {
        "id": "GqeYV9Ko_kAq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer_scale = quantized_params[layer_name_map[\"input_layer\"]][\"scale\"]\n",
        "input_layer_zero_point = quantized_params[layer_name_map[\"input_layer\"]][\"zero_point\"]\n",
        "\n",
        "layer_one_weights = quantized_params[layer_name_map[\"layer_one_weights\"]][\"values\"]\n",
        "layer_one_weights_scale = quantized_params[layer_name_map[\"layer_one_weights\"]][\"scale\"]\n",
        "layer_one_weights_zero_point = quantized_params[layer_name_map[\"layer_one_weights\"]][\"zero_point\"]\n",
        "layer_one_bias = quantized_params[layer_name_map[\"layer_one_bias\"]][\"values\"]\n",
        "\n",
        "layer_one_output_activations_scale = quantized_params[layer_name_map[\"layer_one_output_activations\"]][\"scale\"]\n",
        "layer_one_output_activations_zero_point = quantized_params[layer_name_map[\"layer_one_output_activations\"]][\"zero_point\"]\n",
        "\n",
        "layer_two_weights = quantized_params[layer_name_map[\"layer_two_weights\"]][\"values\"]\n",
        "layer_two_weights_scale = quantized_params[layer_name_map[\"layer_two_weights\"]][\"scale\"]\n",
        "layer_two_weights_zero_point = quantized_params[layer_name_map[\"layer_two_weights\"]][\"zero_point\"]\n",
        "layer_two_bias = quantized_params[layer_name_map[\"layer_two_bias\"]][\"values\"]\n",
        "\n",
        "layer_two_output_activations_scale = quantized_params[layer_name_map[\"layer_two_output_activations\"]][\"scale\"]\n",
        "layer_two_output_activations_zero_point = quantized_params[layer_name_map[\"layer_two_output_activations\"]][\"zero_point\"]\n",
        "\n",
        "layer_three_weights = quantized_params[layer_name_map[\"layer_three_weights\"]][\"values\"]\n",
        "layer_three_weights_scale = quantized_params[layer_name_map[\"layer_three_weights\"]][\"scale\"]\n",
        "layer_three_weights_zero_point = quantized_params[layer_name_map[\"layer_three_weights\"]][\"zero_point\"]\n",
        "layer_three_bias = quantized_params[layer_name_map[\"layer_three_bias\"]][\"values\"]\n",
        "\n",
        "output_layer_scale = quantized_params[layer_name_map[\"output_layer\"]][\"scale\"]\n",
        "output_layer_zero_point = quantized_params[layer_name_map[\"output_layer\"]][\"zero_point\"]"
      ],
      "metadata": {
        "id": "txvONXs98UfK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"input_layer_scale: {input_layer_scale}\")\n",
        "print(f\"input_layer_zero_point: {input_layer_zero_point}\")\n",
        "print(f\"layer_one_weights_scale: {layer_one_weights_scale}\")\n",
        "print(f\"layer_one_weights_zero_point: {layer_one_weights_zero_point}\")\n",
        "print(f\"layer_one_bias: {layer_one_bias}\")\n",
        "print(f\"layer_one_output_activations_scale: {layer_one_output_activations_scale}\")\n",
        "print(f\"layer_one_output_activations_zero_point: {layer_one_output_activations_zero_point}\")\n",
        "print(f\"layer_two_weights_scale: {layer_two_weights_scale}\")\n",
        "print(f\"layer_two_weights_zero_point: {layer_two_weights_zero_point}\")\n",
        "print(f\"layer_two_bias: {layer_two_bias}\")\n",
        "print(f\"layer_two_output_activations_scale: {layer_two_output_activations_scale}\")\n",
        "print(f\"layer_two_output_activations_zero_point: {layer_two_output_activations_zero_point}\")\n",
        "print(f\"layer_three_weights_scale: {layer_three_weights_scale}\")\n",
        "print(f\"layer_three_weights_zero_point: {layer_three_weights_zero_point}\")\n",
        "print(f\"layer_three_bias: {layer_three_bias}\")\n",
        "print(f\"output_layer_scale: {output_layer_scale}\")\n",
        "print(f\"output_layer_zero_point: {output_layer_zero_point}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jb_aBZxbTCI",
        "outputId": "4241e586-866b-4f67-e6c1-cf807159f74b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_layer_scale: [0.0035371]\n",
            "input_layer_zero_point: [-128]\n",
            "layer_one_weights_scale: [0.01433986]\n",
            "layer_one_weights_zero_point: [0]\n",
            "layer_one_bias: [ -794  -427  -911 -4249   535  -187 48632 44484 -1354  -712  -638 -2720\n",
            " -2567  -874 12308 19645 13867  -912 -1573  -814]\n",
            "layer_one_output_activations_scale: [0.00992819]\n",
            "layer_one_output_activations_zero_point: [-128]\n",
            "layer_two_weights_scale: [0.01521643]\n",
            "layer_two_weights_zero_point: [0]\n",
            "layer_two_bias: [ 4109   857 -2242  4634  3145  3164 10734  -237 10977  -375]\n",
            "layer_two_output_activations_scale: [0.01729033]\n",
            "layer_two_output_activations_zero_point: [-128]\n",
            "layer_three_weights_scale: [0.01599465]\n",
            "layer_three_weights_zero_point: [0]\n",
            "layer_three_bias: [ 1055 -3636  2262 -1153   821]\n",
            "output_layer_scale: [0.06460411]\n",
            "output_layer_zero_point: [-1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Layer One Weights (comma-separated):\")\n",
        "for row in layer_one_weights:\n",
        "    print(','.join(map(str, row)))\n",
        "\n",
        "print(\"\\nLayer Two Weights (comma-separated):\")\n",
        "for row in layer_two_weights:\n",
        "    print(','.join(map(str, row)))\n",
        "\n",
        "print(\"\\nLayer Three Weights (comma-separated):\")\n",
        "for row in layer_three_weights:\n",
        "    print(','.join(map(str, row)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4iEk2Jjb_e2",
        "outputId": "1ba374bd-9967-466d-99a0-4c75e37d68e1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer One Weights (comma-separated):\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "18,-3,10,21,17,7,6,6,0,-1,10,18,-1,7,21,20,8,4,-1,0,-3,9,8,-8,-30,36,28,21,18,8,6,11,11\n",
            "-21,1,20,-11,37,6,13,6,11,3,-6,-23,-21,14,-37,0,17,23,26,20,22,22,-20,-6,-3,-22,20,26,16,7,7,21,0\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "-21,27,17,-32,7,-2,2,-12,-18,-2,-4,-1,5,6,3,4,0,-8,0,-15,-5,0,-30,11,15,-36,-14,-21,-17,-21,-34,-17,-28\n",
            "-26,37,7,-35,8,-2,5,-15,-21,-7,-9,-20,10,6,-1,6,7,-3,4,-13,-3,0,-26,17,10,-32,-10,-17,-18,-18,-34,-15,-25\n",
            "12,-4,6,12,-14,9,6,5,-2,5,5,21,12,5,43,11,-1,-14,-19,-23,-20,-15,2,-10,-15,33,16,8,14,1,-10,-5,10\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "-11,-13,-9,27,-42,21,25,38,27,29,18,-18,45,17,67,11,-3,-27,-45,-43,-40,-53,-20,-9,-17,51,10,3,27,-9,-15,-19,10\n",
            "77,13,-42,39,-39,5,-6,4,-10,-12,1,127,8,-3,79,17,-14,-19,-23,-9,-14,-28,57,21,-58,67,-4,-25,-10,-20,-18,-22,16\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "71,-16,-103,65,3,-30,-39,-20,-28,-40,-5,29,30,-68,35,23,6,0,-26,0,-9,-19,3,-10,-80,71,9,-14,-13,-26,14,-14,-18\n",
            "24,-8,-86,39,-29,-6,-8,17,26,11,15,-78,60,-11,-9,-16,-2,0,-15,16,-3,-29,-29,45,6,11,-40,-19,-9,-5,13,-27,-16\n",
            "-10,4,-2,-8,-6,-2,1,0,1,0,-5,-13,5,1,-11,-8,0,0,0,1,0,-4,-7,10,11,-15,-9,-6,-5,-3,-3,-5,-6\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "\n",
            "Layer Two Weights (comma-separated):\n",
            "0,0,0,3,13,0,27,18,-11,0,0,-61,-23,0,47,-8,-2,0,0,0\n",
            "0,0,0,40,50,0,23,18,6,0,0,-53,-86,0,-69,-95,-8,0,0,0\n",
            "0,0,0,39,-23,0,-59,-64,41,0,0,30,127,0,-31,-63,-13,0,0,0\n",
            "0,0,0,38,6,0,-35,-28,23,0,0,52,63,0,100,60,-2,0,0,0\n",
            "0,0,0,-10,-4,0,4,4,-4,0,0,-6,-2,0,-7,1,2,0,0,0\n",
            "0,0,0,9,50,0,43,54,10,0,0,47,-73,0,-101,-20,10,0,0,0\n",
            "0,0,0,-31,-4,0,39,37,-19,0,0,-36,-21,0,-51,1,13,0,0,0\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "0,0,0,-34,-10,0,35,31,-17,0,0,-31,-14,0,-37,1,10,0,0,0\n",
            "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "\n",
            "Layer Three Weights (comma-separated):\n",
            "43,127,-25,-52,1,38,9,0,11,0\n",
            "17,-16,0,-93,13,28,58,0,51,0\n",
            "6,-60,-125,80,-8,-12,-68,0,-56,0\n",
            "-56,0,74,-6,-3,59,8,0,0,0\n",
            "-10,-64,69,59,-1,-123,-7,0,-5,0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fixed_point_multiplier(input_scale, weight_scale, output_scale):\n",
        "    # Calculate M0 and N from M = 2^-N M0 = (S1 * S2 / S3)\n",
        "    multiplier = input_scale * weight_scale / output_scale\n",
        "    shift = 0\n",
        "    while multiplier < 0.5:\n",
        "        multiplier *= 2\n",
        "        shift += 1\n",
        "    quantized_multiplier = multiplier * math.pow(2, 31)\n",
        "    return quantized_multiplier, shift"
      ],
      "metadata": {
        "id": "l00b5QnEaRmX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First layer requantization params\n",
        "layer_one_multiplier, layer_one_shift = calculate_fixed_point_multiplier(input_layer_scale, layer_one_weights_scale, layer_one_output_activations_scale)\n",
        "\n",
        "# Second layer requantization params\n",
        "layer_two_multiplier, layer_two_shift = calculate_fixed_point_multiplier(layer_one_output_activations_scale, layer_two_weights_scale, layer_two_output_activations_scale)\n",
        "\n",
        "# Third layer requantization params\n",
        "layer_three_multiplier, layer_three_shift = calculate_fixed_point_multiplier(layer_two_output_activations_scale, layer_three_weights_scale, output_layer_scale)\n",
        "\n",
        "subscript_printing = str.maketrans(\"123456789\", \"₁₂₃₄₅₆₇₈₉\")\n",
        "print(\"Layer 1 Requantization Params:\")\n",
        "print(\"M01: \".translate(subscript_printing) + f\"{layer_one_multiplier[0]:.2f}\")\n",
        "print(\"N1: \".translate(subscript_printing)  + f\"{layer_one_shift}\")\n",
        "\n",
        "print(\"Layer 2 Requantization Params:\")\n",
        "print(\"M02: \".translate(subscript_printing) + f\"{layer_two_multiplier[0]:.2f}\")\n",
        "print(\"N2: \".translate(subscript_printing)  + f\"{layer_two_shift}\")\n",
        "\n",
        "print(\"Layer 3 Requantization Params:\")\n",
        "print(\"M03: \".translate(subscript_printing) + f\"{layer_three_multiplier[0]:.2f}\")\n",
        "print(\"N3: \".translate(subscript_printing)  + f\"{layer_three_shift}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibGduAd2bMYE",
        "outputId": "df59d8c8-ab9a-4f31-ba31-0edac3af4445"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Requantization Params:\n",
            "M0₁: 1404307200.00\n",
            "N₁: 7\n",
            "Layer 2 Requantization Params:\n",
            "M0₂: 1200851840.00\n",
            "N₂: 6\n",
            "Layer 3 Requantization Params:\n",
            "M0₃: 1176678400.00\n",
            "N₃: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Emulate 8-bit Integer Inference with Numpy**"
      ],
      "metadata": {
        "id": "pDmEYHIJoB5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create copy of test dataset\n",
        "TEST_SET_SIZE = 1400\n",
        "test_dataset_copy = (iter(test_dataset_quantized_inputs.take(TEST_SET_SIZE)))"
      ],
      "metadata": {
        "id": "Sskn2QitsQ1e"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tflite_golden_inference(tflite_model, inputs, debug=False):\n",
        "    #\n",
        "    # Golden Reference Implementation of TFLite Inference running on a single sample\n",
        "    #\n",
        "\n",
        "    # Add batch dim to single data sample\n",
        "    inputs = np.expand_dims(inputs, 0)\n",
        "\n",
        "    # Load TFLite model and allocate tensors\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model, experimental_preserve_all_tensors=True)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensors\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Load input tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], inputs)\n",
        "\n",
        "    # Run the model\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Print each layer's output if needed for verification\n",
        "    if debug:\n",
        "      print({\n",
        "          t['name']: interpreter.get_tensor(t['index'])\n",
        "          for t in interpreter.get_tensor_details()\n",
        "      })\n",
        "\n",
        "    return interpreter.get_tensor(output_details[0]['index'])"
      ],
      "metadata": {
        "id": "jja_F8B4sRqV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_numpy_inference(input):\n",
        "  #\n",
        "  # Numpy Reference Implementation of TFLite Inference running on a single sample\n",
        "  #\n",
        "\n",
        "  # (Inputs * Layer 1 Weights) + Bias followed by ReLU\n",
        "  x = np.matmul((input.numpy().astype(np.int32) - input_layer_zero_point.astype(np.int32)),(layer_one_weights.T.astype(np.int32) - layer_one_weights_zero_point.astype(np.int32)))\n",
        "  x = x + layer_one_bias\n",
        "  x = np.maximum(x, 0)\n",
        "\n",
        "  # Requantization pipeline\n",
        "  x = x * layer_one_multiplier\n",
        "  x = np.round((x / np.power(2,31))).astype(np.int32)\n",
        "  x = np.round((x / np.power(2, layer_one_shift))).astype(np.int32)\n",
        "  x = x + layer_one_output_activations_zero_point.astype(np.int32)\n",
        "  x = np.clip(x, -128, 127)\n",
        "\n",
        "  # (Layer 1 Activations * Layer 2 Weights) + Bias followed by ReLU\n",
        "  x = np.matmul((x.astype(np.int32) - layer_one_output_activations_zero_point.astype(np.int32)), (layer_two_weights.T.astype(np.int32) - layer_two_weights_zero_point.astype(np.int32)))\n",
        "  x = x + layer_two_bias\n",
        "  x = np.maximum(x, 0)\n",
        "\n",
        "  # Requantization pipeline\n",
        "  x = x * layer_two_multiplier\n",
        "  x = np.round((x / np.power(2,31))).astype(np.int32)\n",
        "  x = np.round((x / np.power(2, layer_two_shift))).astype(np.int32)\n",
        "  x = x + layer_two_output_activations_zero_point.astype(np.int32)\n",
        "  x = np.clip(x, -128, 127)\n",
        "\n",
        "  # (Layer 2 Activations * Layer 2 Weights) + Bias\n",
        "  x = np.matmul((x.astype(np.int32) - layer_two_output_activations_zero_point.astype(np.int32)), (layer_three_weights.T.astype(np.int32) - layer_three_weights_zero_point.astype(np.int32)))\n",
        "  x = x + layer_three_bias\n",
        "\n",
        "  # Requantization pipeline\n",
        "  x = x * layer_three_multiplier\n",
        "  x = np.round((x / np.power(2,31))).astype(np.int32)\n",
        "  x = np.round((x / np.power(2, layer_three_shift))).astype(np.int32)\n",
        "  x = x + output_layer_zero_point.astype(np.int32)\n",
        "  x = np.clip(x, -128, 127)\n",
        "\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "5_Hx0jC7oBje"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_correct = 0.0\n",
        "numpy_correct = 0.0\n",
        "\n",
        "for inputs, targets in test_dataset_copy:\n",
        "  tflite_output = run_tflite_golden_inference(tflite_quant_model, inputs)\n",
        "  numpy_output = run_numpy_inference(inputs)\n",
        "\n",
        "  # Make sure that raw values of output tensors match exactly to validate numpy reference implementation\n",
        "  if sum(abs(tflite_output.flatten() - numpy_output.flatten())) > OUTPUT_SIZE: # error might be ~1 per dimension\n",
        "    print(tflite_output.flatten())\n",
        "    print(numpy_output.flatten())\n",
        "    print(\"ERROR: TFlite Golden Output Tensor does not match Numpy Implementation Output Tensor\")\n",
        "\n",
        "  # Update num correct\n",
        "  if tflite_output.argmax() == targets.numpy():\n",
        "    tflite_correct += 1\n",
        "  if numpy_output.argmax() == targets.numpy():\n",
        "    numpy_correct += 1\n",
        "\n",
        "\n",
        "# Make sure accuracy is exactly the same to validate numpy implementation\n",
        "print(f\"TF Lite Accuracy: {tflite_correct / TEST_SET_SIZE}\")\n",
        "print(f\"Numpy Accuracy: {numpy_correct / TEST_SET_SIZE}\")\n",
        "print(\"Numpy Implementation matches TFLite Golden Implementation!\" if tflite_correct / TEST_SET_SIZE == numpy_correct / TEST_SET_SIZE else \"Numpy Implementation does NOT match TFLite Golden Implementation...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1qDAzufb-ZD",
        "outputId": "289e4b8b-079c-4ca0-a09f-3354410a58b5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Lite Accuracy: 0.8885714285714286\n",
            "Numpy Accuracy: 0.8885714285714286\n",
            "Numpy Implementation matches TFLite Golden Implementation!\n"
          ]
        }
      ]
    }
  ]
}