{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **FlexiBench Food Spoilage MLP Quantization**\n",
        "### Author: Shvetank Prakash\n",
        "### Date: Jan 2025\n",
        "#### Helpful links:\n",
        "\n",
        "[Co-Design of Approximate Multilayer Perceptron for Ultra-Resource Constrained Printed Circuits](https://arxiv.org/abs/2302.14576) (Paper Results Reproduced)\n",
        "\n",
        "[Gemmlowp Paper](https://arxiv.org/pdf/1712.05877)\n",
        "\n",
        "[Gemmlowp Implementation](https://github.com/google/gemmlowp/tree/master)\n"
      ],
      "metadata": {
        "id": "B6xKk1RMw3zG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Imports and Global Defs"
      ],
      "metadata": {
        "id": "zmARXYN5xetM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-model-optimization==0.8.0\n",
        "!pip install ucimlrepo==0.0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "collapsed": true,
        "id": "-7RwFYPOH7xC",
        "outputId": "85dcbb94-4ce6-406f-fb88-f626077089d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-model-optimization==0.8.0\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (0.1.9)\n",
            "Collecting numpy~=1.23 (from tensorflow-model-optimization==0.8.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six~=1.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (1.17.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization==0.8.0) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow-model-optimization==0.8.0) (1.17.2)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorflow-model-optimization\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 tensorflow-model-optimization-0.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "03558d4a254c48fd852dab8cac66ca58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo==0.0.7\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo==0.0.7) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo==0.0.7) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo==0.0.7) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Setting environment variables\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8tYezt7xmzUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "INPUT_SIZE = 11\n",
        "HIDDEN_SIZE_1 = 16\n",
        "# HIDDEN_SIZE_2 = 32\n",
        "OUTPUT_SIZE = 4\n",
        "\n",
        "# Training Parameters\n",
        "EPOCHS = 400\n",
        "BATCH_SIZE_TRAIN = 200\n",
        "BATCH_SIZE_TEST = 64\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "fMfZ9zYJIdCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read and Preprocess Dataset**"
      ],
      "metadata": {
        "id": "2Jml3fzQBpSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    labels = df.pop('Label')\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df.values.astype('float32') / 255., labels.values.astype('int32')))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Load in dataset from csv\n",
        "train_dataset = load_csv_data('quantized_train.csv')\n",
        "print(train_dataset)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(list(train_dataset)), seed=SEED)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE_TRAIN)\n",
        "\n",
        "test_dataset = load_csv_data('samples.csv')\n",
        "print(test_dataset)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE_TEST)"
      ],
      "metadata": {
        "id": "YJ_KJE3dn4kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a729f361-4843-4067-9271-9671d7864045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_TensorSliceDataset element_spec=(TensorSpec(shape=(11,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n",
            "<_TensorSliceDataset element_spec=(TensorSpec(shape=(11,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print a single element from each dataset\n",
        "# print(\"Single element from train_dataset:\")\n",
        "# for element in train_dataset.take(1):\n",
        "#     print(element)\n",
        "\n",
        "print(\"\\nSingle element from test_dataset:\")\n",
        "for element in test_dataset.take(1):\n",
        "    print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cqoYwwI9JLVs",
        "outputId": "dda57d86-6828-441e-aceb-85c9e8c2857b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Single element from test_dataset:\n",
            "(<tf.Tensor: shape=(64, 11), dtype=float32, numpy=\n",
            "array([[0.73333335, 0.3254902 , 0.15294118, 0.9843137 , 0.8       ,\n",
            "        0.83137256, 0.3764706 , 0.21568628, 0.30980393, 0.57254905,\n",
            "        0.9529412 ],\n",
            "       [0.7137255 , 0.3137255 , 0.14509805, 0.9411765 , 0.7411765 ,\n",
            "        0.78431374, 0.36862746, 0.20392157, 0.30588236, 0.5372549 ,\n",
            "        0.8784314 ],\n",
            "       [0.69803923, 0.30588236, 0.14901961, 0.92941177, 0.7137255 ,\n",
            "        0.75686276, 0.38431373, 0.20784314, 0.30980393, 0.5372549 ,\n",
            "        0.8784314 ],\n",
            "       [0.6862745 , 0.3019608 , 0.15686275, 0.9411765 , 0.70980394,\n",
            "        0.75686276, 0.36078432, 0.19607843, 0.30980393, 0.5137255 ,\n",
            "        0.8862745 ],\n",
            "       [0.6862745 , 0.3019608 , 0.16078432, 0.9490196 , 0.69411767,\n",
            "        0.7490196 , 0.3647059 , 0.21176471, 0.3137255 , 0.74509805,\n",
            "        0.9254902 ],\n",
            "       [0.6862745 , 0.29803923, 0.16470589, 0.9372549 , 0.6862745 ,\n",
            "        0.74509805, 0.36078432, 0.21176471, 0.31764707, 0.57254905,\n",
            "        0.9372549 ],\n",
            "       [0.67058825, 0.2901961 , 0.16862746, 0.9019608 , 0.6509804 ,\n",
            "        0.72156864, 0.35686275, 0.21568628, 0.31764707, 0.56078434,\n",
            "        0.9019608 ],\n",
            "       [0.6666667 , 0.2784314 , 0.16862746, 0.88235295, 0.62352943,\n",
            "        0.7058824 , 0.3529412 , 0.21960784, 0.31764707, 0.5529412 ,\n",
            "        0.91764706],\n",
            "       [0.58431375, 0.25882354, 0.16078432, 0.8117647 , 0.5686275 ,\n",
            "        0.6666667 , 0.3254902 , 0.21176471, 0.3137255 , 0.5058824 ,\n",
            "        0.80784315],\n",
            "       [0.59607846, 0.25882354, 0.16862746, 0.81960785, 0.5686275 ,\n",
            "        0.654902  , 0.33333334, 0.21960784, 0.31764707, 0.5372549 ,\n",
            "        0.85882354],\n",
            "       [0.627451  , 0.26666668, 0.18039216, 0.84313726, 0.5882353 ,\n",
            "        0.6745098 , 0.34509805, 0.22352941, 0.32941177, 0.7137255 ,\n",
            "        0.9607843 ],\n",
            "       [0.6313726 , 0.25882354, 0.18431373, 0.8235294 , 0.54509807,\n",
            "        0.6666667 , 0.34117648, 0.22745098, 0.3254902 , 0.57254905,\n",
            "        0.9019608 ],\n",
            "       [0.6       , 0.24705882, 0.18039216, 0.78039217, 0.52156866,\n",
            "        0.63529414, 0.32941177, 0.21568628, 0.32156864, 0.5372549 ,\n",
            "        0.87058824],\n",
            "       [0.61960787, 0.24313726, 0.1882353 , 0.78039217, 0.5019608 ,\n",
            "        0.6392157 , 0.3372549 , 0.22745098, 0.32941177, 0.59607846,\n",
            "        0.9098039 ],\n",
            "       [0.6117647 , 0.23921569, 0.1882353 , 0.75686276, 0.5058824 ,\n",
            "        0.627451  , 0.32941177, 0.24313726, 0.32941177, 0.6666667 ,\n",
            "        0.9254902 ],\n",
            "       [0.6       , 0.23137255, 0.1882353 , 0.7372549 , 0.4745098 ,\n",
            "        0.60784316, 0.32156864, 0.22745098, 0.32941177, 0.5294118 ,\n",
            "        0.87058824],\n",
            "       [0.5882353 , 0.22352941, 0.19215687, 0.7176471 , 0.4745098 ,\n",
            "        0.59607846, 0.31764707, 0.22352941, 0.32941177, 0.54509807,\n",
            "        0.91764706],\n",
            "       [0.5882353 , 0.21960784, 0.19607843, 0.7058824 , 0.45882353,\n",
            "        0.5921569 , 0.32156864, 0.23529412, 0.32941177, 0.69411767,\n",
            "        0.9098039 ],\n",
            "       [0.60784316, 0.21960784, 0.20392157, 0.7137255 , 0.45490196,\n",
            "        0.5921569 , 0.32941177, 0.24313726, 0.3372549 , 0.54509807,\n",
            "        0.94509804],\n",
            "       [0.58431375, 0.21176471, 0.20392157, 0.6784314 , 0.42352942,\n",
            "        0.57254905, 0.31764707, 0.22745098, 0.3372549 , 0.5882353 ,\n",
            "        0.91764706],\n",
            "       [0.5294118 , 0.19607843, 0.2       , 0.63529414, 0.38039216,\n",
            "        0.5411765 , 0.3019608 , 0.23529412, 0.33333334, 0.57254905,\n",
            "        0.8509804 ],\n",
            "       [0.5411765 , 0.19215687, 0.19607843, 0.61960787, 0.3647059 ,\n",
            "        0.5254902 , 0.29803923, 0.23529412, 0.32941177, 0.52156866,\n",
            "        0.8156863 ],\n",
            "       [0.5294118 , 0.18431373, 0.19607843, 0.6039216 , 0.34901962,\n",
            "        0.5176471 , 0.29411766, 0.24313726, 0.32941177, 0.5294118 ,\n",
            "        0.8       ],\n",
            "       [0.5176471 , 0.1764706 , 0.2       , 0.5803922 , 0.33333334,\n",
            "        0.49803922, 0.28627452, 0.27058825, 0.33333334, 0.49411765,\n",
            "        0.8156863 ],\n",
            "       [0.49411765, 0.17254902, 0.19607843, 0.5647059 , 0.31764707,\n",
            "        0.49019608, 0.2784314 , 0.24705882, 0.32941177, 0.49803922,\n",
            "        0.7882353 ],\n",
            "       [0.5058824 , 0.17254902, 0.2       , 0.54509807, 0.3137255 ,\n",
            "        0.49019608, 0.28235295, 0.24705882, 0.3372549 , 0.5137255 ,\n",
            "        0.73333335],\n",
            "       [0.5058824 , 0.17254902, 0.2       , 0.5372549 , 0.3019608 ,\n",
            "        0.4862745 , 0.28235295, 0.25490198, 0.3372549 , 0.47843137,\n",
            "        0.7921569 ],\n",
            "       [0.5058824 , 0.16862746, 0.20392157, 0.52156866, 0.3019608 ,\n",
            "        0.48235294, 0.2784314 , 0.25490198, 0.3372549 , 0.4862745 ,\n",
            "        0.74509805],\n",
            "       [0.5411765 , 0.17254902, 0.21568628, 0.5411765 , 0.30980393,\n",
            "        0.49411765, 0.29803923, 0.2627451 , 0.34509805, 0.5294118 ,\n",
            "        0.7921569 ],\n",
            "       [0.5529412 , 0.1764706 , 0.22352941, 0.54509807, 0.30588236,\n",
            "        0.49411765, 0.30588236, 0.26666668, 0.3529412 , 0.54509807,\n",
            "        0.8627451 ],\n",
            "       [0.5058824 , 0.16078432, 0.21176471, 0.49411765, 0.26666668,\n",
            "        0.4509804 , 0.28627452, 0.26666668, 0.34901962, 0.52156866,\n",
            "        0.78039217],\n",
            "       [0.49803922, 0.15686275, 0.21568628, 0.49803922, 0.2627451 ,\n",
            "        0.44705883, 0.28235295, 0.25490198, 0.34509805, 0.5137255 ,\n",
            "        0.74509805],\n",
            "       [0.5176471 , 0.16078432, 0.22352941, 0.4862745 , 0.27450982,\n",
            "        0.45882353, 0.2901961 , 0.26666668, 0.34901962, 0.4862745 ,\n",
            "        0.6509804 ],\n",
            "       [0.50980395, 0.15686275, 0.22352941, 0.47058824, 0.25490198,\n",
            "        0.4509804 , 0.29411766, 0.25882354, 0.34901962, 0.5372549 ,\n",
            "        0.7607843 ],\n",
            "       [0.52156866, 0.15686275, 0.23137255, 0.4745098 , 0.24705882,\n",
            "        0.44705883, 0.29411766, 0.26666668, 0.3529412 , 0.57254905,\n",
            "        0.78039217],\n",
            "       [0.5058824 , 0.15294118, 0.23137255, 0.45882353, 0.24313726,\n",
            "        0.43529412, 0.2901961 , 0.2784314 , 0.3529412 , 0.5529412 ,\n",
            "        0.78039217],\n",
            "       [0.53333336, 0.15686275, 0.24313726, 0.4745098 , 0.25490198,\n",
            "        0.44705883, 0.29803923, 0.27058825, 0.36078432, 0.56078434,\n",
            "        0.81960785],\n",
            "       [0.5176471 , 0.14901961, 0.24313726, 0.44705883, 0.23921569,\n",
            "        0.42745098, 0.29411766, 0.27058825, 0.36078432, 0.5803922 ,\n",
            "        0.7921569 ],\n",
            "       [0.50980395, 0.14509805, 0.24705882, 0.44313726, 0.23529412,\n",
            "        0.41568628, 0.28627452, 0.27058825, 0.36078432, 0.5803922 ,\n",
            "        0.8       ],\n",
            "       [0.50980395, 0.14509805, 0.2509804 , 0.4392157 , 0.23529412,\n",
            "        0.41960785, 0.29411766, 0.27450982, 0.3647059 , 0.59607846,\n",
            "        0.8156863 ],\n",
            "       [0.5058824 , 0.14509805, 0.25490198, 0.42745098, 0.23137255,\n",
            "        0.41568628, 0.29411766, 0.27450982, 0.3647059 , 0.5803922 ,\n",
            "        0.7921569 ],\n",
            "       [0.49803922, 0.14117648, 0.25882354, 0.40784314, 0.22745098,\n",
            "        0.40784314, 0.2901961 , 0.2784314 , 0.3647059 , 0.6117647 ,\n",
            "        0.7529412 ],\n",
            "       [0.46666667, 0.13333334, 0.25490198, 0.38431373, 0.21176471,\n",
            "        0.38431373, 0.2784314 , 0.3254902 , 0.36078432, 0.56078434,\n",
            "        0.78039217],\n",
            "       [0.47843137, 0.13333334, 0.2627451 , 0.3882353 , 0.21568628,\n",
            "        0.3882353 , 0.28627452, 0.36862746, 0.36078432, 0.49803922,\n",
            "        0.76862746],\n",
            "       [0.5058824 , 0.13333334, 0.27058825, 0.38431373, 0.21568628,\n",
            "        0.3882353 , 0.28627452, 0.28235295, 0.36862746, 0.6039216 ,\n",
            "        0.7607843 ],\n",
            "       [0.49019608, 0.12941177, 0.26666668, 0.3647059 , 0.20392157,\n",
            "        0.3764706 , 0.2784314 , 0.28627452, 0.36862746, 0.54509807,\n",
            "        0.73333335],\n",
            "       [0.47843137, 0.1254902 , 0.27058825, 0.3529412 , 0.20392157,\n",
            "        0.37254903, 0.2784314 , 0.28627452, 0.36862746, 0.52156866,\n",
            "        0.6627451 ],\n",
            "       [0.47058824, 0.1254902 , 0.27450982, 0.34901962, 0.2       ,\n",
            "        0.37254903, 0.27450982, 0.28235295, 0.36862746, 0.5137255 ,\n",
            "        0.7254902 ],\n",
            "       [0.47843137, 0.1254902 , 0.28235295, 0.34901962, 0.20392157,\n",
            "        0.37254903, 0.2784314 , 0.29803923, 0.37254903, 0.57254905,\n",
            "        0.7607843 ],\n",
            "       [0.47843137, 0.12156863, 0.2784314 , 0.33333334, 0.19607843,\n",
            "        0.35686275, 0.27058825, 0.2901961 , 0.36862746, 0.54509807,\n",
            "        0.78039217],\n",
            "       [0.48235294, 0.1254902 , 0.28627452, 0.33333334, 0.2       ,\n",
            "        0.35686275, 0.2784314 , 0.2901961 , 0.37254903, 0.56078434,\n",
            "        0.74509805],\n",
            "       [0.47058824, 0.12156863, 0.29411766, 0.32941177, 0.2       ,\n",
            "        0.35686275, 0.27450982, 0.32156864, 0.3764706 , 0.5529412 ,\n",
            "        0.74509805],\n",
            "       [0.47058824, 0.12156863, 0.29803923, 0.31764707, 0.19215687,\n",
            "        0.3529412 , 0.27058825, 0.2784314 , 0.3764706 , 0.56078434,\n",
            "        0.77254903],\n",
            "       [0.46666667, 0.11764706, 0.30588236, 0.31764707, 0.19215687,\n",
            "        0.34901962, 0.27450982, 0.2901961 , 0.3764706 , 0.62352943,\n",
            "        0.7254902 ],\n",
            "       [0.45490196, 0.11764706, 0.30588236, 0.30588236, 0.19215687,\n",
            "        0.34117648, 0.27450982, 0.3019608 , 0.3764706 , 0.56078434,\n",
            "        0.77254903],\n",
            "       [0.45490196, 0.11372549, 0.30980393, 0.29803923, 0.1882353 ,\n",
            "        0.32941177, 0.26666668, 0.32156864, 0.3764706 , 0.5372549 ,\n",
            "        0.77254903],\n",
            "       [0.4509804 , 0.11764706, 0.31764707, 0.29803923, 0.19215687,\n",
            "        0.33333334, 0.27058825, 0.33333334, 0.38431373, 0.5803922 ,\n",
            "        0.7019608 ],\n",
            "       [0.45490196, 0.11372549, 0.32156864, 0.2901961 , 0.18431373,\n",
            "        0.32156864, 0.26666668, 0.3254902 , 0.38039216, 0.5294118 ,\n",
            "        0.77254903],\n",
            "       [0.4509804 , 0.11372549, 0.3254902 , 0.2784314 , 0.18431373,\n",
            "        0.32156864, 0.26666668, 0.3372549 , 0.38431373, 0.56078434,\n",
            "        0.77254903],\n",
            "       [0.4509804 , 0.11372549, 0.32941177, 0.2784314 , 0.18431373,\n",
            "        0.31764707, 0.26666668, 0.3372549 , 0.38431373, 0.49803922,\n",
            "        0.7607843 ],\n",
            "       [0.44313726, 0.10980392, 0.3372549 , 0.27450982, 0.1882353 ,\n",
            "        0.31764707, 0.26666668, 0.34509805, 0.3882353 , 0.5294118 ,\n",
            "        0.7058824 ],\n",
            "       [0.42352942, 0.10980392, 0.33333334, 0.25490198, 0.18039216,\n",
            "        0.3019608 , 0.2627451 , 0.32941177, 0.38431373, 0.6117647 ,\n",
            "        0.74509805],\n",
            "       [0.3882353 , 0.10196079, 0.3254902 , 0.22745098, 0.16078432,\n",
            "        0.28627452, 0.24705882, 0.34509805, 0.3764706 , 0.5294118 ,\n",
            "        0.7137255 ],\n",
            "       [0.3764706 , 0.09803922, 0.3254902 , 0.21568628, 0.15686275,\n",
            "        0.2784314 , 0.24313726, 0.31764707, 0.3764706 , 0.5372549 ,\n",
            "        0.68235296]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "      dtype=int32)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Model**"
      ],
      "metadata": {
        "id": "dSBlaBAaAYEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "def create_mlp(input_size, hidden_size1, output_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.InputLayer(shape=(input_size,)),\n",
        "        layers.Dense(hidden_size1, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        # layers.Dense(hidden_size2, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        layers.Dense(output_size, kernel_regularizer=l2(0.001))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = create_mlp(INPUT_SIZE, HIDDEN_SIZE_1, OUTPUT_SIZE)\n",
        "opt = Adam(learning_rate=LEARNING_RATE, )\n",
        "model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RTuXXZnTKVJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "93da2c4c-112e-413a-a8b0-548963df6dfc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m192\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m68\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m260\u001b[0m (1.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> (1.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m260\u001b[0m (1.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> (1.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train from Scratch in FP32**"
      ],
      "metadata": {
        "id": "OSfrYoT5_vc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial non-quant training\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_dataset\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "trLEGuB-ItPp",
        "outputId": "48b4e334-f4ec-4c79-da7a-d8b7111e6c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.4018 - loss: 1.3634 - val_accuracy: 0.7297 - val_loss: 1.2829\n",
            "Epoch 2/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7241 - loss: 1.2557 - val_accuracy: 0.7207 - val_loss: 1.1961\n",
            "Epoch 3/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7187 - loss: 1.1776 - val_accuracy: 0.7009 - val_loss: 1.1199\n",
            "Epoch 4/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7100 - loss: 1.1119 - val_accuracy: 0.6973 - val_loss: 1.0570\n",
            "Epoch 5/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7223 - loss: 1.0422 - val_accuracy: 0.6919 - val_loss: 1.0018\n",
            "Epoch 6/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7309 - loss: 0.9756 - val_accuracy: 0.6955 - val_loss: 0.9562\n",
            "Epoch 7/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7243 - loss: 0.9309 - val_accuracy: 0.6955 - val_loss: 0.9210\n",
            "Epoch 8/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7191 - loss: 0.8960 - val_accuracy: 0.6973 - val_loss: 0.8916\n",
            "Epoch 9/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7234 - loss: 0.8537 - val_accuracy: 0.7027 - val_loss: 0.8671\n",
            "Epoch 10/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7192 - loss: 0.8255 - val_accuracy: 0.7045 - val_loss: 0.8447\n",
            "Epoch 11/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7221 - loss: 0.7976 - val_accuracy: 0.7099 - val_loss: 0.8235\n",
            "Epoch 12/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7297 - loss: 0.7698 - val_accuracy: 0.7189 - val_loss: 0.8064\n",
            "Epoch 13/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7147 - loss: 0.7621 - val_accuracy: 0.7207 - val_loss: 0.7908\n",
            "Epoch 14/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7373 - loss: 0.7117 - val_accuracy: 0.7207 - val_loss: 0.7744\n",
            "Epoch 15/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7245 - loss: 0.7153 - val_accuracy: 0.7117 - val_loss: 0.7613\n",
            "Epoch 16/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7220 - loss: 0.6990 - val_accuracy: 0.7099 - val_loss: 0.7470\n",
            "Epoch 17/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7356 - loss: 0.6817 - val_accuracy: 0.7135 - val_loss: 0.7345\n",
            "Epoch 18/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7412 - loss: 0.6776 - val_accuracy: 0.7117 - val_loss: 0.7240\n",
            "Epoch 19/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7505 - loss: 0.6583 - val_accuracy: 0.7171 - val_loss: 0.7114\n",
            "Epoch 20/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7680 - loss: 0.6280 - val_accuracy: 0.7189 - val_loss: 0.6995\n",
            "Epoch 21/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7605 - loss: 0.6337 - val_accuracy: 0.7243 - val_loss: 0.6889\n",
            "Epoch 22/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7667 - loss: 0.6172 - val_accuracy: 0.7225 - val_loss: 0.6800\n",
            "Epoch 23/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7686 - loss: 0.6150 - val_accuracy: 0.7279 - val_loss: 0.6686\n",
            "Epoch 24/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7771 - loss: 0.6074 - val_accuracy: 0.7297 - val_loss: 0.6588\n",
            "Epoch 25/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7856 - loss: 0.5812 - val_accuracy: 0.7315 - val_loss: 0.6501\n",
            "Epoch 26/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7806 - loss: 0.5907 - val_accuracy: 0.7297 - val_loss: 0.6414\n",
            "Epoch 27/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7842 - loss: 0.5718 - val_accuracy: 0.7351 - val_loss: 0.6294\n",
            "Epoch 28/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7933 - loss: 0.5533 - val_accuracy: 0.7351 - val_loss: 0.6198\n",
            "Epoch 29/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8122 - loss: 0.5385 - val_accuracy: 0.7351 - val_loss: 0.6103\n",
            "Epoch 30/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8014 - loss: 0.5388 - val_accuracy: 0.7423 - val_loss: 0.6000\n",
            "Epoch 31/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8156 - loss: 0.5324 - val_accuracy: 0.7495 - val_loss: 0.5915\n",
            "Epoch 32/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8103 - loss: 0.5205 - val_accuracy: 0.7514 - val_loss: 0.5823\n",
            "Epoch 33/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8081 - loss: 0.5202 - val_accuracy: 0.7604 - val_loss: 0.5722\n",
            "Epoch 34/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8196 - loss: 0.5028 - val_accuracy: 0.7604 - val_loss: 0.5646\n",
            "Epoch 35/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8411 - loss: 0.4822 - val_accuracy: 0.7604 - val_loss: 0.5564\n",
            "Epoch 36/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8360 - loss: 0.4776 - val_accuracy: 0.7604 - val_loss: 0.5463\n",
            "Epoch 37/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8317 - loss: 0.4765 - val_accuracy: 0.7640 - val_loss: 0.5420\n",
            "Epoch 38/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8375 - loss: 0.4734 - val_accuracy: 0.7658 - val_loss: 0.5362\n",
            "Epoch 39/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8285 - loss: 0.4761 - val_accuracy: 0.7676 - val_loss: 0.5288\n",
            "Epoch 40/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8391 - loss: 0.4663 - val_accuracy: 0.7712 - val_loss: 0.5228\n",
            "Epoch 41/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8476 - loss: 0.4519 - val_accuracy: 0.7766 - val_loss: 0.5161\n",
            "Epoch 42/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8480 - loss: 0.4550 - val_accuracy: 0.7766 - val_loss: 0.5128\n",
            "Epoch 43/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8520 - loss: 0.4400 - val_accuracy: 0.7856 - val_loss: 0.5051\n",
            "Epoch 44/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8542 - loss: 0.4391 - val_accuracy: 0.7856 - val_loss: 0.5045\n",
            "Epoch 45/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8586 - loss: 0.4323 - val_accuracy: 0.8036 - val_loss: 0.4953\n",
            "Epoch 46/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8607 - loss: 0.4334 - val_accuracy: 0.7982 - val_loss: 0.4943\n",
            "Epoch 47/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8620 - loss: 0.4167 - val_accuracy: 0.8234 - val_loss: 0.4891\n",
            "Epoch 48/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8680 - loss: 0.4229 - val_accuracy: 0.8216 - val_loss: 0.4857\n",
            "Epoch 49/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8678 - loss: 0.4162 - val_accuracy: 0.8288 - val_loss: 0.4809\n",
            "Epoch 50/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8710 - loss: 0.4180 - val_accuracy: 0.8234 - val_loss: 0.4797\n",
            "Epoch 51/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8706 - loss: 0.4071 - val_accuracy: 0.8378 - val_loss: 0.4745\n",
            "Epoch 52/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8844 - loss: 0.4124 - val_accuracy: 0.8324 - val_loss: 0.4733\n",
            "Epoch 53/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8754 - loss: 0.4007 - val_accuracy: 0.8486 - val_loss: 0.4689\n",
            "Epoch 54/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8926 - loss: 0.4015 - val_accuracy: 0.8378 - val_loss: 0.4659\n",
            "Epoch 55/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8871 - loss: 0.3942 - val_accuracy: 0.8432 - val_loss: 0.4640\n",
            "Epoch 56/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8868 - loss: 0.3986 - val_accuracy: 0.8432 - val_loss: 0.4603\n",
            "Epoch 57/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8842 - loss: 0.3885 - val_accuracy: 0.8541 - val_loss: 0.4602\n",
            "Epoch 58/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8836 - loss: 0.3923 - val_accuracy: 0.8523 - val_loss: 0.4556\n",
            "Epoch 59/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8881 - loss: 0.3901 - val_accuracy: 0.8559 - val_loss: 0.4538\n",
            "Epoch 60/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8961 - loss: 0.3822 - val_accuracy: 0.8595 - val_loss: 0.4540\n",
            "Epoch 61/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8893 - loss: 0.3888 - val_accuracy: 0.8577 - val_loss: 0.4471\n",
            "Epoch 62/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8831 - loss: 0.3893 - val_accuracy: 0.8667 - val_loss: 0.4516\n",
            "Epoch 63/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9020 - loss: 0.3743 - val_accuracy: 0.8595 - val_loss: 0.4449\n",
            "Epoch 64/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8860 - loss: 0.3822 - val_accuracy: 0.8613 - val_loss: 0.4435\n",
            "Epoch 65/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8934 - loss: 0.3732 - val_accuracy: 0.8685 - val_loss: 0.4437\n",
            "Epoch 66/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9012 - loss: 0.3812 - val_accuracy: 0.8613 - val_loss: 0.4401\n",
            "Epoch 67/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8959 - loss: 0.3738 - val_accuracy: 0.8685 - val_loss: 0.4402\n",
            "Epoch 68/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8917 - loss: 0.3740 - val_accuracy: 0.8685 - val_loss: 0.4380\n",
            "Epoch 69/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8975 - loss: 0.3738 - val_accuracy: 0.8721 - val_loss: 0.4354\n",
            "Epoch 70/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8992 - loss: 0.3734 - val_accuracy: 0.8649 - val_loss: 0.4347\n",
            "Epoch 71/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8937 - loss: 0.3735 - val_accuracy: 0.8685 - val_loss: 0.4327\n",
            "Epoch 72/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9023 - loss: 0.3665 - val_accuracy: 0.8685 - val_loss: 0.4306\n",
            "Epoch 73/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9055 - loss: 0.3699 - val_accuracy: 0.8667 - val_loss: 0.4292\n",
            "Epoch 74/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8945 - loss: 0.3724 - val_accuracy: 0.8739 - val_loss: 0.4273\n",
            "Epoch 75/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8999 - loss: 0.3661 - val_accuracy: 0.8649 - val_loss: 0.4296\n",
            "Epoch 76/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9096 - loss: 0.3613 - val_accuracy: 0.8685 - val_loss: 0.4245\n",
            "Epoch 77/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9022 - loss: 0.3648 - val_accuracy: 0.8721 - val_loss: 0.4260\n",
            "Epoch 78/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9154 - loss: 0.3567 - val_accuracy: 0.8685 - val_loss: 0.4234\n",
            "Epoch 79/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9020 - loss: 0.3716 - val_accuracy: 0.8703 - val_loss: 0.4210\n",
            "Epoch 80/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9001 - loss: 0.3603 - val_accuracy: 0.8775 - val_loss: 0.4221\n",
            "Epoch 81/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9154 - loss: 0.3475 - val_accuracy: 0.8757 - val_loss: 0.4204\n",
            "Epoch 82/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9137 - loss: 0.3565 - val_accuracy: 0.8757 - val_loss: 0.4185\n",
            "Epoch 83/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9088 - loss: 0.3531 - val_accuracy: 0.8775 - val_loss: 0.4162\n",
            "Epoch 84/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9057 - loss: 0.3630 - val_accuracy: 0.8811 - val_loss: 0.4158\n",
            "Epoch 85/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9096 - loss: 0.3636 - val_accuracy: 0.8757 - val_loss: 0.4160\n",
            "Epoch 86/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9117 - loss: 0.3562 - val_accuracy: 0.8811 - val_loss: 0.4141\n",
            "Epoch 87/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9026 - loss: 0.3582 - val_accuracy: 0.8793 - val_loss: 0.4139\n",
            "Epoch 88/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9167 - loss: 0.3495 - val_accuracy: 0.8739 - val_loss: 0.4143\n",
            "Epoch 89/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9136 - loss: 0.3561 - val_accuracy: 0.8847 - val_loss: 0.4079\n",
            "Epoch 90/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9134 - loss: 0.3489 - val_accuracy: 0.8829 - val_loss: 0.4088\n",
            "Epoch 91/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9232 - loss: 0.3373 - val_accuracy: 0.8793 - val_loss: 0.4113\n",
            "Epoch 92/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9264 - loss: 0.3418 - val_accuracy: 0.8847 - val_loss: 0.4061\n",
            "Epoch 93/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9075 - loss: 0.3552 - val_accuracy: 0.8847 - val_loss: 0.4050\n",
            "Epoch 94/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9174 - loss: 0.3482 - val_accuracy: 0.8793 - val_loss: 0.4083\n",
            "Epoch 95/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9188 - loss: 0.3457 - val_accuracy: 0.8793 - val_loss: 0.4049\n",
            "Epoch 96/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9206 - loss: 0.3393 - val_accuracy: 0.8829 - val_loss: 0.4034\n",
            "Epoch 97/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9152 - loss: 0.3422 - val_accuracy: 0.8793 - val_loss: 0.4049\n",
            "Epoch 98/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9107 - loss: 0.3475 - val_accuracy: 0.8793 - val_loss: 0.4055\n",
            "Epoch 99/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9267 - loss: 0.3429 - val_accuracy: 0.8757 - val_loss: 0.4045\n",
            "Epoch 100/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9252 - loss: 0.3376 - val_accuracy: 0.8829 - val_loss: 0.4014\n",
            "Epoch 101/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9238 - loss: 0.3495 - val_accuracy: 0.8793 - val_loss: 0.4016\n",
            "Epoch 102/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9150 - loss: 0.3501 - val_accuracy: 0.8847 - val_loss: 0.3989\n",
            "Epoch 103/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9173 - loss: 0.3460 - val_accuracy: 0.8829 - val_loss: 0.3998\n",
            "Epoch 104/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9239 - loss: 0.3367 - val_accuracy: 0.8829 - val_loss: 0.3983\n",
            "Epoch 105/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9209 - loss: 0.3388 - val_accuracy: 0.8811 - val_loss: 0.3981\n",
            "Epoch 106/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9254 - loss: 0.3380 - val_accuracy: 0.8847 - val_loss: 0.3965\n",
            "Epoch 107/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9255 - loss: 0.3402 - val_accuracy: 0.8847 - val_loss: 0.3951\n",
            "Epoch 108/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9178 - loss: 0.3405 - val_accuracy: 0.8865 - val_loss: 0.3946\n",
            "Epoch 109/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9263 - loss: 0.3383 - val_accuracy: 0.8847 - val_loss: 0.3964\n",
            "Epoch 110/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9271 - loss: 0.3337 - val_accuracy: 0.8829 - val_loss: 0.3953\n",
            "Epoch 111/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9292 - loss: 0.3420 - val_accuracy: 0.8811 - val_loss: 0.3968\n",
            "Epoch 112/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9211 - loss: 0.3394 - val_accuracy: 0.8847 - val_loss: 0.3917\n",
            "Epoch 113/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9275 - loss: 0.3340 - val_accuracy: 0.8829 - val_loss: 0.3932\n",
            "Epoch 114/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9214 - loss: 0.3399 - val_accuracy: 0.8829 - val_loss: 0.3923\n",
            "Epoch 115/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9258 - loss: 0.3290 - val_accuracy: 0.8901 - val_loss: 0.3882\n",
            "Epoch 116/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9297 - loss: 0.3337 - val_accuracy: 0.8829 - val_loss: 0.3903\n",
            "Epoch 117/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9299 - loss: 0.3290 - val_accuracy: 0.8865 - val_loss: 0.3918\n",
            "Epoch 118/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9280 - loss: 0.3304 - val_accuracy: 0.8829 - val_loss: 0.3888\n",
            "Epoch 119/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9289 - loss: 0.3353 - val_accuracy: 0.8865 - val_loss: 0.3872\n",
            "Epoch 120/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9259 - loss: 0.3373 - val_accuracy: 0.8901 - val_loss: 0.3867\n",
            "Epoch 121/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9293 - loss: 0.3303 - val_accuracy: 0.8847 - val_loss: 0.3859\n",
            "Epoch 122/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9325 - loss: 0.3314 - val_accuracy: 0.8883 - val_loss: 0.3877\n",
            "Epoch 123/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9266 - loss: 0.3337 - val_accuracy: 0.8865 - val_loss: 0.3854\n",
            "Epoch 124/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9289 - loss: 0.3283 - val_accuracy: 0.8883 - val_loss: 0.3843\n",
            "Epoch 125/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9269 - loss: 0.3358 - val_accuracy: 0.8865 - val_loss: 0.3839\n",
            "Epoch 126/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9314 - loss: 0.3311 - val_accuracy: 0.8847 - val_loss: 0.3852\n",
            "Epoch 127/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9298 - loss: 0.3285 - val_accuracy: 0.8883 - val_loss: 0.3818\n",
            "Epoch 128/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9314 - loss: 0.3275 - val_accuracy: 0.8865 - val_loss: 0.3831\n",
            "Epoch 129/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9289 - loss: 0.3339 - val_accuracy: 0.8865 - val_loss: 0.3831\n",
            "Epoch 130/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9347 - loss: 0.3234 - val_accuracy: 0.8919 - val_loss: 0.3817\n",
            "Epoch 131/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9301 - loss: 0.3319 - val_accuracy: 0.8883 - val_loss: 0.3804\n",
            "Epoch 132/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9252 - loss: 0.3304 - val_accuracy: 0.8865 - val_loss: 0.3806\n",
            "Epoch 133/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9303 - loss: 0.3278 - val_accuracy: 0.8901 - val_loss: 0.3800\n",
            "Epoch 134/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9301 - loss: 0.3278 - val_accuracy: 0.8919 - val_loss: 0.3786\n",
            "Epoch 135/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9326 - loss: 0.3256 - val_accuracy: 0.8919 - val_loss: 0.3782\n",
            "Epoch 136/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9389 - loss: 0.3247 - val_accuracy: 0.8865 - val_loss: 0.3826\n",
            "Epoch 137/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9307 - loss: 0.3286 - val_accuracy: 0.8937 - val_loss: 0.3794\n",
            "Epoch 138/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9364 - loss: 0.3256 - val_accuracy: 0.8865 - val_loss: 0.3788\n",
            "Epoch 139/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9328 - loss: 0.3260 - val_accuracy: 0.8901 - val_loss: 0.3788\n",
            "Epoch 140/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9340 - loss: 0.3269 - val_accuracy: 0.8865 - val_loss: 0.3781\n",
            "Epoch 141/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9290 - loss: 0.3290 - val_accuracy: 0.8919 - val_loss: 0.3761\n",
            "Epoch 142/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9322 - loss: 0.3298 - val_accuracy: 0.8901 - val_loss: 0.3778\n",
            "Epoch 143/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9310 - loss: 0.3251 - val_accuracy: 0.8883 - val_loss: 0.3750\n",
            "Epoch 144/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9298 - loss: 0.3317 - val_accuracy: 0.8937 - val_loss: 0.3748\n",
            "Epoch 145/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9336 - loss: 0.3226 - val_accuracy: 0.8883 - val_loss: 0.3767\n",
            "Epoch 146/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9319 - loss: 0.3264 - val_accuracy: 0.8883 - val_loss: 0.3758\n",
            "Epoch 147/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9271 - loss: 0.3306 - val_accuracy: 0.8919 - val_loss: 0.3742\n",
            "Epoch 148/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9395 - loss: 0.3259 - val_accuracy: 0.8919 - val_loss: 0.3735\n",
            "Epoch 149/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9315 - loss: 0.3282 - val_accuracy: 0.8937 - val_loss: 0.3722\n",
            "Epoch 150/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9319 - loss: 0.3261 - val_accuracy: 0.8937 - val_loss: 0.3741\n",
            "Epoch 151/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9298 - loss: 0.3248 - val_accuracy: 0.8955 - val_loss: 0.3710\n",
            "Epoch 152/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9314 - loss: 0.3249 - val_accuracy: 0.8937 - val_loss: 0.3725\n",
            "Epoch 153/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9294 - loss: 0.3262 - val_accuracy: 0.8919 - val_loss: 0.3726\n",
            "Epoch 154/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9354 - loss: 0.3232 - val_accuracy: 0.8901 - val_loss: 0.3737\n",
            "Epoch 155/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9392 - loss: 0.3234 - val_accuracy: 0.8973 - val_loss: 0.3690\n",
            "Epoch 156/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9297 - loss: 0.3246 - val_accuracy: 0.8955 - val_loss: 0.3692\n",
            "Epoch 157/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9361 - loss: 0.3182 - val_accuracy: 0.8901 - val_loss: 0.3740\n",
            "Epoch 158/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9298 - loss: 0.3257 - val_accuracy: 0.8991 - val_loss: 0.3675\n",
            "Epoch 159/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9361 - loss: 0.3248 - val_accuracy: 0.8901 - val_loss: 0.3712\n",
            "Epoch 160/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9299 - loss: 0.3283 - val_accuracy: 0.8955 - val_loss: 0.3694\n",
            "Epoch 161/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9339 - loss: 0.3216 - val_accuracy: 0.8937 - val_loss: 0.3680\n",
            "Epoch 162/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9355 - loss: 0.3261 - val_accuracy: 0.8973 - val_loss: 0.3681\n",
            "Epoch 163/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9345 - loss: 0.3227 - val_accuracy: 0.8919 - val_loss: 0.3676\n",
            "Epoch 164/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9377 - loss: 0.3214 - val_accuracy: 0.8955 - val_loss: 0.3692\n",
            "Epoch 165/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9405 - loss: 0.3154 - val_accuracy: 0.8973 - val_loss: 0.3680\n",
            "Epoch 166/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9342 - loss: 0.3221 - val_accuracy: 0.8955 - val_loss: 0.3682\n",
            "Epoch 167/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9303 - loss: 0.3255 - val_accuracy: 0.8991 - val_loss: 0.3668\n",
            "Epoch 168/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9324 - loss: 0.3287 - val_accuracy: 0.8919 - val_loss: 0.3678\n",
            "Epoch 169/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9390 - loss: 0.3178 - val_accuracy: 0.8955 - val_loss: 0.3674\n",
            "Epoch 170/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9347 - loss: 0.3236 - val_accuracy: 0.8991 - val_loss: 0.3664\n",
            "Epoch 171/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9325 - loss: 0.3259 - val_accuracy: 0.8955 - val_loss: 0.3648\n",
            "Epoch 172/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9302 - loss: 0.3242 - val_accuracy: 0.8991 - val_loss: 0.3652\n",
            "Epoch 173/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9375 - loss: 0.3269 - val_accuracy: 0.9009 - val_loss: 0.3639\n",
            "Epoch 174/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9361 - loss: 0.3199 - val_accuracy: 0.8937 - val_loss: 0.3674\n",
            "Epoch 175/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9388 - loss: 0.3180 - val_accuracy: 0.9009 - val_loss: 0.3640\n",
            "Epoch 176/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9337 - loss: 0.3191 - val_accuracy: 0.8991 - val_loss: 0.3643\n",
            "Epoch 177/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9334 - loss: 0.3283 - val_accuracy: 0.8973 - val_loss: 0.3665\n",
            "Epoch 178/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9330 - loss: 0.3186 - val_accuracy: 0.8973 - val_loss: 0.3641\n",
            "Epoch 179/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9332 - loss: 0.3198 - val_accuracy: 0.8973 - val_loss: 0.3648\n",
            "Epoch 180/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9359 - loss: 0.3183 - val_accuracy: 0.8991 - val_loss: 0.3633\n",
            "Epoch 181/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9356 - loss: 0.3207 - val_accuracy: 0.8955 - val_loss: 0.3636\n",
            "Epoch 182/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9363 - loss: 0.3219 - val_accuracy: 0.9009 - val_loss: 0.3610\n",
            "Epoch 183/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9346 - loss: 0.3183 - val_accuracy: 0.9009 - val_loss: 0.3615\n",
            "Epoch 184/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9316 - loss: 0.3278 - val_accuracy: 0.8973 - val_loss: 0.3649\n",
            "Epoch 185/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9320 - loss: 0.3242 - val_accuracy: 0.9009 - val_loss: 0.3612\n",
            "Epoch 186/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9374 - loss: 0.3195 - val_accuracy: 0.9027 - val_loss: 0.3620\n",
            "Epoch 187/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9329 - loss: 0.3236 - val_accuracy: 0.8991 - val_loss: 0.3625\n",
            "Epoch 188/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9343 - loss: 0.3233 - val_accuracy: 0.9009 - val_loss: 0.3609\n",
            "Epoch 189/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9426 - loss: 0.3139 - val_accuracy: 0.8955 - val_loss: 0.3640\n",
            "Epoch 190/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9399 - loss: 0.3187 - val_accuracy: 0.9045 - val_loss: 0.3604\n",
            "Epoch 191/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9436 - loss: 0.3093 - val_accuracy: 0.8991 - val_loss: 0.3632\n",
            "Epoch 192/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9387 - loss: 0.3195 - val_accuracy: 0.9009 - val_loss: 0.3606\n",
            "Epoch 193/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9419 - loss: 0.3199 - val_accuracy: 0.8991 - val_loss: 0.3606\n",
            "Epoch 194/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9347 - loss: 0.3227 - val_accuracy: 0.9063 - val_loss: 0.3603\n",
            "Epoch 195/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9373 - loss: 0.3229 - val_accuracy: 0.9009 - val_loss: 0.3603\n",
            "Epoch 196/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9427 - loss: 0.3182 - val_accuracy: 0.9009 - val_loss: 0.3608\n",
            "Epoch 197/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9395 - loss: 0.3163 - val_accuracy: 0.8991 - val_loss: 0.3620\n",
            "Epoch 198/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9424 - loss: 0.3183 - val_accuracy: 0.9009 - val_loss: 0.3593\n",
            "Epoch 199/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9379 - loss: 0.3196 - val_accuracy: 0.9009 - val_loss: 0.3578\n",
            "Epoch 200/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9404 - loss: 0.3140 - val_accuracy: 0.8991 - val_loss: 0.3590\n",
            "Epoch 201/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9361 - loss: 0.3199 - val_accuracy: 0.9009 - val_loss: 0.3600\n",
            "Epoch 202/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9404 - loss: 0.3156 - val_accuracy: 0.8973 - val_loss: 0.3609\n",
            "Epoch 203/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9412 - loss: 0.3188 - val_accuracy: 0.9009 - val_loss: 0.3573\n",
            "Epoch 204/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9439 - loss: 0.3202 - val_accuracy: 0.9027 - val_loss: 0.3586\n",
            "Epoch 205/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9385 - loss: 0.3162 - val_accuracy: 0.9009 - val_loss: 0.3595\n",
            "Epoch 206/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9381 - loss: 0.3180 - val_accuracy: 0.9009 - val_loss: 0.3580\n",
            "Epoch 207/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9398 - loss: 0.3161 - val_accuracy: 0.9009 - val_loss: 0.3579\n",
            "Epoch 208/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9367 - loss: 0.3211 - val_accuracy: 0.9009 - val_loss: 0.3596\n",
            "Epoch 209/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9395 - loss: 0.3177 - val_accuracy: 0.9009 - val_loss: 0.3589\n",
            "Epoch 210/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9403 - loss: 0.3132 - val_accuracy: 0.8991 - val_loss: 0.3579\n",
            "Epoch 211/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9390 - loss: 0.3169 - val_accuracy: 0.8991 - val_loss: 0.3594\n",
            "Epoch 212/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9415 - loss: 0.3121 - val_accuracy: 0.9045 - val_loss: 0.3564\n",
            "Epoch 213/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9372 - loss: 0.3197 - val_accuracy: 0.9009 - val_loss: 0.3568\n",
            "Epoch 214/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9337 - loss: 0.3216 - val_accuracy: 0.9009 - val_loss: 0.3563\n",
            "Epoch 215/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9382 - loss: 0.3178 - val_accuracy: 0.9009 - val_loss: 0.3557\n",
            "Epoch 216/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9394 - loss: 0.3153 - val_accuracy: 0.9009 - val_loss: 0.3568\n",
            "Epoch 217/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9403 - loss: 0.3154 - val_accuracy: 0.9045 - val_loss: 0.3565\n",
            "Epoch 218/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9445 - loss: 0.3056 - val_accuracy: 0.8991 - val_loss: 0.3584\n",
            "Epoch 219/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9487 - loss: 0.3114 - val_accuracy: 0.9009 - val_loss: 0.3558\n",
            "Epoch 220/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9383 - loss: 0.3225 - val_accuracy: 0.9009 - val_loss: 0.3560\n",
            "Epoch 221/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9384 - loss: 0.3188 - val_accuracy: 0.9027 - val_loss: 0.3576\n",
            "Epoch 222/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9358 - loss: 0.3185 - val_accuracy: 0.8991 - val_loss: 0.3573\n",
            "Epoch 223/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9413 - loss: 0.3132 - val_accuracy: 0.9081 - val_loss: 0.3538\n",
            "Epoch 224/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9406 - loss: 0.3125 - val_accuracy: 0.9045 - val_loss: 0.3565\n",
            "Epoch 225/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9416 - loss: 0.3171 - val_accuracy: 0.9009 - val_loss: 0.3559\n",
            "Epoch 226/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9339 - loss: 0.3261 - val_accuracy: 0.9063 - val_loss: 0.3544\n",
            "Epoch 227/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9363 - loss: 0.3196 - val_accuracy: 0.9045 - val_loss: 0.3541\n",
            "Epoch 228/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9433 - loss: 0.3126 - val_accuracy: 0.9045 - val_loss: 0.3558\n",
            "Epoch 229/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9391 - loss: 0.3180 - val_accuracy: 0.9081 - val_loss: 0.3547\n",
            "Epoch 230/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9374 - loss: 0.3125 - val_accuracy: 0.9045 - val_loss: 0.3554\n",
            "Epoch 231/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9452 - loss: 0.3077 - val_accuracy: 0.9045 - val_loss: 0.3556\n",
            "Epoch 232/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9377 - loss: 0.3202 - val_accuracy: 0.9081 - val_loss: 0.3536\n",
            "Epoch 233/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9428 - loss: 0.3120 - val_accuracy: 0.8991 - val_loss: 0.3581\n",
            "Epoch 234/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9475 - loss: 0.3080 - val_accuracy: 0.9045 - val_loss: 0.3532\n",
            "Epoch 235/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9354 - loss: 0.3234 - val_accuracy: 0.9099 - val_loss: 0.3531\n",
            "Epoch 236/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9391 - loss: 0.3156 - val_accuracy: 0.9045 - val_loss: 0.3549\n",
            "Epoch 237/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9421 - loss: 0.3168 - val_accuracy: 0.9045 - val_loss: 0.3547\n",
            "Epoch 238/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9419 - loss: 0.3099 - val_accuracy: 0.9081 - val_loss: 0.3516\n",
            "Epoch 239/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9399 - loss: 0.3195 - val_accuracy: 0.9099 - val_loss: 0.3522\n",
            "Epoch 240/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9400 - loss: 0.3154 - val_accuracy: 0.9027 - val_loss: 0.3562\n",
            "Epoch 241/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9445 - loss: 0.3152 - val_accuracy: 0.9009 - val_loss: 0.3546\n",
            "Epoch 242/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9401 - loss: 0.3156 - val_accuracy: 0.9027 - val_loss: 0.3548\n",
            "Epoch 243/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9424 - loss: 0.3091 - val_accuracy: 0.9063 - val_loss: 0.3539\n",
            "Epoch 244/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9411 - loss: 0.3119 - val_accuracy: 0.9063 - val_loss: 0.3541\n",
            "Epoch 245/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9455 - loss: 0.3104 - val_accuracy: 0.9045 - val_loss: 0.3542\n",
            "Epoch 246/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9398 - loss: 0.3140 - val_accuracy: 0.9081 - val_loss: 0.3533\n",
            "Epoch 247/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9432 - loss: 0.3124 - val_accuracy: 0.9045 - val_loss: 0.3544\n",
            "Epoch 248/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9415 - loss: 0.3182 - val_accuracy: 0.9117 - val_loss: 0.3512\n",
            "Epoch 249/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9397 - loss: 0.3147 - val_accuracy: 0.9045 - val_loss: 0.3539\n",
            "Epoch 250/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9406 - loss: 0.3149 - val_accuracy: 0.9081 - val_loss: 0.3523\n",
            "Epoch 251/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9392 - loss: 0.3140 - val_accuracy: 0.9063 - val_loss: 0.3531\n",
            "Epoch 252/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9447 - loss: 0.3159 - val_accuracy: 0.9045 - val_loss: 0.3517\n",
            "Epoch 253/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9407 - loss: 0.3115 - val_accuracy: 0.9099 - val_loss: 0.3525\n",
            "Epoch 254/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9415 - loss: 0.3146 - val_accuracy: 0.9045 - val_loss: 0.3526\n",
            "Epoch 255/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9458 - loss: 0.3130 - val_accuracy: 0.9099 - val_loss: 0.3505\n",
            "Epoch 256/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9395 - loss: 0.3131 - val_accuracy: 0.9063 - val_loss: 0.3518\n",
            "Epoch 257/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9466 - loss: 0.3137 - val_accuracy: 0.9063 - val_loss: 0.3539\n",
            "Epoch 258/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9376 - loss: 0.3221 - val_accuracy: 0.9099 - val_loss: 0.3515\n",
            "Epoch 259/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9382 - loss: 0.3125 - val_accuracy: 0.9081 - val_loss: 0.3526\n",
            "Epoch 260/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9424 - loss: 0.3107 - val_accuracy: 0.9081 - val_loss: 0.3519\n",
            "Epoch 261/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9468 - loss: 0.3121 - val_accuracy: 0.9081 - val_loss: 0.3510\n",
            "Epoch 262/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9368 - loss: 0.3150 - val_accuracy: 0.9081 - val_loss: 0.3523\n",
            "Epoch 263/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9467 - loss: 0.3124 - val_accuracy: 0.9099 - val_loss: 0.3501\n",
            "Epoch 264/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9449 - loss: 0.3073 - val_accuracy: 0.9099 - val_loss: 0.3506\n",
            "Epoch 265/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9464 - loss: 0.3074 - val_accuracy: 0.9081 - val_loss: 0.3520\n",
            "Epoch 266/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9458 - loss: 0.3148 - val_accuracy: 0.9099 - val_loss: 0.3505\n",
            "Epoch 267/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9454 - loss: 0.3110 - val_accuracy: 0.9081 - val_loss: 0.3510\n",
            "Epoch 268/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9373 - loss: 0.3168 - val_accuracy: 0.9081 - val_loss: 0.3507\n",
            "Epoch 269/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9385 - loss: 0.3177 - val_accuracy: 0.9081 - val_loss: 0.3499\n",
            "Epoch 270/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9427 - loss: 0.3086 - val_accuracy: 0.9063 - val_loss: 0.3537\n",
            "Epoch 271/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9442 - loss: 0.3113 - val_accuracy: 0.9081 - val_loss: 0.3523\n",
            "Epoch 272/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9405 - loss: 0.3107 - val_accuracy: 0.9081 - val_loss: 0.3517\n",
            "Epoch 273/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9410 - loss: 0.3179 - val_accuracy: 0.9099 - val_loss: 0.3512\n",
            "Epoch 274/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9379 - loss: 0.3160 - val_accuracy: 0.9099 - val_loss: 0.3510\n",
            "Epoch 275/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9408 - loss: 0.3155 - val_accuracy: 0.9117 - val_loss: 0.3507\n",
            "Epoch 276/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9438 - loss: 0.3166 - val_accuracy: 0.9081 - val_loss: 0.3515\n",
            "Epoch 277/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9387 - loss: 0.3124 - val_accuracy: 0.9099 - val_loss: 0.3496\n",
            "Epoch 278/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9409 - loss: 0.3100 - val_accuracy: 0.9099 - val_loss: 0.3499\n",
            "Epoch 279/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9397 - loss: 0.3154 - val_accuracy: 0.9099 - val_loss: 0.3506\n",
            "Epoch 280/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9443 - loss: 0.3083 - val_accuracy: 0.9081 - val_loss: 0.3510\n",
            "Epoch 281/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9435 - loss: 0.3117 - val_accuracy: 0.9099 - val_loss: 0.3514\n",
            "Epoch 282/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9385 - loss: 0.3143 - val_accuracy: 0.9117 - val_loss: 0.3488\n",
            "Epoch 283/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9455 - loss: 0.3083 - val_accuracy: 0.9099 - val_loss: 0.3502\n",
            "Epoch 284/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9405 - loss: 0.3144 - val_accuracy: 0.9099 - val_loss: 0.3496\n",
            "Epoch 285/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9468 - loss: 0.3084 - val_accuracy: 0.9081 - val_loss: 0.3517\n",
            "Epoch 286/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9477 - loss: 0.3088 - val_accuracy: 0.9117 - val_loss: 0.3486\n",
            "Epoch 287/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9425 - loss: 0.3087 - val_accuracy: 0.9081 - val_loss: 0.3529\n",
            "Epoch 288/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9422 - loss: 0.3163 - val_accuracy: 0.9117 - val_loss: 0.3487\n",
            "Epoch 289/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9444 - loss: 0.3111 - val_accuracy: 0.9063 - val_loss: 0.3504\n",
            "Epoch 290/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9431 - loss: 0.3122 - val_accuracy: 0.9099 - val_loss: 0.3499\n",
            "Epoch 291/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9413 - loss: 0.3158 - val_accuracy: 0.9099 - val_loss: 0.3487\n",
            "Epoch 292/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9410 - loss: 0.3109 - val_accuracy: 0.9135 - val_loss: 0.3495\n",
            "Epoch 293/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9381 - loss: 0.3121 - val_accuracy: 0.9081 - val_loss: 0.3505\n",
            "Epoch 294/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9424 - loss: 0.3125 - val_accuracy: 0.9117 - val_loss: 0.3484\n",
            "Epoch 295/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9483 - loss: 0.3091 - val_accuracy: 0.9117 - val_loss: 0.3494\n",
            "Epoch 296/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9418 - loss: 0.3134 - val_accuracy: 0.9117 - val_loss: 0.3498\n",
            "Epoch 297/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9437 - loss: 0.3145 - val_accuracy: 0.9135 - val_loss: 0.3467\n",
            "Epoch 298/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9381 - loss: 0.3188 - val_accuracy: 0.9135 - val_loss: 0.3488\n",
            "Epoch 299/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9444 - loss: 0.3125 - val_accuracy: 0.9153 - val_loss: 0.3490\n",
            "Epoch 300/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9425 - loss: 0.3094 - val_accuracy: 0.9081 - val_loss: 0.3511\n",
            "Epoch 301/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9422 - loss: 0.3088 - val_accuracy: 0.9117 - val_loss: 0.3489\n",
            "Epoch 302/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9489 - loss: 0.3101 - val_accuracy: 0.9117 - val_loss: 0.3481\n",
            "Epoch 303/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9420 - loss: 0.3159 - val_accuracy: 0.9135 - val_loss: 0.3481\n",
            "Epoch 304/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9403 - loss: 0.3180 - val_accuracy: 0.9135 - val_loss: 0.3474\n",
            "Epoch 305/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9382 - loss: 0.3190 - val_accuracy: 0.9099 - val_loss: 0.3486\n",
            "Epoch 306/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9433 - loss: 0.3103 - val_accuracy: 0.9117 - val_loss: 0.3480\n",
            "Epoch 307/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9412 - loss: 0.3113 - val_accuracy: 0.9117 - val_loss: 0.3490\n",
            "Epoch 308/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9449 - loss: 0.3058 - val_accuracy: 0.9081 - val_loss: 0.3509\n",
            "Epoch 309/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9416 - loss: 0.3128 - val_accuracy: 0.9099 - val_loss: 0.3495\n",
            "Epoch 310/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9424 - loss: 0.3111 - val_accuracy: 0.9117 - val_loss: 0.3480\n",
            "Epoch 311/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9380 - loss: 0.3140 - val_accuracy: 0.9135 - val_loss: 0.3477\n",
            "Epoch 312/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9469 - loss: 0.3064 - val_accuracy: 0.9117 - val_loss: 0.3492\n",
            "Epoch 313/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9432 - loss: 0.3115 - val_accuracy: 0.9135 - val_loss: 0.3476\n",
            "Epoch 314/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9444 - loss: 0.3053 - val_accuracy: 0.9117 - val_loss: 0.3499\n",
            "Epoch 315/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9437 - loss: 0.3093 - val_accuracy: 0.9117 - val_loss: 0.3486\n",
            "Epoch 316/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9438 - loss: 0.3104 - val_accuracy: 0.9117 - val_loss: 0.3467\n",
            "Epoch 317/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9396 - loss: 0.3129 - val_accuracy: 0.9117 - val_loss: 0.3483\n",
            "Epoch 318/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9383 - loss: 0.3159 - val_accuracy: 0.9099 - val_loss: 0.3495\n",
            "Epoch 319/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9379 - loss: 0.3112 - val_accuracy: 0.9171 - val_loss: 0.3467\n",
            "Epoch 320/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9416 - loss: 0.3172 - val_accuracy: 0.9135 - val_loss: 0.3471\n",
            "Epoch 321/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9436 - loss: 0.3128 - val_accuracy: 0.9099 - val_loss: 0.3493\n",
            "Epoch 322/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9446 - loss: 0.3092 - val_accuracy: 0.9099 - val_loss: 0.3495\n",
            "Epoch 323/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.3107 - val_accuracy: 0.9099 - val_loss: 0.3484\n",
            "Epoch 324/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9429 - loss: 0.3068 - val_accuracy: 0.9117 - val_loss: 0.3510\n",
            "Epoch 325/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9456 - loss: 0.3116 - val_accuracy: 0.9135 - val_loss: 0.3458\n",
            "Epoch 326/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9457 - loss: 0.3084 - val_accuracy: 0.9117 - val_loss: 0.3489\n",
            "Epoch 327/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9441 - loss: 0.3114 - val_accuracy: 0.9153 - val_loss: 0.3479\n",
            "Epoch 328/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9456 - loss: 0.3057 - val_accuracy: 0.9135 - val_loss: 0.3472\n",
            "Epoch 329/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9412 - loss: 0.3075 - val_accuracy: 0.9117 - val_loss: 0.3487\n",
            "Epoch 330/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9405 - loss: 0.3113 - val_accuracy: 0.9153 - val_loss: 0.3474\n",
            "Epoch 331/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9413 - loss: 0.3102 - val_accuracy: 0.9171 - val_loss: 0.3484\n",
            "Epoch 332/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9474 - loss: 0.3113 - val_accuracy: 0.9135 - val_loss: 0.3467\n",
            "Epoch 333/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9418 - loss: 0.3155 - val_accuracy: 0.9153 - val_loss: 0.3473\n",
            "Epoch 334/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9453 - loss: 0.3129 - val_accuracy: 0.9153 - val_loss: 0.3460\n",
            "Epoch 335/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9407 - loss: 0.3122 - val_accuracy: 0.9117 - val_loss: 0.3486\n",
            "Epoch 336/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9433 - loss: 0.3144 - val_accuracy: 0.9135 - val_loss: 0.3469\n",
            "Epoch 337/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9390 - loss: 0.3106 - val_accuracy: 0.9135 - val_loss: 0.3453\n",
            "Epoch 338/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9424 - loss: 0.3114 - val_accuracy: 0.9153 - val_loss: 0.3485\n",
            "Epoch 339/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9466 - loss: 0.3078 - val_accuracy: 0.9171 - val_loss: 0.3478\n",
            "Epoch 340/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9462 - loss: 0.3137 - val_accuracy: 0.9135 - val_loss: 0.3469\n",
            "Epoch 341/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9436 - loss: 0.3110 - val_accuracy: 0.9171 - val_loss: 0.3476\n",
            "Epoch 342/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9499 - loss: 0.3029 - val_accuracy: 0.9135 - val_loss: 0.3473\n",
            "Epoch 343/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9412 - loss: 0.3185 - val_accuracy: 0.9135 - val_loss: 0.3464\n",
            "Epoch 344/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9421 - loss: 0.3111 - val_accuracy: 0.9135 - val_loss: 0.3475\n",
            "Epoch 345/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9490 - loss: 0.3059 - val_accuracy: 0.9153 - val_loss: 0.3474\n",
            "Epoch 346/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9501 - loss: 0.2992 - val_accuracy: 0.9135 - val_loss: 0.3467\n",
            "Epoch 347/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9461 - loss: 0.3055 - val_accuracy: 0.9171 - val_loss: 0.3454\n",
            "Epoch 348/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9397 - loss: 0.3132 - val_accuracy: 0.9153 - val_loss: 0.3456\n",
            "Epoch 349/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9430 - loss: 0.3101 - val_accuracy: 0.9135 - val_loss: 0.3479\n",
            "Epoch 350/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9443 - loss: 0.3129 - val_accuracy: 0.9135 - val_loss: 0.3473\n",
            "Epoch 351/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9441 - loss: 0.3079 - val_accuracy: 0.9135 - val_loss: 0.3475\n",
            "Epoch 352/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9428 - loss: 0.3112 - val_accuracy: 0.9135 - val_loss: 0.3480\n",
            "Epoch 353/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9452 - loss: 0.3056 - val_accuracy: 0.9135 - val_loss: 0.3481\n",
            "Epoch 354/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9437 - loss: 0.3102 - val_accuracy: 0.9135 - val_loss: 0.3476\n",
            "Epoch 355/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9489 - loss: 0.3067 - val_accuracy: 0.9135 - val_loss: 0.3462\n",
            "Epoch 356/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9406 - loss: 0.3110 - val_accuracy: 0.9153 - val_loss: 0.3453\n",
            "Epoch 357/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9405 - loss: 0.3105 - val_accuracy: 0.9171 - val_loss: 0.3459\n",
            "Epoch 358/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9439 - loss: 0.3083 - val_accuracy: 0.9117 - val_loss: 0.3494\n",
            "Epoch 359/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9483 - loss: 0.3112 - val_accuracy: 0.9171 - val_loss: 0.3448\n",
            "Epoch 360/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9399 - loss: 0.3127 - val_accuracy: 0.9135 - val_loss: 0.3464\n",
            "Epoch 361/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9479 - loss: 0.3031 - val_accuracy: 0.9135 - val_loss: 0.3472\n",
            "Epoch 362/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9455 - loss: 0.3105 - val_accuracy: 0.9171 - val_loss: 0.3456\n",
            "Epoch 363/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9430 - loss: 0.3117 - val_accuracy: 0.9135 - val_loss: 0.3472\n",
            "Epoch 364/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9389 - loss: 0.3148 - val_accuracy: 0.9153 - val_loss: 0.3442\n",
            "Epoch 365/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9407 - loss: 0.3108 - val_accuracy: 0.9153 - val_loss: 0.3469\n",
            "Epoch 366/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9518 - loss: 0.3074 - val_accuracy: 0.9153 - val_loss: 0.3467\n",
            "Epoch 367/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9437 - loss: 0.3111 - val_accuracy: 0.9171 - val_loss: 0.3448\n",
            "Epoch 368/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9398 - loss: 0.3129 - val_accuracy: 0.9171 - val_loss: 0.3461\n",
            "Epoch 369/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9439 - loss: 0.3088 - val_accuracy: 0.9135 - val_loss: 0.3462\n",
            "Epoch 370/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9441 - loss: 0.3086 - val_accuracy: 0.9135 - val_loss: 0.3472\n",
            "Epoch 371/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9476 - loss: 0.3078 - val_accuracy: 0.9189 - val_loss: 0.3448\n",
            "Epoch 372/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9509 - loss: 0.2993 - val_accuracy: 0.9171 - val_loss: 0.3466\n",
            "Epoch 373/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9446 - loss: 0.3072 - val_accuracy: 0.9153 - val_loss: 0.3432\n",
            "Epoch 374/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9454 - loss: 0.3102 - val_accuracy: 0.9153 - val_loss: 0.3474\n",
            "Epoch 375/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9467 - loss: 0.3078 - val_accuracy: 0.9135 - val_loss: 0.3457\n",
            "Epoch 376/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9450 - loss: 0.3104 - val_accuracy: 0.9135 - val_loss: 0.3458\n",
            "Epoch 377/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9480 - loss: 0.3091 - val_accuracy: 0.9135 - val_loss: 0.3451\n",
            "Epoch 378/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9440 - loss: 0.3151 - val_accuracy: 0.9135 - val_loss: 0.3471\n",
            "Epoch 379/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9445 - loss: 0.3132 - val_accuracy: 0.9189 - val_loss: 0.3449\n",
            "Epoch 380/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9427 - loss: 0.3106 - val_accuracy: 0.9135 - val_loss: 0.3463\n",
            "Epoch 381/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9466 - loss: 0.3074 - val_accuracy: 0.9171 - val_loss: 0.3455\n",
            "Epoch 382/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9443 - loss: 0.3090 - val_accuracy: 0.9135 - val_loss: 0.3454\n",
            "Epoch 383/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9451 - loss: 0.3101 - val_accuracy: 0.9135 - val_loss: 0.3457\n",
            "Epoch 384/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9445 - loss: 0.3070 - val_accuracy: 0.9135 - val_loss: 0.3451\n",
            "Epoch 385/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9379 - loss: 0.3151 - val_accuracy: 0.9153 - val_loss: 0.3465\n",
            "Epoch 386/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9411 - loss: 0.3153 - val_accuracy: 0.9135 - val_loss: 0.3460\n",
            "Epoch 387/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9402 - loss: 0.3094 - val_accuracy: 0.9153 - val_loss: 0.3448\n",
            "Epoch 388/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9475 - loss: 0.3059 - val_accuracy: 0.9171 - val_loss: 0.3454\n",
            "Epoch 389/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9493 - loss: 0.3079 - val_accuracy: 0.9135 - val_loss: 0.3480\n",
            "Epoch 390/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9452 - loss: 0.3128 - val_accuracy: 0.9153 - val_loss: 0.3460\n",
            "Epoch 391/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9462 - loss: 0.3038 - val_accuracy: 0.9153 - val_loss: 0.3458\n",
            "Epoch 392/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9457 - loss: 0.3073 - val_accuracy: 0.9171 - val_loss: 0.3462\n",
            "Epoch 393/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9408 - loss: 0.3157 - val_accuracy: 0.9135 - val_loss: 0.3439\n",
            "Epoch 394/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9403 - loss: 0.3162 - val_accuracy: 0.9171 - val_loss: 0.3493\n",
            "Epoch 395/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9462 - loss: 0.3110 - val_accuracy: 0.9153 - val_loss: 0.3451\n",
            "Epoch 396/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9428 - loss: 0.3088 - val_accuracy: 0.9171 - val_loss: 0.3445\n",
            "Epoch 397/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9406 - loss: 0.3140 - val_accuracy: 0.9135 - val_loss: 0.3455\n",
            "Epoch 398/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9479 - loss: 0.3056 - val_accuracy: 0.9171 - val_loss: 0.3463\n",
            "Epoch 399/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9450 - loss: 0.3079 - val_accuracy: 0.9135 - val_loss: 0.3450\n",
            "Epoch 400/400\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9453 - loss: 0.3081 - val_accuracy: 0.9153 - val_loss: 0.3443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vizualize Training History**"
      ],
      "metadata": {
        "id": "17dCr9zgAnQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "BpMKkmkDmweF",
        "outputId": "eeb8e863-e7bc-4768-9fa3-7729d5442b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Set Classification Accuracy of FP32 Model**"
      ],
      "metadata": {
        "id": "egHE_K6vArh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the FP32 model\n",
        "results = model.evaluate(test_dataset)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")\n",
        "\n",
        "# Goal: Target Accuracy >= 0.85"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IElgrf-TJJPC",
        "outputId": "a2e21b77-4c79-4972-abb3-411367e8ef64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8837 - loss: 0.4174\n",
            "Test Loss: 0.34613266587257385, Test Accuracy: 0.9153153300285339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to TFLite Model**"
      ],
      "metadata": {
        "id": "tdyvZiHWA-A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "# Print tensor details\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "for tensor in interpreter.get_tensor_details():\n",
        "    print(tensor['name'], tensor['dtype'])\n",
        "    try:\n",
        "        # Attempt to get tensor data\n",
        "        tensor_data = interpreter.get_tensor(tensor['index'])\n",
        "        print(tensor_data)\n",
        "    except ValueError:\n",
        "        # Skip tensors with null data\n",
        "        print(f\"Skipping tensor '{tensor['name']}' as it has null data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC6reRpb_00G",
        "outputId": "fb3b2887-6b73-4056-804e-bef01f123d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp4ay3_6l3'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 11), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135998401644688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135998401650256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135998401650832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135998401645456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "serving_default_keras_tensor:0 <class 'numpy.float32'>\n",
            "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.7078864e-23\n",
            "  0.0000000e+00 2.8025969e-45 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  2.9287138e-43]]\n",
            "arith.constant <class 'numpy.float32'>\n",
            "[[ 1.8696122e+00  2.7505174e+00 -2.6215825e-34  2.6149541e-01\n",
            "   1.5002978e+00 -1.4240094e+00 -1.6747156e-01 -1.1317186e+00]\n",
            " [ 1.0205084e+00 -5.6434619e-01  2.1857645e-33  1.6357198e+00\n",
            "   9.6308750e-01 -5.2900318e-02  1.1546996e+00 -9.5029044e-01]\n",
            " [-1.2973880e+00 -1.6553266e+00  4.0639598e-33  1.5187211e+00\n",
            "  -1.2290552e+00  4.0195766e-01  8.1763124e-01  4.8545739e-03]\n",
            " [-1.6826895e+00 -5.2731586e-01  3.5826434e-33 -3.6827316e+00\n",
            "  -1.3103433e+00  1.6811335e+00 -1.7250106e+00  1.5136169e+00]]\n",
            "sequential_1/dense_1_2/BiasAdd/ReadVariableOp <class 'numpy.float32'>\n",
            "[-0.8077127   0.03172665  0.7343251   0.04079077]\n",
            "sequential_1/dense_1/BiasAdd/ReadVariableOp <class 'numpy.float32'>\n",
            "[ 7.8085017e-01  2.3498234e-01 -5.3040113e-19  1.1964298e+00\n",
            "  5.6844944e-01  8.3156389e-01  4.7940576e-01  6.4660197e-01]\n",
            "sequential_1/dense_1/MatMul <class 'numpy.float32'>\n",
            "[[ 9.05085504e-02  1.44766271e-01 -1.93972552e+00  8.48340616e-02\n",
            "   1.97138071e-01 -3.40823561e-01  1.38547218e+00 -1.09210432e+00\n",
            "   4.97334421e-01  8.40166926e-01  1.00909984e+00]\n",
            " [-3.89877409e-01 -1.16291255e-01 -1.64657116e+00  2.64027506e-01\n",
            "   4.57225919e-01  9.42811430e-01  2.01327920e+00 -9.91360545e-01\n",
            "  -4.33453768e-01  2.74248868e-01  1.56788886e+00]\n",
            " [ 4.40426583e-33  2.32735282e-33 -4.97927652e-34  4.70007163e-33\n",
            "   1.61291931e-33 -5.54379997e-33 -1.89749382e-33 -3.81507573e-33\n",
            "  -4.34845152e-33 -3.23703887e-33 -2.75754004e-33]\n",
            " [ 2.32203633e-01  1.61650634e+00 -9.26346004e-01  5.13071239e-01\n",
            "   7.47622609e-01 -2.65718031e+00  1.76590073e+00 -8.87255847e-01\n",
            "   1.89751327e-01  1.07375169e+00  4.26541209e-01]\n",
            " [ 9.61534232e-02  1.91361248e-01 -1.68871045e+00  1.36817405e-02\n",
            "   1.70357555e-01 -1.50711343e-01  1.08526635e+00 -7.57993817e-01\n",
            "   4.40036416e-01  7.99091935e-01  7.60267198e-01]\n",
            " [ 5.25516570e-01 -3.90428081e-02  1.47082412e+00 -5.13926968e-02\n",
            "  -1.70099884e-01  5.36042690e-01 -1.30587780e+00  9.32550907e-01\n",
            "   5.15118062e-01 -1.74964387e-02 -6.43056452e-01]\n",
            " [ 4.44489449e-01  8.37084532e-01 -3.34370255e-01  1.95864081e-01\n",
            "   2.29961887e-01 -1.62453616e+00  5.83542705e-01 -3.28593403e-01\n",
            "   1.07472435e-01  5.84964991e-01  1.46998957e-01]\n",
            " [ 9.20978487e-02 -2.62166262e-01  1.44826031e+00  4.28482592e-02\n",
            "  -7.62220845e-02  1.04390466e+00 -9.77590084e-01  1.00604355e+00\n",
            "   2.96389759e-01 -3.48434359e-01 -3.28768253e-01]]\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd <class 'numpy.float32'>\n",
            "Skipping tensor 'sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd' as it has null data.\n",
            "StatefulPartitionedCall_1:0 <class 'numpy.float32'>\n",
            "[[-5.197854e+37  4.437071e-41  4.624285e-44  0.000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to Quantized Model**"
      ],
      "metadata": {
        "id": "C1gaSy-8xaMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide a representative dataset to guide the quantization process\n",
        "def representative_dataset_gen():\n",
        "    for data, _ in test_dataset.unbatch().batch(1).take(100):\n",
        "        yield [data]\n",
        "\n",
        "# Convert the model to int8 format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "converter.experimental_new_quantizer = False  # Optional: Use the default quantizer\n",
        "converter._experimental_disable_per_channel = True\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('model_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "# Print tensor details\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "for tensor in interpreter.get_tensor_details():\n",
        "    print(tensor['name'], tensor['dtype'])\n",
        "    try:\n",
        "        # Attempt to get tensor data\n",
        "        tensor_data = interpreter.get_tensor(tensor['index'])\n",
        "        print(tensor_data)\n",
        "    except ValueError:\n",
        "        # Skip tensors with null data\n",
        "        print(f\"Skipping tensor '{tensor['name']}' as it has null data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu50RF1HBJZ_",
        "outputId": "7ed33e3e-b876-4312-ddeb-9db4739dd2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpa_y4bzwx'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 11), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  135998401644688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135998401650256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135998401650832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  135998401645456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "serving_default_keras_tensor:0_int8 <class 'numpy.int8'>\n",
            "[[ 32  15 -21  89 -79 123   0   0  32  15 -21]]\n",
            "arith.constant <class 'numpy.int8'>\n",
            "[[  64   95    0    9   52  -49   -6  -39]\n",
            " [  35  -19    0   56   33   -2   40  -33]\n",
            " [ -45  -57    0   52  -42   14   28    0]\n",
            " [ -58  -18    0 -127  -45   58  -59   52]]\n",
            "sequential_1/dense_1_2/BiasAdd/ReadVariableOp <class 'numpy.int32'>\n",
            "[-2269    89  2063   115]\n",
            "sequential_1/dense_1/BiasAdd/ReadVariableOp <class 'numpy.int32'>\n",
            "[ 9668  2910     0 14814  7039 10296  5936  8006]\n",
            "sequential_1/dense_1/MatMul <class 'numpy.int8'>\n",
            "[[   4    7  -93    4    9  -16   66  -52   24   40   48]\n",
            " [ -19   -6  -79   13   22   45   96  -47  -21   13   75]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  11   77  -44   25   36 -127   84  -42    9   51   20]\n",
            " [   5    9  -81    1    8   -7   52  -36   21   38   36]\n",
            " [  25   -2   70   -2   -8   26  -62   45   25   -1  -31]\n",
            " [  21   40  -16    9   11  -78   28  -16    5   28    7]\n",
            " [   4  -13   69    2   -4   50  -47   48   14  -17  -16]]\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd <class 'numpy.int8'>\n",
            "Skipping tensor 'sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd' as it has null data.\n",
            "StatefulPartitionedCall_1:0_int8 <class 'numpy.int8'>\n",
            "[[48  0  0  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Set Classification Accuracy of 8-bit Model**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SZZ-Oth4nJOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test accuracy of quantized model\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "correct = 0.0\n",
        "total = 0.0\n",
        "\n",
        "# Define the transformation function\n",
        "def convert_to_int8(X, y):\n",
        "    X = tf.cast((X*255)-128, tf.int8)  # Multiply by 255 and subtract 128 cast to int8\n",
        "    return X, y\n",
        "\n",
        "# Apply the transformation to the dataset\n",
        "test_dataset_quantized_inputs = test_dataset.map(convert_to_int8).unbatch()\n",
        "\n",
        "# Example: Inspect the first batch\n",
        "for X_batch, y_batch in test_dataset_quantized_inputs.take(1):\n",
        "    print(X_batch.numpy(), y_batch.numpy())\n",
        "for input, label in test_dataset_quantized_inputs.batch(1):\n",
        "    interpreter.set_tensor(input_details[0]['index'], input.numpy().astype('int8'))\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    total += 1\n",
        "    if output.argmax() == label.numpy()[0]:\n",
        "        correct += 1\n",
        "\n",
        "print(f\"Quantized Model Test Set Accuracy: {correct / total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSPokUuAfoOQ",
        "outputId": "6c21ae39-8a0f-4deb-bc8c-6f682c4acfac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 59 -45 -89 123  76  84 -32 -73 -49  18 115] 0\n",
            "Quantized Model Test Set Accuracy: 0.9135135135135135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the quantized TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Extract tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "all_tensor_details = interpreter.get_tensor_details()\n",
        "\n",
        "# Extract weights, biases, scales, and zero points from allocated tensors\n",
        "quantized_params = {}\n",
        "for tensor in all_tensor_details:\n",
        "    # Check if the tensor has quantization parameters and valid data\n",
        "    try:\n",
        "        # Attempt to get tensor data\n",
        "        tensor_data = interpreter.get_tensor(tensor['index'])\n",
        "\n",
        "        if 'quantization_parameters' in tensor and tensor['quantization_parameters']['scales'].size > 0:\n",
        "            quantized_params[tensor['name']] = {\n",
        "                'values': tensor_data,\n",
        "                'scale': tensor['quantization_parameters']['scales'],\n",
        "                'zero_point': tensor['quantization_parameters']['zero_points']\n",
        "            }\n",
        "    except ValueError:\n",
        "        # Skip tensors with null data\n",
        "        if 'quantization_parameters' in tensor and tensor['quantization_parameters']['scales'].size > 0:\n",
        "            quantized_params[tensor['name']] = {\n",
        "                'scale': tensor['quantization_parameters']['scales'],\n",
        "                'zero_point': tensor['quantization_parameters']['zero_points']\n",
        "            }\n",
        "        print(f\"Skipping tensor '{tensor['name']}' as it has null data.\")\n",
        "\n",
        "print(\"Quantized parameters extracted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv4RuhPA3MDI",
        "outputId": "a8dcf386-4fc2-4b3b-ac66-445863f27c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping tensor 'sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd' as it has null data.\n",
            "Quantized parameters extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Print Names, Weights, Scales, and Zero Points of Quantized Model Tensors'**"
      ],
      "metadata": {
        "id": "V3QtTCSOe_8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, params in quantized_params.items():\n",
        "    print(f\"{name} - Scale: {params['scale']}, Zero Point: {params['zero_point']}\")\n",
        "    if 'values' in params:\n",
        "      print(params['values'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vO6T2gMerb9",
        "outputId": "7f9d5bc0-6f08-4a38-bf7b-2eb1db2d9e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "serving_default_keras_tensor:0_int8 - Scale: [0.00386005], Zero Point: [-128]\n",
            "[[0 0 0 0 0 0 0 0 0 0 0]]\n",
            "arith.constant - Scale: [0.02899789], Zero Point: [0]\n",
            "[[  64   95    0    9   52  -49   -6  -39]\n",
            " [  35  -19    0   56   33   -2   40  -33]\n",
            " [ -45  -57    0   52  -42   14   28    0]\n",
            " [ -58  -18    0 -127  -45   58  -59   52]]\n",
            "sequential_1/dense_1_2/BiasAdd/ReadVariableOp - Scale: [0.00035592], Zero Point: [0]\n",
            "[-2269    89  2063   115]\n",
            "sequential_1/dense_1/BiasAdd/ReadVariableOp - Scale: [8.076267e-05], Zero Point: [0]\n",
            "[ 9668  2910     0 14814  7039 10296  5936  8006]\n",
            "sequential_1/dense_1/MatMul - Scale: [0.02092268], Zero Point: [0]\n",
            "[[   4    7  -93    4    9  -16   66  -52   24   40   48]\n",
            " [ -19   -6  -79   13   22   45   96  -47  -21   13   75]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0]\n",
            " [  11   77  -44   25   36 -127   84  -42    9   51   20]\n",
            " [   5    9  -81    1    8   -7   52  -36   21   38   36]\n",
            " [  25   -2   70   -2   -8   26  -62   45   25   -1  -31]\n",
            " [  21   40  -16    9   11  -78   28  -16    5   28    7]\n",
            " [   4  -13   69    2   -4   50  -47   48   14  -17  -16]]\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd - Scale: [0.0122739], Zero Point: [-128]\n",
            "StatefulPartitionedCall_1:0_int8 - Scale: [0.11437944], Zero Point: [11]\n",
            "[[0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Map TFLite Provided Names to Intuitive Ones**\n",
        "The TFLite Layer names after quantization are not very intuitive.\n",
        "Use the names above + the [Netron](https://netron.app/) application to update dictionary below so that it is very clear which layer is which. *You may need to update the names if any changes are made to the notebook.*"
      ],
      "metadata": {
        "id": "dDUXkdPJAlNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, params in quantized_params.items():\n",
        "    print(f\"{name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5CLjrd6r2M_",
        "outputId": "b6829b7b-95b7-4f37-9f4e-4f934a30fbd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "serving_default_keras_tensor:0_int8\n",
            "arith.constant\n",
            "sequential_1/dense_1_2/BiasAdd/ReadVariableOp\n",
            "sequential_1/dense_1/BiasAdd/ReadVariableOp\n",
            "sequential_1/dense_1/MatMul\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd\n",
            "StatefulPartitionedCall_1:0_int8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UQwlPsSStV4i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "VHxC7esBsJqN",
        "outputId": "9de6ac26-97eb-4ac7-e608-6508d8f0d80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m48\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m132\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,022\u001b[0m (4.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,022</span> (4.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m340\u001b[0m (1.33 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">340</span> (1.33 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m682\u001b[0m (2.67 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">682</span> (2.67 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, params in quantized_params.items():\n",
        "    print(f\"Tensor Name: {name}, shape: {params['values'].shape if 'values' in params else 'N/A'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxp3BjWMuiz6",
        "outputId": "1f51abd5-df81-4c1d-f5b5-f61bddfb9515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor Name: serving_default_keras_tensor:0_int8, shape: (1, 11)\n",
            "Tensor Name: arith.constant, shape: (4, 8)\n",
            "Tensor Name: sequential_1/dense_1_2/BiasAdd/ReadVariableOp, shape: (4,)\n",
            "Tensor Name: sequential_1/dense_1/BiasAdd/ReadVariableOp, shape: (8,)\n",
            "Tensor Name: sequential_1/dense_1/MatMul, shape: (8, 11)\n",
            "Tensor Name: sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd, shape: N/A\n",
            "Tensor Name: StatefulPartitionedCall_1:0_int8, shape: (1, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l4RXXe-rtuMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer name map\n",
        "layer_name_map = {\n",
        "    \"input_layer\": \"serving_default_keras_tensor:0_int8\",\n",
        "    \"layer_one_weights\": \"sequential_1/dense_1/MatMul\",\n",
        "    \"layer_one_bias\": \"sequential_1/dense_1/BiasAdd/ReadVariableOp\",\n",
        "    \"layer_one_output_activations\": \"sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/BiasAdd\",\n",
        "    \"layer_two_weights\": \"arith.constant\",\n",
        "    \"layer_two_bias\": \"sequential_1/dense_1_2/BiasAdd/ReadVariableOp\",\n",
        "    \"output_layer\": \"StatefulPartitionedCall_1:0_int8\"\n",
        "}"
      ],
      "metadata": {
        "id": "GqeYV9Ko_kAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer_scale = quantized_params[layer_name_map[\"input_layer\"]][\"scale\"]\n",
        "input_layer_zero_point = quantized_params[layer_name_map[\"input_layer\"]][\"zero_point\"]\n",
        "\n",
        "layer_one_weights = quantized_params[layer_name_map[\"layer_one_weights\"]][\"values\"]\n",
        "layer_one_weights_scale = quantized_params[layer_name_map[\"layer_one_weights\"]][\"scale\"]\n",
        "layer_one_weights_zero_point = quantized_params[layer_name_map[\"layer_one_weights\"]][\"zero_point\"]\n",
        "layer_one_bias = quantized_params[layer_name_map[\"layer_one_bias\"]][\"values\"]\n",
        "\n",
        "layer_one_output_activations_scale = quantized_params[layer_name_map[\"layer_one_output_activations\"]][\"scale\"]\n",
        "layer_one_output_activations_zero_point = quantized_params[layer_name_map[\"layer_one_output_activations\"]][\"zero_point\"]\n",
        "\n",
        "layer_two_weights = quantized_params[layer_name_map[\"layer_two_weights\"]][\"values\"]\n",
        "layer_two_weights_scale = quantized_params[layer_name_map[\"layer_two_weights\"]][\"scale\"]\n",
        "layer_two_weights_zero_point = quantized_params[layer_name_map[\"layer_two_weights\"]][\"zero_point\"]\n",
        "layer_two_bias = quantized_params[layer_name_map[\"layer_two_bias\"]][\"values\"]\n",
        "\n",
        "output_layer_scale = quantized_params[layer_name_map[\"output_layer\"]][\"scale\"]\n",
        "output_layer_zero_point = quantized_params[layer_name_map[\"output_layer\"]][\"zero_point\"]"
      ],
      "metadata": {
        "id": "txvONXs98UfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"input_layer_scale: {input_layer_scale}\")\n",
        "print(f\"input_layer_zero_point: {input_layer_zero_point}\")\n",
        "print(f\"layer_one_weights_scale: {layer_one_weights_scale}\")\n",
        "print(f\"layer_one_weights_zero_point: {layer_one_weights_zero_point}\")\n",
        "print(f\"layer_one_bias: {layer_one_bias}\")\n",
        "print(f\"layer_one_output_activations_scale: {layer_one_output_activations_scale}\")\n",
        "print(f\"layer_one_output_activations_zero_point: {layer_one_output_activations_zero_point}\")\n",
        "print(f\"layer_two_weights_scale: {layer_two_weights_scale}\")\n",
        "print(f\"layer_two_weights_zero_point: {layer_two_weights_zero_point}\")\n",
        "print(f\"layer_two_bias: {layer_two_bias}\")\n",
        "print(f\"output_layer_scale: {output_layer_scale}\")\n",
        "print(f\"output_layer_zero_point: {output_layer_zero_point}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jb_aBZxbTCI",
        "outputId": "1b1657ee-8382-4430-e7f1-20d81d2be3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_layer_scale: [0.00386005]\n",
            "input_layer_zero_point: [-128]\n",
            "layer_one_weights_scale: [0.02092268]\n",
            "layer_one_weights_zero_point: [0]\n",
            "layer_one_bias: [ 9668  2910     0 14814  7039 10296  5936  8006]\n",
            "layer_one_output_activations_scale: [0.0122739]\n",
            "layer_one_output_activations_zero_point: [-128]\n",
            "layer_two_weights_scale: [0.02899789]\n",
            "layer_two_weights_zero_point: [0]\n",
            "layer_two_bias: [-2269    89  2063   115]\n",
            "output_layer_scale: [0.11437944]\n",
            "output_layer_zero_point: [11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Layer One Weights (comma-separated):\")\n",
        "for row in layer_one_weights:\n",
        "    print(','.join(map(str, row)))\n",
        "\n",
        "print(\"\\nLayer Two Weights (comma-separated):\")\n",
        "for row in layer_two_weights:\n",
        "    print(','.join(map(str, row)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4iEk2Jjb_e2",
        "outputId": "9ffd0e5a-dff0-47b5-b885-cef028763ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer One Weights (comma-separated):\n",
            "4,7,-93,4,9,-16,66,-52,24,40,48\n",
            "-19,-6,-79,13,22,45,96,-47,-21,13,75\n",
            "0,0,0,0,0,0,0,0,0,0,0\n",
            "11,77,-44,25,36,-127,84,-42,9,51,20\n",
            "5,9,-81,1,8,-7,52,-36,21,38,36\n",
            "25,-2,70,-2,-8,26,-62,45,25,-1,-31\n",
            "21,40,-16,9,11,-78,28,-16,5,28,7\n",
            "4,-13,69,2,-4,50,-47,48,14,-17,-16\n",
            "\n",
            "Layer Two Weights (comma-separated):\n",
            "64,95,0,9,52,-49,-6,-39\n",
            "35,-19,0,56,33,-2,40,-33\n",
            "-45,-57,0,52,-42,14,28,0\n",
            "-58,-18,0,-127,-45,58,-59,52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fixed_point_multiplier(input_scale, weight_scale, output_scale):\n",
        "    # Calculate M0 and N from M = 2^-N M0 = (S1 * S2 / S3)\n",
        "    multiplier = input_scale * weight_scale / output_scale\n",
        "    shift = 0\n",
        "    while multiplier < 0.5:\n",
        "        multiplier *= 2\n",
        "        shift += 1\n",
        "    quantized_multiplier = multiplier * math.pow(2, 31)\n",
        "    return quantized_multiplier, shift"
      ],
      "metadata": {
        "id": "l00b5QnEaRmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First layer requantization params\n",
        "layer_one_multiplier, layer_one_shift = calculate_fixed_point_multiplier(input_layer_scale, layer_one_weights_scale, layer_one_output_activations_scale)\n",
        "\n",
        "# Second layer requantization params\n",
        "layer_two_multiplier, layer_two_shift = calculate_fixed_point_multiplier(layer_one_output_activations_scale, layer_two_weights_scale, output_layer_scale)\n",
        "\n",
        "subscript_printing = str.maketrans(\"123456789\", \"₁₂₃₄₅₆₇₈₉\")\n",
        "print(\"Layer 1 Requantization Params:\")\n",
        "print(\"M01: \".translate(subscript_printing) + f\"{layer_one_multiplier[0]:.2f}\")\n",
        "print(\"N1: \".translate(subscript_printing)  + f\"{layer_one_shift}\")\n",
        "\n",
        "print(\"Layer 2 Requantization Params:\")\n",
        "print(\"M02: \".translate(subscript_printing) + f\"{layer_two_multiplier[0]:.2f}\")\n",
        "print(\"N2: \".translate(subscript_printing)  + f\"{layer_two_shift}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibGduAd2bMYE",
        "outputId": "a1aef3fa-2eec-428a-a8de-b6df6e1f5c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Requantization Params:\n",
            "M0₁: 1808705664.00\n",
            "N₁: 7\n",
            "Layer 2 Requantization Params:\n",
            "M0₂: 1710688256.00\n",
            "N₂: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Emulate 8-bit Integer Inference with Numpy**"
      ],
      "metadata": {
        "id": "pDmEYHIJoB5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create copy of test dataset\n",
        "TEST_SET_SIZE = 555\n",
        "test_dataset_copy = (iter(test_dataset_quantized_inputs.take(TEST_SET_SIZE)))"
      ],
      "metadata": {
        "id": "Sskn2QitsQ1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tflite_golden_inference(tflite_model, inputs, debug=False):\n",
        "    #\n",
        "    # Golden Reference Implementation of TFLite Inference running on a single sample\n",
        "    #\n",
        "\n",
        "    # Add batch dim to single data sample\n",
        "    inputs = np.expand_dims(inputs, 0)\n",
        "\n",
        "    # Load TFLite model and allocate tensors\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model, experimental_preserve_all_tensors=True)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensors\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Load input tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], inputs)\n",
        "\n",
        "    # Run the model\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Print each layer's output if needed for verification\n",
        "    if debug:\n",
        "      print({\n",
        "          t['name']: interpreter.get_tensor(t['index'])\n",
        "          for t in interpreter.get_tensor_details()\n",
        "      })\n",
        "\n",
        "    return interpreter.get_tensor(output_details[0]['index'])"
      ],
      "metadata": {
        "id": "jja_F8B4sRqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_numpy_inference(input):\n",
        "  #\n",
        "  # Numpy Reference Implementation of TFLite Inference running on a single sample\n",
        "  #\n",
        "\n",
        "  # (Inputs * Layer 1 Weights) + Bias followed by ReLU\n",
        "  x = np.matmul((input.numpy().astype(np.int32) - input_layer_zero_point.astype(np.int32)),(layer_one_weights.T.astype(np.int32) - layer_one_weights_zero_point.astype(np.int32)))\n",
        "  x = x + layer_one_bias\n",
        "  x = np.maximum(x, 0)\n",
        "\n",
        "  # Requantization pipeline\n",
        "  x = x * layer_one_multiplier\n",
        "  x = np.round((x / np.power(2,31))).astype(np.int32)\n",
        "  x = np.round((x / np.power(2, layer_one_shift))).astype(np.int32)\n",
        "  x = x + layer_one_output_activations_zero_point.astype(np.int32)\n",
        "  x = np.clip(x, -128, 127)\n",
        "\n",
        "  # (Layer 1 Activations * Layer 2 Weights) + Bias\n",
        "  x = np.matmul((x.astype(np.int32) - layer_one_output_activations_zero_point.astype(np.int32)), (layer_two_weights.T.astype(np.int32) - layer_two_weights_zero_point.astype(np.int32)))\n",
        "  x = x + layer_two_bias\n",
        "\n",
        "  # Requantization pipeline\n",
        "  x = x * layer_two_multiplier\n",
        "  x = np.round((x / np.power(2,31))).astype(np.int32)\n",
        "  x = np.round((x / np.power(2, layer_two_shift))).astype(np.int32)\n",
        "  x = x + output_layer_zero_point.astype(np.int32)\n",
        "  x = np.clip(x, -128, 127)\n",
        "\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "5_Hx0jC7oBje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_correct = 0.0\n",
        "numpy_correct = 0.0\n",
        "\n",
        "for inputs, targets in test_dataset_copy:\n",
        "  tflite_output = run_tflite_golden_inference(tflite_quant_model, inputs)\n",
        "  numpy_output = run_numpy_inference(inputs)\n",
        "\n",
        "  # Make sure that raw values of output tensors match exactly to validate numpy reference implementation\n",
        "  if np.array_equal(tflite_output.flatten()[0], numpy_output.flatten()):\n",
        "    print(tflite_output)\n",
        "    print(numpy_output)\n",
        "    print(\"ERROR: TFlite Golden Output Tensor does not match Numpy Implementation Output Tensor\")\n",
        "\n",
        "  # Update num correct\n",
        "  if tflite_output.argmax() == targets.numpy():\n",
        "    tflite_correct += 1\n",
        "  if numpy_output.argmax() == targets.numpy():\n",
        "    numpy_correct += 1\n",
        "\n",
        "\n",
        "\n",
        "# Make sure accuracy is exactly the same to validate numpy implementation\n",
        "print(f\"TF Lite Accuracy: {tflite_correct / TEST_SET_SIZE}\")\n",
        "print(f\"Numpy Accuracy: {numpy_correct / TEST_SET_SIZE}\")\n",
        "print(\"Numpy Implementation matches TFLite Golden Implementation!\" if tflite_correct / TEST_SET_SIZE == numpy_correct / TEST_SET_SIZE else \"Numpy Implementation does NOT match TFLite Golden Implementation...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1qDAzufb-ZD",
        "outputId": "376dde5e-860b-4daf-b46d-00b74d8a5e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Lite Accuracy: 0.9135135135135135\n",
            "Numpy Accuracy: 0.9135135135135135\n",
            "Numpy Implementation matches TFLite Golden Implementation!\n"
          ]
        }
      ]
    }
  ]
}