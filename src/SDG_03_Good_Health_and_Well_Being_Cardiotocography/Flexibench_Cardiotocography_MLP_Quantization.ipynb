{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **FlexiBench Cardiotocography MLP Quantization**\n",
        "### Author: Shvetank Prakash\n",
        "### Date: Jan 2025\n",
        "#### Dataset: [UCI Cardiotocography](https://archive.ics.uci.edu/dataset/193/cardiotocography)\n",
        "#### Helpful links:\n",
        "\n",
        "[Co-Design of Approximate Multilayer Perceptron for Ultra-Resource Constrained Printed Circuits](https://arxiv.org/abs/2302.14576) (Paper Results Reproduced)\n",
        "\n",
        "[Gemmlowp Paper](https://arxiv.org/pdf/1712.05877)\n",
        "\n",
        "[Gemmlowp Implementation](https://github.com/google/gemmlowp/tree/master)\n"
      ],
      "metadata": {
        "id": "B6xKk1RMw3zG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Imports and Global Defs"
      ],
      "metadata": {
        "id": "zmARXYN5xetM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-model-optimization==0.8.0\n",
        "!pip install ucimlrepo==0.0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-7RwFYPOH7xC",
        "outputId": "5d74fd64-724e-482a-a1ba-7db8a901f176"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-model-optimization==0.8.0 in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow-model-optimization==0.8.0) (1.17.0)\n",
            "Requirement already satisfied: ucimlrepo==0.0.7 in /usr/local/lib/python3.11/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo==0.0.7) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo==0.0.7) (2024.12.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo==0.0.7) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo==0.0.7) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Setting environment variables\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8tYezt7xmzUa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "INPUT_SIZE = 21\n",
        "HIDDEN_SIZE = 3\n",
        "OUTPUT_SIZE = 3\n",
        "\n",
        "# Training Parameters\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE_TRAIN = 256\n",
        "BATCH_SIZE_TEST = 64\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "fMfZ9zYJIdCK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read and Preprocess Dataset**"
      ],
      "metadata": {
        "id": "2Jml3fzQBpSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Fetch dataset\n",
        "cardiotocography = fetch_ucirepo(id=193)\n",
        "\n",
        "# Data (as pandas dataframes)\n",
        "X = cardiotocography.data.original.iloc[:, :21].astype('float32')\n",
        "X = (X - X.min()) / (X.max() - X.min())  # normalize all columns of X between 0 and 1\n",
        "y = cardiotocography.data.original['NSP']  # fetal state class code (N=normal; S=suspect; P=pathologic)\n",
        "y = y.values - 1  # adjust class labels from [1,2,3] --> [0,1,2]\n",
        "\n",
        "# Split the data using train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)  # Adjust test_size and random_state as needed\n",
        "\n",
        "# Create tf.data.Dataset objects\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "# Shuffle and batch the datasets\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(X_train), seed=SEED).batch(BATCH_SIZE_TRAIN)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE_TEST)"
      ],
      "metadata": {
        "id": "YJ_KJE3dn4kd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_data(csv_file, keep_quant=False):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    inputs = df.iloc[:, :-1].values\n",
        "    targets = df.iloc[:, -1].values - 1\n",
        "    if not keep_quant:\n",
        "        inputs, targets\n",
        "        inputs = inputs.astype('float32') / 255.0\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Load in dataset from csv (Old implementation replaced with getting data from python package)\n",
        "# train_dataset = load_csv_data('quantized_training_data.csv')\n",
        "# print(train_dataset)\n",
        "# train_dataset = train_dataset.shuffle(buffer_size=len(list(train_dataset)), seed=SEED)\n",
        "# train_dataset = train_dataset.batch(BATCH_SIZE_TRAIN)\n",
        "\n",
        "# test_dataset = load_csv_data('quantized_test_data.csv')\n",
        "# print(test_dataset)\n",
        "# test_dataset = test_dataset.batch(BATCH_SIZE_TEST)"
      ],
      "metadata": {
        "id": "lK7gMbdbJloh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print one sample from train and test dataset each\n",
        "print(\"Train Dataset Sample:\")\n",
        "for x, y in train_dataset.take(1):\n",
        "    print(f\"Inputs: {x}\")\n",
        "    print(f\"Label:  {y}\")\n",
        "\n",
        "print(\"Test Dataset Sample:\")\n",
        "for x, y in test_dataset.take(1):\n",
        "    print(f\"Inputs: {x}\")\n",
        "    print(f\"Label:  {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceLFqDdGNIjv",
        "outputId": "ee22e0f5-d765-4031-8f9e-b7a1f69ae009"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Sample:\n",
            "Inputs: [[0.7777778  0.         0.         ... 0.73394495 0.00371747 1.        ]\n",
            " [0.6851852  0.         0.         ... 0.6238532  0.00371747 1.        ]\n",
            " [0.5        0.         0.002079   ... 0.5504587  0.         0.5       ]\n",
            " ...\n",
            " [0.6666667  0.68421054 0.         ... 0.6880734  0.01858736 0.5       ]\n",
            " [0.6481481  0.05263158 0.         ... 0.6605505  0.00371747 1.        ]\n",
            " [0.25925925 0.         0.         ... 0.22018349 0.23791821 0.        ]]\n",
            "Label:  [1 1 0 0 2 0 0 0 0 0 2 2 1 0 0 0 2 0 0 0 0 0 2 0 1 0 2 0 0 0 0 0 2 0 0 0 0\n",
            " 2 0 0 2 1 0 0 1 1 1 0 1 0 0 2 0 0 2 2 0 2 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 2 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 2 0 0\n",
            " 0 1 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 2 1 0 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 2 0 0 0 2 0 1 2 0 0 2 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 2 1 2 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 2]\n",
            "Test Dataset Sample:\n",
            "Inputs: [[0.5        0.10526317 0.02079002 ... 0.559633   0.03345725 0.5       ]\n",
            " [0.35185185 0.         0.002079   ... 0.44036698 0.10780669 1.        ]\n",
            " [0.46296296 0.21052633 0.00623701 ... 0.5045872  0.07806692 0.5       ]\n",
            " ...\n",
            " [0.4814815  0.36842108 0.         ... 0.56880736 0.38289964 0.5       ]\n",
            " [0.6666667  0.         0.00623701 ... 0.64220184 0.01486989 1.        ]\n",
            " [0.25925925 0.68421054 0.01247401 ... 0.5412844  0.03345725 1.        ]]\n",
            "Label:  [0 0 0 0 0 2 0 2 0 1 0 2 0 0 0 0 0 0 1 0 0 0 2 0 0 2 2 0 0 0 0 0 0 1 0 0 0\n",
            " 2 1 1 0 1 0 0 0 0 0 2 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Model**"
      ],
      "metadata": {
        "id": "dSBlaBAaAYEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp(input_size, hidden_size, output_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.InputLayer(input_shape=(input_size,)),\n",
        "        layers.Dense(hidden_size, activation='relu'),\n",
        "        layers.Dense(output_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = create_mlp(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
        "opt = Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RTuXXZnTKVJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "c02d40f2-8722-49a5-df21-2c1b0a49921c",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m66\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m12\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78\u001b[0m (312.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78</span> (312.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78\u001b[0m (312.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78</span> (312.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train from Scratch in FP32**"
      ],
      "metadata": {
        "id": "OSfrYoT5_vc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial non-quant training\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_dataset\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "trLEGuB-ItPp",
        "outputId": "0b8b1bc3-5c0a-4173-9f8b-7f9487f428cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.7814 - loss: 0.9062 - val_accuracy: 0.7817 - val_loss: 0.8887\n",
            "Epoch 2/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7809 - loss: 0.8840 - val_accuracy: 0.7817 - val_loss: 0.8676\n",
            "Epoch 3/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7745 - loss: 0.8682 - val_accuracy: 0.7817 - val_loss: 0.8465\n",
            "Epoch 4/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7789 - loss: 0.8467 - val_accuracy: 0.7817 - val_loss: 0.8258\n",
            "Epoch 5/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7630 - loss: 0.8353 - val_accuracy: 0.7817 - val_loss: 0.8058\n",
            "Epoch 6/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7834 - loss: 0.7965 - val_accuracy: 0.7817 - val_loss: 0.7862\n",
            "Epoch 7/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7881 - loss: 0.7735 - val_accuracy: 0.7817 - val_loss: 0.7678\n",
            "Epoch 8/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7802 - loss: 0.7658 - val_accuracy: 0.7817 - val_loss: 0.7507\n",
            "Epoch 9/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7713 - loss: 0.7635 - val_accuracy: 0.7817 - val_loss: 0.7352\n",
            "Epoch 10/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7860 - loss: 0.7267 - val_accuracy: 0.7817 - val_loss: 0.7210\n",
            "Epoch 11/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7882 - loss: 0.7142 - val_accuracy: 0.7817 - val_loss: 0.7086\n",
            "Epoch 12/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7750 - loss: 0.7207 - val_accuracy: 0.7817 - val_loss: 0.6977\n",
            "Epoch 13/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7795 - loss: 0.7042 - val_accuracy: 0.7817 - val_loss: 0.6881\n",
            "Epoch 14/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7652 - loss: 0.7125 - val_accuracy: 0.7817 - val_loss: 0.6798\n",
            "Epoch 15/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7709 - loss: 0.6935 - val_accuracy: 0.7817 - val_loss: 0.6725\n",
            "Epoch 16/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7713 - loss: 0.6922 - val_accuracy: 0.7817 - val_loss: 0.6662\n",
            "Epoch 17/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7646 - loss: 0.6929 - val_accuracy: 0.7817 - val_loss: 0.6606\n",
            "Epoch 18/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7679 - loss: 0.6862 - val_accuracy: 0.7817 - val_loss: 0.6555\n",
            "Epoch 19/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7794 - loss: 0.6684 - val_accuracy: 0.7817 - val_loss: 0.6510\n",
            "Epoch 20/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7806 - loss: 0.6589 - val_accuracy: 0.7817 - val_loss: 0.6469\n",
            "Epoch 21/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7803 - loss: 0.6517 - val_accuracy: 0.7817 - val_loss: 0.6431\n",
            "Epoch 22/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7623 - loss: 0.6849 - val_accuracy: 0.7817 - val_loss: 0.6396\n",
            "Epoch 23/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7678 - loss: 0.6703 - val_accuracy: 0.7817 - val_loss: 0.6362\n",
            "Epoch 24/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7798 - loss: 0.6499 - val_accuracy: 0.7817 - val_loss: 0.6332\n",
            "Epoch 25/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7719 - loss: 0.6551 - val_accuracy: 0.7817 - val_loss: 0.6302\n",
            "Epoch 26/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7730 - loss: 0.6537 - val_accuracy: 0.7817 - val_loss: 0.6273\n",
            "Epoch 27/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7864 - loss: 0.6256 - val_accuracy: 0.7817 - val_loss: 0.6245\n",
            "Epoch 28/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7675 - loss: 0.6559 - val_accuracy: 0.7817 - val_loss: 0.6219\n",
            "Epoch 29/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7748 - loss: 0.6354 - val_accuracy: 0.7817 - val_loss: 0.6193\n",
            "Epoch 30/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7824 - loss: 0.6244 - val_accuracy: 0.7817 - val_loss: 0.6168\n",
            "Epoch 31/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7717 - loss: 0.6409 - val_accuracy: 0.7817 - val_loss: 0.6143\n",
            "Epoch 32/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7704 - loss: 0.6448 - val_accuracy: 0.7817 - val_loss: 0.6119\n",
            "Epoch 33/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7809 - loss: 0.6316 - val_accuracy: 0.7817 - val_loss: 0.6095\n",
            "Epoch 34/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7710 - loss: 0.6359 - val_accuracy: 0.7817 - val_loss: 0.6070\n",
            "Epoch 35/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7760 - loss: 0.6275 - val_accuracy: 0.7817 - val_loss: 0.6048\n",
            "Epoch 36/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7733 - loss: 0.6290 - val_accuracy: 0.7817 - val_loss: 0.6024\n",
            "Epoch 37/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7797 - loss: 0.6101 - val_accuracy: 0.7817 - val_loss: 0.6001\n",
            "Epoch 38/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7712 - loss: 0.6206 - val_accuracy: 0.7817 - val_loss: 0.5978\n",
            "Epoch 39/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7745 - loss: 0.6249 - val_accuracy: 0.7817 - val_loss: 0.5956\n",
            "Epoch 40/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7993 - loss: 0.5779 - val_accuracy: 0.7817 - val_loss: 0.5935\n",
            "Epoch 41/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7778 - loss: 0.6060 - val_accuracy: 0.7817 - val_loss: 0.5911\n",
            "Epoch 42/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7751 - loss: 0.6014 - val_accuracy: 0.7817 - val_loss: 0.5888\n",
            "Epoch 43/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7799 - loss: 0.5963 - val_accuracy: 0.7817 - val_loss: 0.5866\n",
            "Epoch 44/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7810 - loss: 0.5991 - val_accuracy: 0.7817 - val_loss: 0.5846\n",
            "Epoch 45/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7887 - loss: 0.5872 - val_accuracy: 0.7817 - val_loss: 0.5825\n",
            "Epoch 46/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7849 - loss: 0.5808 - val_accuracy: 0.7817 - val_loss: 0.5803\n",
            "Epoch 47/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7770 - loss: 0.5978 - val_accuracy: 0.7817 - val_loss: 0.5782\n",
            "Epoch 48/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7800 - loss: 0.5893 - val_accuracy: 0.7817 - val_loss: 0.5761\n",
            "Epoch 49/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7768 - loss: 0.5951 - val_accuracy: 0.7817 - val_loss: 0.5740\n",
            "Epoch 50/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7763 - loss: 0.6006 - val_accuracy: 0.7817 - val_loss: 0.5719\n",
            "Epoch 51/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7814 - loss: 0.5835 - val_accuracy: 0.7817 - val_loss: 0.5698\n",
            "Epoch 52/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7734 - loss: 0.5927 - val_accuracy: 0.7817 - val_loss: 0.5677\n",
            "Epoch 53/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7821 - loss: 0.5823 - val_accuracy: 0.7817 - val_loss: 0.5657\n",
            "Epoch 54/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7719 - loss: 0.5869 - val_accuracy: 0.7817 - val_loss: 0.5636\n",
            "Epoch 55/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7683 - loss: 0.5917 - val_accuracy: 0.7817 - val_loss: 0.5616\n",
            "Epoch 56/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7759 - loss: 0.5877 - val_accuracy: 0.7817 - val_loss: 0.5595\n",
            "Epoch 57/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7716 - loss: 0.5860 - val_accuracy: 0.7817 - val_loss: 0.5575\n",
            "Epoch 58/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7729 - loss: 0.5871 - val_accuracy: 0.7817 - val_loss: 0.5554\n",
            "Epoch 59/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7738 - loss: 0.5769 - val_accuracy: 0.7817 - val_loss: 0.5534\n",
            "Epoch 60/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7834 - loss: 0.5571 - val_accuracy: 0.7817 - val_loss: 0.5513\n",
            "Epoch 61/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7784 - loss: 0.5640 - val_accuracy: 0.7817 - val_loss: 0.5492\n",
            "Epoch 62/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7821 - loss: 0.5593 - val_accuracy: 0.7817 - val_loss: 0.5472\n",
            "Epoch 63/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7699 - loss: 0.5761 - val_accuracy: 0.7817 - val_loss: 0.5452\n",
            "Epoch 64/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7854 - loss: 0.5567 - val_accuracy: 0.7817 - val_loss: 0.5432\n",
            "Epoch 65/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7844 - loss: 0.5454 - val_accuracy: 0.7817 - val_loss: 0.5413\n",
            "Epoch 66/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7876 - loss: 0.5333 - val_accuracy: 0.7817 - val_loss: 0.5392\n",
            "Epoch 67/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7704 - loss: 0.5700 - val_accuracy: 0.7817 - val_loss: 0.5373\n",
            "Epoch 68/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7915 - loss: 0.5250 - val_accuracy: 0.7817 - val_loss: 0.5353\n",
            "Epoch 69/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7752 - loss: 0.5525 - val_accuracy: 0.7817 - val_loss: 0.5333\n",
            "Epoch 70/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7708 - loss: 0.5641 - val_accuracy: 0.7817 - val_loss: 0.5314\n",
            "Epoch 71/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7720 - loss: 0.5561 - val_accuracy: 0.7817 - val_loss: 0.5295\n",
            "Epoch 72/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7833 - loss: 0.5413 - val_accuracy: 0.7817 - val_loss: 0.5275\n",
            "Epoch 73/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7744 - loss: 0.5495 - val_accuracy: 0.7817 - val_loss: 0.5255\n",
            "Epoch 74/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7958 - loss: 0.5168 - val_accuracy: 0.7817 - val_loss: 0.5237\n",
            "Epoch 75/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7784 - loss: 0.5443 - val_accuracy: 0.7817 - val_loss: 0.5217\n",
            "Epoch 76/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7804 - loss: 0.5306 - val_accuracy: 0.7817 - val_loss: 0.5197\n",
            "Epoch 77/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7784 - loss: 0.5365 - val_accuracy: 0.7817 - val_loss: 0.5178\n",
            "Epoch 78/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7870 - loss: 0.5156 - val_accuracy: 0.7817 - val_loss: 0.5158\n",
            "Epoch 79/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7847 - loss: 0.5294 - val_accuracy: 0.7817 - val_loss: 0.5140\n",
            "Epoch 80/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7792 - loss: 0.5302 - val_accuracy: 0.7817 - val_loss: 0.5121\n",
            "Epoch 81/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7840 - loss: 0.5168 - val_accuracy: 0.7817 - val_loss: 0.5102\n",
            "Epoch 82/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7753 - loss: 0.5343 - val_accuracy: 0.7817 - val_loss: 0.5083\n",
            "Epoch 83/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7769 - loss: 0.5220 - val_accuracy: 0.7817 - val_loss: 0.5065\n",
            "Epoch 84/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7776 - loss: 0.5225 - val_accuracy: 0.7817 - val_loss: 0.5046\n",
            "Epoch 85/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7860 - loss: 0.5044 - val_accuracy: 0.7817 - val_loss: 0.5027\n",
            "Epoch 86/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7789 - loss: 0.5135 - val_accuracy: 0.7817 - val_loss: 0.5008\n",
            "Epoch 87/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7751 - loss: 0.5204 - val_accuracy: 0.7817 - val_loss: 0.4989\n",
            "Epoch 88/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7693 - loss: 0.5215 - val_accuracy: 0.7817 - val_loss: 0.4970\n",
            "Epoch 89/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7649 - loss: 0.5287 - val_accuracy: 0.7817 - val_loss: 0.4952\n",
            "Epoch 90/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7814 - loss: 0.5106 - val_accuracy: 0.7817 - val_loss: 0.4933\n",
            "Epoch 91/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7822 - loss: 0.4980 - val_accuracy: 0.7817 - val_loss: 0.4912\n",
            "Epoch 92/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7842 - loss: 0.4955 - val_accuracy: 0.7817 - val_loss: 0.4894\n",
            "Epoch 93/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7747 - loss: 0.5155 - val_accuracy: 0.7817 - val_loss: 0.4875\n",
            "Epoch 94/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7843 - loss: 0.4884 - val_accuracy: 0.7817 - val_loss: 0.4856\n",
            "Epoch 95/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7694 - loss: 0.5191 - val_accuracy: 0.7817 - val_loss: 0.4837\n",
            "Epoch 96/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7767 - loss: 0.5059 - val_accuracy: 0.7817 - val_loss: 0.4818\n",
            "Epoch 97/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8002 - loss: 0.4698 - val_accuracy: 0.7817 - val_loss: 0.4800\n",
            "Epoch 98/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7850 - loss: 0.4798 - val_accuracy: 0.7817 - val_loss: 0.4780\n",
            "Epoch 99/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7696 - loss: 0.5109 - val_accuracy: 0.7817 - val_loss: 0.4761\n",
            "Epoch 100/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7917 - loss: 0.4836 - val_accuracy: 0.7817 - val_loss: 0.4743\n",
            "Epoch 101/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7846 - loss: 0.4810 - val_accuracy: 0.7817 - val_loss: 0.4724\n",
            "Epoch 102/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7787 - loss: 0.4966 - val_accuracy: 0.7817 - val_loss: 0.4707\n",
            "Epoch 103/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7802 - loss: 0.4858 - val_accuracy: 0.7817 - val_loss: 0.4687\n",
            "Epoch 104/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7929 - loss: 0.4745 - val_accuracy: 0.7817 - val_loss: 0.4669\n",
            "Epoch 105/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7792 - loss: 0.4854 - val_accuracy: 0.7817 - val_loss: 0.4651\n",
            "Epoch 106/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7770 - loss: 0.5037 - val_accuracy: 0.7840 - val_loss: 0.4633\n",
            "Epoch 107/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7856 - loss: 0.4758 - val_accuracy: 0.7840 - val_loss: 0.4615\n",
            "Epoch 108/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7921 - loss: 0.4737 - val_accuracy: 0.7840 - val_loss: 0.4597\n",
            "Epoch 109/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7757 - loss: 0.4879 - val_accuracy: 0.7840 - val_loss: 0.4579\n",
            "Epoch 110/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7922 - loss: 0.4609 - val_accuracy: 0.7840 - val_loss: 0.4563\n",
            "Epoch 111/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7937 - loss: 0.4687 - val_accuracy: 0.7864 - val_loss: 0.4547\n",
            "Epoch 112/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7832 - loss: 0.4825 - val_accuracy: 0.7911 - val_loss: 0.4530\n",
            "Epoch 113/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7925 - loss: 0.4644 - val_accuracy: 0.7934 - val_loss: 0.4513\n",
            "Epoch 114/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7875 - loss: 0.4808 - val_accuracy: 0.7958 - val_loss: 0.4497\n",
            "Epoch 115/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7934 - loss: 0.4644 - val_accuracy: 0.7958 - val_loss: 0.4480\n",
            "Epoch 116/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7912 - loss: 0.4626 - val_accuracy: 0.7981 - val_loss: 0.4463\n",
            "Epoch 117/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7910 - loss: 0.4629 - val_accuracy: 0.8005 - val_loss: 0.4447\n",
            "Epoch 118/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7948 - loss: 0.4577 - val_accuracy: 0.8005 - val_loss: 0.4431\n",
            "Epoch 119/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7910 - loss: 0.4555 - val_accuracy: 0.7981 - val_loss: 0.4416\n",
            "Epoch 120/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7958 - loss: 0.4482 - val_accuracy: 0.8005 - val_loss: 0.4402\n",
            "Epoch 121/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7953 - loss: 0.4400 - val_accuracy: 0.8005 - val_loss: 0.4387\n",
            "Epoch 122/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7951 - loss: 0.4426 - val_accuracy: 0.8028 - val_loss: 0.4373\n",
            "Epoch 123/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7956 - loss: 0.4547 - val_accuracy: 0.8028 - val_loss: 0.4360\n",
            "Epoch 124/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8000 - loss: 0.4381 - val_accuracy: 0.8028 - val_loss: 0.4347\n",
            "Epoch 125/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7985 - loss: 0.4462 - val_accuracy: 0.8028 - val_loss: 0.4334\n",
            "Epoch 126/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7854 - loss: 0.4610 - val_accuracy: 0.8028 - val_loss: 0.4320\n",
            "Epoch 127/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7917 - loss: 0.4552 - val_accuracy: 0.8028 - val_loss: 0.4307\n",
            "Epoch 128/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8053 - loss: 0.4403 - val_accuracy: 0.8028 - val_loss: 0.4292\n",
            "Epoch 129/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7974 - loss: 0.4386 - val_accuracy: 0.8028 - val_loss: 0.4278\n",
            "Epoch 130/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8049 - loss: 0.4369 - val_accuracy: 0.8028 - val_loss: 0.4265\n",
            "Epoch 131/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8011 - loss: 0.4335 - val_accuracy: 0.8052 - val_loss: 0.4252\n",
            "Epoch 132/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7886 - loss: 0.4491 - val_accuracy: 0.8075 - val_loss: 0.4240\n",
            "Epoch 133/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8102 - loss: 0.4168 - val_accuracy: 0.8099 - val_loss: 0.4227\n",
            "Epoch 134/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8147 - loss: 0.4141 - val_accuracy: 0.8099 - val_loss: 0.4214\n",
            "Epoch 135/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8043 - loss: 0.4291 - val_accuracy: 0.8099 - val_loss: 0.4202\n",
            "Epoch 136/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8095 - loss: 0.4251 - val_accuracy: 0.8099 - val_loss: 0.4190\n",
            "Epoch 137/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8056 - loss: 0.4163 - val_accuracy: 0.8099 - val_loss: 0.4178\n",
            "Epoch 138/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7937 - loss: 0.4482 - val_accuracy: 0.8099 - val_loss: 0.4168\n",
            "Epoch 139/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8111 - loss: 0.4132 - val_accuracy: 0.8099 - val_loss: 0.4155\n",
            "Epoch 140/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8027 - loss: 0.4263 - val_accuracy: 0.8122 - val_loss: 0.4144\n",
            "Epoch 141/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8163 - loss: 0.4194 - val_accuracy: 0.8122 - val_loss: 0.4132\n",
            "Epoch 142/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8051 - loss: 0.4350 - val_accuracy: 0.8122 - val_loss: 0.4121\n",
            "Epoch 143/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8038 - loss: 0.4243 - val_accuracy: 0.8122 - val_loss: 0.4110\n",
            "Epoch 144/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8209 - loss: 0.4156 - val_accuracy: 0.8122 - val_loss: 0.4098\n",
            "Epoch 145/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8197 - loss: 0.3958 - val_accuracy: 0.8122 - val_loss: 0.4087\n",
            "Epoch 146/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8145 - loss: 0.4114 - val_accuracy: 0.8122 - val_loss: 0.4076\n",
            "Epoch 147/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8119 - loss: 0.4137 - val_accuracy: 0.8169 - val_loss: 0.4065\n",
            "Epoch 148/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8153 - loss: 0.4047 - val_accuracy: 0.8169 - val_loss: 0.4054\n",
            "Epoch 149/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8090 - loss: 0.4100 - val_accuracy: 0.8169 - val_loss: 0.4043\n",
            "Epoch 150/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8007 - loss: 0.4165 - val_accuracy: 0.8169 - val_loss: 0.4032\n",
            "Epoch 151/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8126 - loss: 0.4196 - val_accuracy: 0.8216 - val_loss: 0.4021\n",
            "Epoch 152/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8113 - loss: 0.4110 - val_accuracy: 0.8216 - val_loss: 0.4010\n",
            "Epoch 153/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8120 - loss: 0.4081 - val_accuracy: 0.8239 - val_loss: 0.3999\n",
            "Epoch 154/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8122 - loss: 0.4124 - val_accuracy: 0.8239 - val_loss: 0.3988\n",
            "Epoch 155/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8248 - loss: 0.3978 - val_accuracy: 0.8239 - val_loss: 0.3977\n",
            "Epoch 156/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8180 - loss: 0.4043 - val_accuracy: 0.8263 - val_loss: 0.3966\n",
            "Epoch 157/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8239 - loss: 0.3987 - val_accuracy: 0.8263 - val_loss: 0.3956\n",
            "Epoch 158/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8147 - loss: 0.4081 - val_accuracy: 0.8333 - val_loss: 0.3948\n",
            "Epoch 159/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8187 - loss: 0.4012 - val_accuracy: 0.8357 - val_loss: 0.3938\n",
            "Epoch 160/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8286 - loss: 0.3906 - val_accuracy: 0.8357 - val_loss: 0.3927\n",
            "Epoch 161/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8258 - loss: 0.3890 - val_accuracy: 0.8357 - val_loss: 0.3916\n",
            "Epoch 162/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8223 - loss: 0.3973 - val_accuracy: 0.8404 - val_loss: 0.3908\n",
            "Epoch 163/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8143 - loss: 0.4098 - val_accuracy: 0.8427 - val_loss: 0.3900\n",
            "Epoch 164/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8339 - loss: 0.3936 - val_accuracy: 0.8427 - val_loss: 0.3888\n",
            "Epoch 165/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8209 - loss: 0.3969 - val_accuracy: 0.8451 - val_loss: 0.3878\n",
            "Epoch 166/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8293 - loss: 0.3882 - val_accuracy: 0.8498 - val_loss: 0.3868\n",
            "Epoch 167/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8240 - loss: 0.3982 - val_accuracy: 0.8521 - val_loss: 0.3858\n",
            "Epoch 168/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8489 - loss: 0.3644 - val_accuracy: 0.8498 - val_loss: 0.3847\n",
            "Epoch 169/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8398 - loss: 0.3796 - val_accuracy: 0.8498 - val_loss: 0.3837\n",
            "Epoch 170/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8355 - loss: 0.3871 - val_accuracy: 0.8521 - val_loss: 0.3831\n",
            "Epoch 171/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8245 - loss: 0.3971 - val_accuracy: 0.8568 - val_loss: 0.3823\n",
            "Epoch 172/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8261 - loss: 0.3897 - val_accuracy: 0.8568 - val_loss: 0.3813\n",
            "Epoch 173/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8326 - loss: 0.3766 - val_accuracy: 0.8568 - val_loss: 0.3804\n",
            "Epoch 174/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8344 - loss: 0.3743 - val_accuracy: 0.8592 - val_loss: 0.3795\n",
            "Epoch 175/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8377 - loss: 0.3752 - val_accuracy: 0.8592 - val_loss: 0.3788\n",
            "Epoch 176/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8378 - loss: 0.3740 - val_accuracy: 0.8615 - val_loss: 0.3780\n",
            "Epoch 177/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8389 - loss: 0.3807 - val_accuracy: 0.8615 - val_loss: 0.3773\n",
            "Epoch 178/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8340 - loss: 0.3764 - val_accuracy: 0.8592 - val_loss: 0.3763\n",
            "Epoch 179/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8281 - loss: 0.3838 - val_accuracy: 0.8592 - val_loss: 0.3755\n",
            "Epoch 180/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8388 - loss: 0.3691 - val_accuracy: 0.8592 - val_loss: 0.3746\n",
            "Epoch 181/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8356 - loss: 0.3811 - val_accuracy: 0.8592 - val_loss: 0.3738\n",
            "Epoch 182/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8372 - loss: 0.3654 - val_accuracy: 0.8592 - val_loss: 0.3729\n",
            "Epoch 183/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8425 - loss: 0.3722 - val_accuracy: 0.8615 - val_loss: 0.3721\n",
            "Epoch 184/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8355 - loss: 0.3793 - val_accuracy: 0.8615 - val_loss: 0.3714\n",
            "Epoch 185/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8472 - loss: 0.3701 - val_accuracy: 0.8615 - val_loss: 0.3708\n",
            "Epoch 186/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8314 - loss: 0.3922 - val_accuracy: 0.8662 - val_loss: 0.3703\n",
            "Epoch 187/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8381 - loss: 0.3608 - val_accuracy: 0.8662 - val_loss: 0.3694\n",
            "Epoch 188/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8434 - loss: 0.3689 - val_accuracy: 0.8662 - val_loss: 0.3686\n",
            "Epoch 189/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8484 - loss: 0.3588 - val_accuracy: 0.8638 - val_loss: 0.3675\n",
            "Epoch 190/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8394 - loss: 0.3779 - val_accuracy: 0.8662 - val_loss: 0.3668\n",
            "Epoch 191/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8431 - loss: 0.3650 - val_accuracy: 0.8638 - val_loss: 0.3661\n",
            "Epoch 192/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8444 - loss: 0.3609 - val_accuracy: 0.8662 - val_loss: 0.3655\n",
            "Epoch 193/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8478 - loss: 0.3616 - val_accuracy: 0.8662 - val_loss: 0.3649\n",
            "Epoch 194/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8545 - loss: 0.3568 - val_accuracy: 0.8709 - val_loss: 0.3644\n",
            "Epoch 195/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8574 - loss: 0.3579 - val_accuracy: 0.8709 - val_loss: 0.3635\n",
            "Epoch 196/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8506 - loss: 0.3655 - val_accuracy: 0.8709 - val_loss: 0.3630\n",
            "Epoch 197/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8596 - loss: 0.3629 - val_accuracy: 0.8732 - val_loss: 0.3624\n",
            "Epoch 198/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8581 - loss: 0.3582 - val_accuracy: 0.8732 - val_loss: 0.3618\n",
            "Epoch 199/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8581 - loss: 0.3637 - val_accuracy: 0.8732 - val_loss: 0.3612\n",
            "Epoch 200/200\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8597 - loss: 0.3664 - val_accuracy: 0.8732 - val_loss: 0.3604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vizualize Training History**"
      ],
      "metadata": {
        "id": "17dCr9zgAnQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "BpMKkmkDmweF",
        "outputId": "71cc0683-0300-4bab-85df-7a571a4d358e",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Set Classification Accuracy of FP32 Model**"
      ],
      "metadata": {
        "id": "egHE_K6vArh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the FP32 model\n",
        "results = model.evaluate(test_dataset)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")\n",
        "\n",
        "# Goal: Target Accuracy >= 0.85"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IElgrf-TJJPC",
        "outputId": "a09bc71c-6ff3-4cf9-e965-113cba45e394"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8749 - loss: 0.3740\n",
            "Test Loss: 0.3603757619857788, Test Accuracy: 0.8732394576072693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to TFLite Model**"
      ],
      "metadata": {
        "id": "tdyvZiHWA-A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "# Print tensor details\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "for tensor in interpreter.get_tensor_details():\n",
        "    print(tensor['name'], tensor['dtype'])\n",
        "    try:\n",
        "        # Attempt to get tensor data\n",
        "        tensor_data = interpreter.get_tensor(tensor['index'])\n",
        "        print(tensor_data)\n",
        "    except ValueError:\n",
        "        # Skip tensors with null data\n",
        "        print(f\"Skipping tensor '{tensor['name']}' as it has null data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC6reRpb_00G",
        "outputId": "4e05bce0-18af-4746-e883-63640c86d133"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp4d1ngffz'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 21), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132167652528336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132167652526992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132167652528144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132167652526800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "serving_default_keras_tensor:0 <class 'numpy.float32'>\n",
            "[[1.5602182e+11 4.3122158e-41 1.5602182e+11 4.3122158e-41 1.0361846e-31\n",
            "  0.0000000e+00 1.5601500e+11 4.3122158e-41 0.0000000e+00 0.0000000e+00\n",
            "  4.6242849e-44 0.0000000e+00 1.0342921e-31 0.0000000e+00 1.5601500e+11\n",
            "  4.3122158e-41 1.2331426e-42 0.0000000e+00 1.7936620e-43 0.0000000e+00\n",
            "  1.0213057e-31]]\n",
            "arith.constant <class 'numpy.float32'>\n",
            "[[ 1.7178907   0.5369177   0.406291  ]\n",
            " [-1.0972614   1.2349787   0.0587976 ]\n",
            " [ 0.20161922 -1.6161891  -0.8564017 ]]\n",
            "sequential_1/dense_1_2/Add/ReadVariableOp <class 'numpy.float32'>\n",
            "[-0.11548933 -0.4943558   0.41480836]\n",
            "sequential_1/dense_1/Add/ReadVariableOp <class 'numpy.float32'>\n",
            "[0.42679787 0.08903583 0.        ]\n",
            "sequential_1/dense_1/MatMul <class 'numpy.float32'>\n",
            "[[-0.39603305  1.5248641  -0.31371272  0.86114156  0.36810553 -0.501193\n",
            "  -1.3469673  -0.5931729   0.9470291  -1.0037192   0.4072675   0.21254016\n",
            "   0.09986855 -0.07866956 -0.2179709   0.01702797 -0.18771489  0.14431632\n",
            "   0.3426753  -0.83860344  0.3882447 ]\n",
            " [ 0.32799965  0.2482648  -0.41967225  0.0463035  -0.5484754  -0.84312767\n",
            "  -1.3981338  -0.20966049 -0.32976303 -0.98259103  1.0363114   0.06223584\n",
            "  -0.07267078  0.09994201  0.37120286  0.14376637  0.5769873  -0.01097327\n",
            "   0.77022    -0.9178136   0.3421496 ]\n",
            " [-0.14128923  0.1523323   0.02378929 -0.05364251  0.00294638 -0.3076191\n",
            "  -0.46235502 -0.23745918 -0.0377146  -0.29983795 -0.39010704 -0.37897873\n",
            "   0.18129575  0.06363583  0.45452142 -0.24468029  0.11641371 -0.38032115\n",
            "  -0.06815016 -0.4613073  -0.12401867]]\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/Add <class 'numpy.float32'>\n",
            "Skipping tensor 'sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/Add' as it has null data.\n",
            "StatefulPartitionedCall_1:0 <class 'numpy.float32'>\n",
            "[[3.4428496e-36 0.0000000e+00 1.0342940e-31]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert to Quantized Model**"
      ],
      "metadata": {
        "id": "C1gaSy-8xaMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide a representative dataset to guide the quantization process\n",
        "def representative_dataset_gen():\n",
        "    for data, _ in test_dataset.unbatch().batch(1).take(100):\n",
        "        yield [data]\n",
        "\n",
        "# Convert the model to int8 format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "converter.experimental_new_quantizer = False  # Optional: Use the default quantizer\n",
        "converter._experimental_disable_per_channel = True\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('model_quantized.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)\n",
        "\n",
        "# Print tensor details\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "for tensor in interpreter.get_tensor_details():\n",
        "    print(tensor['name'], tensor['dtype'])\n",
        "    try:\n",
        "        # Attempt to get tensor data\n",
        "        tensor_data = interpreter.get_tensor(tensor['index'])\n",
        "        print(tensor_data)\n",
        "    except ValueError:\n",
        "        # Skip tensors with null data\n",
        "        print(f\"Skipping tensor '{tensor['name']}' as it has null data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu50RF1HBJZ_",
        "outputId": "a30d2cb5-fe1a-4d81-e536-a90f55de5557"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpv__t4ufv'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 21), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132167652528336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132167652526992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132167652528144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132167652526800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "serving_default_keras_tensor:0_int8 <class 'numpy.int8'>\n",
            "[[ 16  79  17  82  53 120   0   0  96  88  14  12   0   0   0   0  48  36\n",
            "  -58   8   0]]\n",
            "arith.constant <class 'numpy.int8'>\n",
            "[[ 127   40   30]\n",
            " [ -81   91    4]\n",
            " [  15 -119  -63]]\n",
            "sequential_1/dense_1_2/Add/ReadVariableOp <class 'numpy.int32'>\n",
            "[ -906 -3877  3253]\n",
            "sequential_1/dense_1/Add/ReadVariableOp <class 'numpy.int32'>\n",
            "[9064 1891    0]\n",
            "sequential_1/dense_1/MatMul <class 'numpy.int8'>\n",
            "[[ -33  127  -26   72   31  -42 -112  -49   79  -84   34   18    8   -7\n",
            "   -18    1  -16   12   29  -70   32]\n",
            " [  27   21  -35    4  -46  -70 -116  -17  -27  -82   86    5   -6    8\n",
            "    31   12   48   -1   64  -76   28]\n",
            " [ -12   13    2   -4    0  -26  -39  -20   -3  -25  -32  -32   15    5\n",
            "    38  -20   10  -32   -6  -38  -10]]\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/Add <class 'numpy.int8'>\n",
            "Skipping tensor 'sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/Add' as it has null data.\n",
            "StatefulPartitionedCall_1:0_int8 <class 'numpy.int8'>\n",
            "[[  16  -44 -117]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Set Classification Accuracy of 8-bit Model**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SZZ-Oth4nJOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test accuracy of quantized model\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "correct = 0.0\n",
        "total = 0.0\n",
        "\n",
        "# Define the transformation function\n",
        "def convert_to_int8(X, y):\n",
        "    X = tf.cast((X * 255) - 128, tf.int8)  # Multiply by 255 and subtract 128 cast to int8\n",
        "    return X, y\n",
        "\n",
        "# Apply the transformation to the dataset\n",
        "test_dataset_quantized_inputs = test_dataset.map(convert_to_int8).unbatch()\n",
        "\n",
        "# Example: Inspect the first batch\n",
        "for X_batch, y_batch in test_dataset_quantized_inputs.take(1):\n",
        "    print(X_batch.numpy(), y_batch.numpy())\n",
        "for input, label in test_dataset_quantized_inputs.batch(1):\n",
        "    interpreter.set_tensor(input_details[0]['index'], input.numpy().astype('int8'))\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    total += 1\n",
        "    if output.argmax() == label.numpy()[0]:\n",
        "        correct += 1\n",
        "\n",
        "print(f\"Quantized Model Test Set Accuracy: {correct / total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSPokUuAfoOQ",
        "outputId": "c6d2dc78-035b-41f1-c78a-b2c337699000"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0 -101 -122  -77  -94 -128 -128  -12  -94 -128  -50  -32  -22  -35\n",
            "  -57 -128   30   17   14 -119    0] 0\n",
            "Quantized Model Test Set Accuracy: 0.8732394366197183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the quantized TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Extract tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "all_tensor_details = interpreter.get_tensor_details()\n",
        "\n",
        "# Extract weights, biases, scales, and zero points from allocated tensors\n",
        "quantized_params = {}\n",
        "for tensor in all_tensor_details:\n",
        "    # Check if the tensor has quantization parameters and valid data\n",
        "    try:\n",
        "        # Attempt to get tensor data\n",
        "        tensor_data = interpreter.get_tensor(tensor['index'])\n",
        "\n",
        "        if 'quantization_parameters' in tensor and tensor['quantization_parameters']['scales'].size > 0:\n",
        "            quantized_params[tensor['name']] = {\n",
        "                'values': tensor_data,\n",
        "                'scale': tensor['quantization_parameters']['scales'],\n",
        "                'zero_point': tensor['quantization_parameters']['zero_points']\n",
        "            }\n",
        "    except ValueError:\n",
        "        # Skip tensors with null data\n",
        "        if 'quantization_parameters' in tensor and tensor['quantization_parameters']['scales'].size > 0:\n",
        "            quantized_params[tensor['name']] = {\n",
        "                'scale': tensor['quantization_parameters']['scales'],\n",
        "                'zero_point': tensor['quantization_parameters']['zero_points']\n",
        "            }\n",
        "        print(f\"Skipping tensor '{tensor['name']}' as it has null data.\")\n",
        "\n",
        "print(\"Quantized parameters extracted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv4RuhPA3MDI",
        "outputId": "edb9fdae-5035-41a5-9e8f-61ae07192c63"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping tensor 'sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/Add' as it has null data.\n",
            "Quantized parameters extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Print Names, Weights, Scales, and Zero Points of Quantized Model Tensors'**"
      ],
      "metadata": {
        "id": "V3QtTCSOe_8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, params in quantized_params.items():\n",
        "    print(f\"{name} - Scale: {params['scale']}, Zero Point: {params['zero_point']}\")\n",
        "    if 'values' in params:\n",
        "      print(params['values'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vO6T2gMerb9",
        "outputId": "01adeec7-dc43-411b-fe42-caedfa59cd61"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "serving_default_keras_tensor:0_int8 - Scale: [0.00392157], Zero Point: [-128]\n",
            "[[112  80 117  11   0   0   0   0 -16 -35 -12  11   0   0   0   0   0   0\n",
            "    0   0   0]]\n",
            "arith.constant - Scale: [0.0135267], Zero Point: [0]\n",
            "[[ 127   40   30]\n",
            " [ -81   91    4]\n",
            " [  15 -119  -63]]\n",
            "sequential_1/dense_1_2/Add/ReadVariableOp - Scale: [0.0001275], Zero Point: [0]\n",
            "[ -906 -3877  3253]\n",
            "sequential_1/dense_1/Add/ReadVariableOp - Scale: [4.7085505e-05], Zero Point: [0]\n",
            "[9064 1891    0]\n",
            "sequential_1/dense_1/MatMul - Scale: [0.0120068], Zero Point: [0]\n",
            "[[ -33  127  -26   72   31  -42 -112  -49   79  -84   34   18    8   -7\n",
            "   -18    1  -16   12   29  -70   32]\n",
            " [  27   21  -35    4  -46  -70 -116  -17  -27  -82   86    5   -6    8\n",
            "    31   12   48   -1   64  -76   28]\n",
            " [ -12   13    2   -4    0  -26  -39  -20   -3  -25  -32  -32   15    5\n",
            "    38  -20   10  -32   -6  -38  -10]]\n",
            "sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/Add - Scale: [0.00942592], Zero Point: [-128]\n",
            "StatefulPartitionedCall_1:0_int8 - Scale: [0.03068045], Zero Point: [-30]\n",
            "[[0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Map TFLite Provided Names to Intuitive Ones**\n",
        "The TFLite Layer names after quantization are not very intuitive.\n",
        "Use the names above + the [Netron](https://netron.app/) application to update dictionary below so that it is very clear which layer is which. *You may need to update the names if any changes are made to the notebook.*"
      ],
      "metadata": {
        "id": "dDUXkdPJAlNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer name map\n",
        "layer_name_map = {\n",
        "    \"input_layer\": \"serving_default_keras_tensor:0_int8\",\n",
        "    \"layer_one_weights\": \"sequential_1/dense_1/MatMul\",\n",
        "    \"layer_one_bias\": \"sequential_1/dense_1/Add/ReadVariableOp\",\n",
        "    \"layer_one_output_activations\": \"sequential_1/dense_1/MatMul;sequential_1/dense_1/Relu;sequential_1/dense_1/Add\",\n",
        "    \"layer_two_weights\": \"arith.constant\",\n",
        "    \"layer_two_bias\": \"sequential_1/dense_1_2/Add/ReadVariableOp\",\n",
        "    \"output_layer\": \"StatefulPartitionedCall_1:0_int8\"\n",
        "}"
      ],
      "metadata": {
        "id": "GqeYV9Ko_kAq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer_scale = quantized_params[layer_name_map[\"input_layer\"]][\"scale\"]\n",
        "input_layer_zero_point = quantized_params[layer_name_map[\"input_layer\"]][\"zero_point\"]\n",
        "\n",
        "layer_one_weights = quantized_params[layer_name_map[\"layer_one_weights\"]][\"values\"]\n",
        "layer_one_weights_scale = quantized_params[layer_name_map[\"layer_one_weights\"]][\"scale\"]\n",
        "layer_one_weights_zero_point = quantized_params[layer_name_map[\"layer_one_weights\"]][\"zero_point\"]\n",
        "layer_one_bias = quantized_params[layer_name_map[\"layer_one_bias\"]][\"values\"]\n",
        "\n",
        "layer_one_output_activations_scale = quantized_params[layer_name_map[\"layer_one_output_activations\"]][\"scale\"]\n",
        "layer_one_output_activations_zero_point = quantized_params[layer_name_map[\"layer_one_output_activations\"]][\"zero_point\"]\n",
        "\n",
        "layer_two_weights = quantized_params[layer_name_map[\"layer_two_weights\"]][\"values\"]\n",
        "layer_two_weights_scale = quantized_params[layer_name_map[\"layer_two_weights\"]][\"scale\"]\n",
        "layer_two_weights_zero_point = quantized_params[layer_name_map[\"layer_two_weights\"]][\"zero_point\"]\n",
        "layer_two_bias = quantized_params[layer_name_map[\"layer_two_bias\"]][\"values\"]\n",
        "\n",
        "output_layer_scale = quantized_params[layer_name_map[\"output_layer\"]][\"scale\"]\n",
        "output_layer_zero_point = quantized_params[layer_name_map[\"output_layer\"]][\"zero_point\"]"
      ],
      "metadata": {
        "id": "txvONXs98UfK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fixed_point_multiplier(input_scale, weight_scale, output_scale):\n",
        "    # Calculate M0 and N from M = 2^-N M0 = (S1 * S2 / S3)\n",
        "    multiplier = input_scale * weight_scale / output_scale\n",
        "    shift = 0\n",
        "    while multiplier < 0.5:\n",
        "        multiplier *= 2\n",
        "        shift += 1\n",
        "    quantized_multiplier = multiplier * math.pow(2, 31)\n",
        "    return quantized_multiplier, shift"
      ],
      "metadata": {
        "id": "l00b5QnEaRmX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First layer requantization params\n",
        "layer_one_multiplier, layer_one_shift = calculate_fixed_point_multiplier(input_layer_scale, layer_one_weights_scale, layer_one_output_activations_scale)\n",
        "\n",
        "# Second layer requantization params\n",
        "layer_two_multiplier, layer_two_shift = calculate_fixed_point_multiplier(layer_one_output_activations_scale, layer_two_weights_scale, output_layer_scale)\n",
        "\n",
        "subscript_printing = str.maketrans(\"123456789\", \"₁₂₃₄₅₆₇₈₉\")\n",
        "print(\"Layer 1 Requantization Params:\")\n",
        "print(\"M01: \".translate(subscript_printing) + f\"{layer_one_multiplier[0]:.2f}\")\n",
        "print(\"N1: \".translate(subscript_printing)  + f\"{layer_one_shift}\")\n",
        "\n",
        "print(\"Layer 2 Requantization Params:\")\n",
        "print(\"M02: \".translate(subscript_printing) + f\"{layer_two_multiplier[0]:.2f}\")\n",
        "print(\"N2: \".translate(subscript_printing)  + f\"{layer_two_shift}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibGduAd2bMYE",
        "outputId": "6f19c6a4-2faf-4dc6-d37a-91f3f7510916"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Requantization Params:\n",
            "M0₁: 1373103872.00\n",
            "N₁: 7\n",
            "Layer 2 Requantization Params:\n",
            "M0₂: 1142335232.00\n",
            "N₂: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Emulate 8-bit Integer Inference with Numpy**"
      ],
      "metadata": {
        "id": "pDmEYHIJoB5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create copy of test dataset\n",
        "TEST_SET_SIZE = 426\n",
        "test_dataset_copy = (iter(test_dataset_quantized_inputs.take(TEST_SET_SIZE)))"
      ],
      "metadata": {
        "id": "Sskn2QitsQ1e"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tflite_golden_inference(tflite_model, inputs, debug=False):\n",
        "    #\n",
        "    # Golden Reference Implementation of TFLite Inference running on a single sample\n",
        "    #\n",
        "\n",
        "    # Add batch dim to single data sample\n",
        "    inputs = np.expand_dims(inputs, 0)\n",
        "\n",
        "    # Load TFLite model and allocate tensors\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model, experimental_preserve_all_tensors=True)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensors\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Load input tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], inputs)\n",
        "\n",
        "    # Run the model\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Print each layer's output if needed for verification\n",
        "    if debug:\n",
        "      print({\n",
        "          t['name']: interpreter.get_tensor(t['index'])\n",
        "          for t in interpreter.get_tensor_details()\n",
        "      })\n",
        "\n",
        "    return interpreter.get_tensor(output_details[0]['index'])"
      ],
      "metadata": {
        "id": "jja_F8B4sRqV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_numpy_inference(input):\n",
        "  #\n",
        "  # Numpy Reference Implementation of TFLite Inference running on a single sample\n",
        "  #\n",
        "\n",
        "  # (Inputs * Layer 1 Weights) + Bias followed by ReLU\n",
        "  x = np.matmul((input.numpy().astype(np.int32) - input_layer_zero_point.astype(np.int32)),(layer_one_weights.T.astype(np.int32) - layer_one_weights_zero_point.astype(np.int32)))\n",
        "  x = x + layer_one_bias\n",
        "  x = np.maximum(x, 0)\n",
        "\n",
        "  # Requantization pipeline\n",
        "  x = x * layer_one_multiplier\n",
        "  x = np.round((x / np.power(2,31))).astype(np.int32)\n",
        "  x = np.round((x / np.power(2, layer_one_shift))).astype(np.int32)\n",
        "  x = x + layer_one_output_activations_zero_point.astype(np.int32)\n",
        "  x = np.clip(x, -128, 127)\n",
        "\n",
        "  # (Layer 1 Activations * Layer 2 Weights) + Bias\n",
        "  x = np.matmul((x.astype(np.int32) - layer_one_output_activations_zero_point.astype(np.int32)), (layer_two_weights.T.astype(np.int32) - layer_two_weights_zero_point.astype(np.int32)))\n",
        "  x = x + layer_two_bias\n",
        "\n",
        "  # Requantization pipeline\n",
        "  x = x * layer_two_multiplier\n",
        "  x = np.round((x / np.power(2,31))).astype(np.int32)\n",
        "  x = np.round((x / np.power(2, layer_two_shift))).astype(np.int32)\n",
        "  x = x + output_layer_zero_point.astype(np.int32)\n",
        "  x = np.clip(x, -128, 127)\n",
        "\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "5_Hx0jC7oBje"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_correct = 0.0\n",
        "numpy_correct = 0.0\n",
        "\n",
        "for inputs, targets in test_dataset_copy:\n",
        "  tflite_output = run_tflite_golden_inference(tflite_quant_model, inputs)\n",
        "  numpy_output = run_numpy_inference(inputs)\n",
        "\n",
        "  # Make sure that raw values of output tensors match exactly to validate numpy reference implementation\n",
        "  if np.array_equal(tflite_output.flatten()[0], numpy_output.flatten()):\n",
        "    print(tflite_output)\n",
        "    print(numpy_output)\n",
        "    print(\"ERROR: TFlite Golden Output Tensor does not match Numpy Implementation Output Tensor\")\n",
        "\n",
        "  # Update num correct\n",
        "  if tflite_output.argmax() == targets.numpy():\n",
        "    tflite_correct += 1\n",
        "  if numpy_output.argmax() == targets.numpy():\n",
        "    numpy_correct += 1\n",
        "\n",
        "\n",
        "\n",
        "# Make sure accuracy is exactly the same to validate numpy implementation\n",
        "print(f\"TF Lite Accuracy: {tflite_correct / TEST_SET_SIZE}\")\n",
        "print(f\"Numpy Accuracy: {numpy_correct / TEST_SET_SIZE}\")\n",
        "print(\"Numpy Implementation matches TFLite Golden Implementation!\" if tflite_correct / TEST_SET_SIZE == numpy_correct / TEST_SET_SIZE else \"Numpy Implementation does NOT match TFLite Golden Implementation...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1qDAzufb-ZD",
        "outputId": "aa3ac958-f395-43f3-c689-8b438c96e619"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Lite Accuracy: 0.8732394366197183\n",
            "Numpy Accuracy: 0.8732394366197183\n",
            "Numpy Implementation matches TFLite Golden Implementation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Write Test Set Data to CSV for C Implementation in Flexibench Repository**"
      ],
      "metadata": {
        "id": "baAWcWzjEK5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_test_data_to_csv(dataset, headers, filename):\n",
        "  with open(filename, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(headers)\n",
        "    tflite_correct = 0.0\n",
        "    for inputs, targets in dataset:\n",
        "      # Write the original TFLite model prediction along with inputs and ground truth label to the CSV for reference\n",
        "      tflite_model_output = run_tflite_golden_inference(tflite_quant_model, inputs)\n",
        "      tflite_model_prediction = tflite_model_output.argmax()  # get classs label and convert to array\n",
        "      if tflite_model_prediction == targets.numpy():\n",
        "        tflite_correct += 1\n",
        "      writer = csv.writer(csvfile)\n",
        "      writer.writerow(np.append(inputs.numpy(), [tflite_model_prediction, targets.numpy()]))\n",
        "    print(f\"TF Lite Accuracy: {tflite_correct / TEST_SET_SIZE}\")\n",
        "    print(\"CSV saved.\")\n",
        "\n",
        "column_names = cardiotocography.data.original.columns.tolist()[:-2] + ['TFLite_Model_Prediction'] + ['NSP_Golden_Label']\n",
        "dataset_copy_for_csv = iter(test_dataset_quantized_inputs.take(TEST_SET_SIZE))\n",
        "dump_test_data_to_csv(dataset_copy_for_csv, column_names, 'samples.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp2rAmNG_dFh",
        "outputId": "51c787cb-ec45-408c-f2bf-e0fdfff02561"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Lite Accuracy: 0.8732394366197183\n",
            "CSV saved.\n"
          ]
        }
      ]
    }
  ]
}